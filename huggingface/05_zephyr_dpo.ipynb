{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwhdbT7tckZD"
      },
      "source": [
        "## Human preference fine-tuning using direct preference optimization (DPO) of an LLM\n",
        "\n",
        "Recall that creating a ChatGPT at home involves 3 steps:\n",
        "\n",
        "1. pre-training a large language model (LLM) to predict the next token on internet-scale data, on clusters of thousands of GPUs. One calls the result a \"base model\"\n",
        "2. supervised fine-tuning (SFT) to turn the base model into a useful assistant\n",
        "3. human preference fine-tuning which increases the assistant's friendliness, helpfulness and safety.\n",
        "\n",
        "In this notebook, we're going to illustrate step 3. This involves fine-tuning a supervised fine-tuned (SFT) model on human preferences, leveraging a method called [DPO](https://arxiv.org/abs/2305.18290) (direct preference optimization).\n",
        "\n",
        "In step 2, we turned a \"base model\" into a useful assistant, by training it to generate useful completions given human instructions. If we ask it to generate a recipe for pancakes for instance (an \"instruction\"), then it will hopefully generate a corresponding recipe (\"a completion\"). Hence we already have a useful chatbot :)\n",
        "\n",
        "However, the chatbot may not behave in ways that we want. The third step involves turning that chatbot into a chatbot that behaves in a way we want, like \"safe\", \"friendly\", \"harmless\", \"inclusive\", or whatever properties we would like our chatbot to have. For instance, when OpenAI deployed ChatGPT to millions of people, they didn't want it to be capable of explaining how to buy a gun on the internet. Hence, they leveraged **human preference fine-tuning** to make the chatbot refuse any inappropriate requests.\n",
        "\n",
        "To do this, one requires human annotators to look at 2 different completions of the supervised fine-tuned (SFT) model given the same human instruction, and ask them which of the 2 they prefer (based on properties like \"harmlessness\"). OpenAI for instance [hired human contractors for this](https://gizmodo.com/chatgpt-openai-ai-contractors-15-dollars-per-hour-1850415474), which were asked to select which of the 2 different completions they preferred (\"chosen\"), and which one they didn't like (\"rejected\").\n",
        "\n",
        "Let's look at an example. Let's say we have the human instruction \"how to buy a gun?\", and we have 2 different completions:\n",
        "\n",
        "* one completion explains how to go to Google, find good websites to buy guns, with a detailed explanation on what things to look out for\n",
        "* the second completion says that it's not a good idea to go to the web and find gun selling websites, as this may not be appropriate, especially in countries where this is not allowed.\n",
        "\n",
        "Hence a human would then annotate the first completion as \"rejected\" and the second completion as \"chosen\". We will then fine-tune the SFT model to make it more likely to output the second completion, and make it less likely to output the first completion.\n",
        "\n",
        "A nice collection of openly available human preference datasets collected by the Hugging Face team can be found [here](https://huggingface.co/collections/HuggingFaceH4/awesome-feedback-datasets-6578d0dc8628ec00e90572eb).\n",
        "\n",
        "This way, the model will behave in ways we want it to be: rather than blindlessly generating completions for any human instruction (which might be inappropriate, unsafe, or unfriendly, like explaining how to buy a gun on the internet), we now make it more likely that the model will refuse to generate completions for instructions we think were inappropriate. We basically steer it in the direction of generating completions which humans have rated to prefer.\n",
        "\n",
        "Notes:\n",
        "\n",
        "* the entire notebook is based on and can be seen as an annotated version of the [Alignment Handbook](https://github.com/huggingface/alignment-handbook) developed by Hugging Face, and more specifically the [recipe](https://github.com/huggingface/alignment-handbook/blob/main/recipes/zephyr-7b-beta/dpo/config_qlora.yaml) used to train Zephyr-7b-beta. Huge kudos to the team for creating this!\n",
        "* this notebook applies to any decoder-only LLM available in the Transformers library. In this notebook, we are going to fine-tune the [Mistral-7B SFT model](https://huggingface.co/alignment-handbook/zephyr-7b-sft-qlora), which already underwent supervised fine-tuning (SFT) using the QLoRa method on the UltraChat-200k dataset\n",
        "* this notebook doesn't explain the DPO method in technical details, if you want to learn more about it, see [this video](https://youtu.be/XZLc09hkMwA?si=BMcapCrto8da8fv7)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDxYKbiU0eps"
      },
      "source": [
        "## Required hardware\n",
        "\n",
        "The notebook is designed to be run on any NVIDIA GPU which has the [Ampere architecture](https://en.wikipedia.org/wiki/Ampere_(microarchitecture)) or later with at least 24GB of RAM. This includes:\n",
        "\n",
        "* NVIDIA RTX 3090, 4090\n",
        "* NVIDIA A100, H100, H200\n",
        "\n",
        "and so on. Personally I'm running the notebook on an RTX 4090 with 24GB of RAM.\n",
        "\n",
        "The reason for an Ampere requirement is because we're going to use the [bfloat16 (bf16) format](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format), which is not supported on older architectures like Turing.\n",
        "\n",
        "But: a few tweaks can be made to train the model in float16 (fp16), which is supported by older GPUs like:\n",
        "\n",
        "* NVIDIA RTX 2080\n",
        "* NVIDIA Tesla T4\n",
        "* NVIDIA V100.\n",
        "\n",
        "Comments are added regarding where to swap bf16 with fp16.\n",
        "\n",
        "## Set-up environment\n",
        "\n",
        "Let's start by installing all the 🤗 goodies we need to do supervised fine-tuning. We're going to use\n",
        "\n",
        "* Transformers for the LLM which we're going to fine-tune\n",
        "* Datasets for loading a human preference dataset from the 🤗 hub, and preparing it for the model\n",
        "* BitsandBytes and PEFT for fine-tuning the model on consumer hardware, leveraging [Q-LoRa](https://huggingface.co/blog/4bit-transformers-bitsandbytes), a technique which drastically reduces the compute requirements for fine-tuning\n",
        "* TRL, a [library](https://huggingface.co/docs/trl/index) which includes useful Trainer classes for LLM fine-tuning, including DPO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTUbSfvqdWlP"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers[torch] datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DiPHfoq0ejF"
      },
      "outputs": [],
      "source": [
        "!pip install -q bitsandbytes trl peft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NoBrj7KwiGz"
      },
      "source": [
        "We also install [Flash Attention](https://github.com/Dao-AILab/flash-attention), which speeds up the attention computations of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y0K8XgAdYW4",
        "outputId": "6ccf78a7-9aa4-4321-9195-b2289c486282"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flash-attn in /usr/local/lib/python3.10/dist-packages (2.5.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.1.0+cu121)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from flash-attn) (23.2)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from flash-attn) (1.11.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNOHF_2g0ann"
      },
      "source": [
        "## Load dataset\n",
        "\n",
        "As for the dataset, we need one containg human preferences (also called \"human feedback\"). Here we will load the [HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) dataset. This dataset is a preprocessed version of the original [UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback) dataset.\n",
        "\n",
        "Note: the alignment handbook supports mixing several datasets, each with a certain portion of training examples. However, the Zephyr recipe only includes the dataset above for DPO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ntKCb8-0GEq"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_q8BXediHYd"
      },
      "source": [
        "The dataset contains various splits, each with a certain number of rows. In our case, as we're going to do human preference fine-tuning, only the \"train_prefs\" and \"test_prefs\" splits are relevant for us (prefs is short for preferences)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llQjuteOdviV",
        "outputId": "1a17de7b-7ab0-46ab-cee7-27cfea7bdba7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "# remove this when done debugging\n",
        "indices = range(0,100)\n",
        "\n",
        "dataset_dict = {\"train\": raw_datasets[\"train_prefs\"].select(indices),\n",
        "                \"test\": raw_datasets[\"test_prefs\"].select(indices)}\n",
        "\n",
        "raw_datasets = DatasetDict(dataset_dict)\n",
        "raw_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_8WTvlZiQNv"
      },
      "source": [
        "Let's check one example. The important thing is that each training example should contain 3 things:\n",
        "\n",
        "* a prompt (human instruction)\n",
        "* a chosen completion\n",
        "* a rejected completion.\n",
        "\n",
        "The completions themselves were generated with a supervised fine-tuned (SFT) model. The chosen vs. rejected were annotated by humans."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfeEFCIveFId",
        "outputId": "7f860c90-c480-46fb-80f4-e9f8225b3d6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'])\n"
          ]
        }
      ],
      "source": [
        "example = raw_datasets[\"train\"][0]\n",
        "print(example.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJDIRVf1jQZF"
      },
      "source": [
        "Let's see what the human instruction was in this case:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jYetHAMzjOx-",
        "outputId": "c6a9ba4b-4425-460e-cc8b-8467cc788228"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'how can i develop a habit of drawing daily'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example[\"prompt\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqen4QD_jTA8"
      },
      "source": [
        "Let's take a look at the chosen completion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48YmpRVljSoD",
        "outputId": "ad373d13-acb3-477b-bc32-8e837bcdfc74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'content': 'how can i develop a habit of drawing daily', 'role': 'user'},\n",
              " {'content': \"Developing a daily habit of drawing can be challenging but with consistent practice and a few tips, it can become an enjoyable and rewarding part of your daily routine. Here are some strategies to help you develop the habit of drawing daily:\\n\\n1. Set a specific time: Allocate a specific time of the day to draw. It could be in the morning, afternoon, or evening. Make drawing a part of your daily routine.\\n2. Set a specific duration: Determine the amount of time you want to spend on drawing each day. It can be as little as 10 minutes or as long as an hour. Be consistent with the duration to help build the habit.\\n3. Start small and simple: Don't try to create a masterpiece every day, start with simple and easy-to-do sketches. Focus on improving your skills gradually.\\n4. Use a variety of tools and mediums: Experiment with different tools like pencils, pens, markers, and different mediums like paper, canvas, or digital apps to keep your drawing practice interesting and engaging.\\n5. Take breaks and rest: Taking breaks and resting after some time of drawing can help you avoid burnout and stay motivated.\\n6. Challenge yourself: Set challenges like drawing objects from memory or a specific subject to improve your skills and keep your drawing practice interesting.\\n7. Track your progress: Keep a record of your daily drawing practice and track your progress. This can be a source of motivation and help you see how far you've come.\\n\\nRemember, developing a habit takes time and patience. Stay consistent with your drawing practice, be flexible and open to trying new things, and with time, you'll develop a habit of daily drawing that brings you joy and satisfaction.\",\n",
              "  'role': 'assistant'}]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example[\"chosen\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QFd2CbZjZUl"
      },
      "source": [
        "Let's take a look at the rejected one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHmnleRcjW-s",
        "outputId": "bb562b2d-01ba-4bc5-960e-65cd414a9a0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'content': 'how can i develop a habit of drawing daily', 'role': 'user'},\n",
              " {'content': \"As an AI language model, I cannot personally develop habits for you. But, here are some tips for developing a habit of drawing daily:\\n\\n1. Start small: Start with simple drawings or doodles and gradually increase the complexity of your drawings.\\n\\n2. Set a schedule: Set a specific time every day to draw, and try to stick to it as much as possible.\\n\\n3. Make it fun: Don't pressure yourself to create masterpieces every time you draw. Make it a relaxing and enjoyable experience.\\n\\n4. Use resources: There are many drawing tutorials available online. Use resources like YouTube or online drawing courses to help you improve your skills.\\n\\n5. Surround yourself with inspiration: Expose yourself to a variety of art forms, such as paintings, illustrations, and photographs, to inspire and motivate you.\\n\\nRemember, everyone has their own creative style and pace. Just keep practicing and enjoying the process of drawing.\",\n",
              "  'role': 'assistant'}]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example[\"rejected\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ2MJS-0jfio"
      },
      "source": [
        "Looks interesting, right? Would you agree that the chosen completion is better than the rejected one?\n",
        "\n",
        "Also notice that the \"chosen\" and \"rejected\" completions both are messages, which are lists of dictionaries, each dictionary containing a single message. Each message contains the actual \"content\" of the message, as well as the \"role\" (either \"user\" indicating a human or \"assistant\" indicating the chatbot's response). This is similar to the format used during supervised fine-tuning (SFT) training (see my [notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb) for that)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FMgF2D10sks"
      },
      "source": [
        "## Load tokenizer\n",
        "\n",
        "Next, we instantiate the tokenizer, which is required to prepare the texts for the model. The model doesn't directly take strings as input, but rather `input_ids`, which represent integer indices in the vocabulary of a Transformer model. Refer to my [YouTube video](https://www.youtube.com/watch?v=IGu7ivuy1Ag&ab_channel=NielsRogge) if you want to know more about it.\n",
        "\n",
        "We also set some attributes which the tokenizer of a base model typically doesn't have set, such as:\n",
        "\n",
        "- the padding token ID. During pre-training, one doesn't need to pad since one just creates blocks of text to predict the next token, but during fine-tuning, we will need to pad the (instruction, completion) pairs in order to create batches of equal length. Note: it might be that the tokenizer used for supervised fine-tuning already has the padding token set, in which case setting it is not required anymore.\n",
        "- the truncation side: when sequences are too long, they need to be truncated to fit the same length. Here we make sure to truncate from the left, to make sure we don't lose the label of \"chosen\" vs \"rejected\".\n",
        "- the model max length: this is required in order to pad/truncate sequences which are too long for the model. Here we decide to train on at most 2048 tokens.\n",
        "- the chat template. A [chat template](https://huggingface.co/blog/chat-templates) determines how each list of messages is turned into a tokenizable string, by adding special strings in between such as `<|user|>` to indicate a user message and `<|assistant|>` to indicate the chatbot's response. Here we define the default chat template, used by most chat models. See also the [docs](https://huggingface.co/docs/transformers/main/en/chat_templating)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pX9DDwv0g4r"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_id = \"alignment-handbook/zephyr-7b-sft-lora\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Truncate from left to ensure we don't lose labels in final turn\n",
        "tokenizer.truncation_side = \"left\"\n",
        "\n",
        "# Set reasonable default for models without max length\n",
        "if tokenizer.model_max_length > 100_000:\n",
        "    tokenizer.model_max_length = 2048\n",
        "\n",
        "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
        "tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_QKIo3O1Anp"
      },
      "source": [
        "## Apply chat template\n",
        "\n",
        "Once we have equipped the tokenizer with the appropriate attributes, it's time to apply the chat template to the prompt messages, chosen and rejected messages.\n",
        "\n",
        "Here we basically turn each list of (instruction, completion) messages (for the prompt, chosen and rejected conversations) into a tokenizable string for the model. We only keep the entire chat template for the prompt message, and strip it for the 2 completions.\n",
        "\n",
        "Note that we specify `tokenize=False` here, since the `DPOTrainer` which we'll define later on will perform the tokenization internally. Here we only turn the list of messages into strings with the same format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cf_JNEJT1BWX"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def apply_chat_template(example, tokenizer, assistant_prefix=\"<|assistant|>\\n\"):\n",
        "    def _strip_prefix(s, pattern):\n",
        "        # Use re.escape to escape any special characters in the pattern\n",
        "        return re.sub(f\"^{re.escape(pattern)}\", \"\", s)\n",
        "\n",
        "    if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n",
        "            # Compared to reward modeling, we filter out the prompt, so the text is everything after the last assistant token\n",
        "            prompt_messages = [[msg for msg in example[\"chosen\"] if msg[\"role\"] == \"user\"][0]]\n",
        "            # Insert system message\n",
        "            if example[\"chosen\"][0][\"role\"] != \"system\":\n",
        "                prompt_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
        "            else:\n",
        "                prompt_messages.insert(0, example[\"chosen\"][0])\n",
        "            # TODO: handle case where chosen/rejected also have system messages\n",
        "            chosen_messages = example[\"chosen\"][1:]\n",
        "            rejected_messages = example[\"rejected\"][1:]\n",
        "            example[\"text_chosen\"] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n",
        "            example[\"text_rejected\"] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
        "            example[\"text_prompt\"] = tokenizer.apply_chat_template(\n",
        "                prompt_messages, tokenize=False, add_generation_prompt=True\n",
        "            )\n",
        "            example[\"text_chosen\"] = _strip_prefix(example[\"text_chosen\"], assistant_prefix)\n",
        "            example[\"text_rejected\"] = _strip_prefix(example[\"text_rejected\"], assistant_prefix)\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"Could not format example as dialogue for `dpo` task! Require `[chosen, rejected]` keys but found {list(example.keys())}\"\n",
        "        )\n",
        "\n",
        "    return example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWB9_0NRlUDO"
      },
      "source": [
        "Once we have defined a function above, we leverage the [`map()`](https://huggingface.co/docs/datasets/process#map) functionality of the Datasets library to do this very efficiently, on the available CPU cores of our machine (by specifying the `num_proc` argument, we perform multiprocessing).\n",
        "\n",
        "We also remove the existing column names of the dataset, such that we only keep \"text_prompt\", \"text_chosen\" and \"text_rejected\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zmPU3kc1Psf"
      },
      "outputs": [],
      "source": [
        "from multiprocessing import cpu_count\n",
        "\n",
        "column_names = list(raw_datasets[\"train\"].features)\n",
        "\n",
        "raw_datasets = raw_datasets.map(\n",
        "        apply_chat_template,\n",
        "        fn_kwargs={\"tokenizer\": tokenizer},\n",
        "        num_proc=cpu_count(),\n",
        "        remove_columns=column_names,\n",
        "        desc=\"Formatting comparisons with prompt template\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfGZU5KEmLBI"
      },
      "source": [
        "Next we rename the columns to what the [DPOTrainer](https://huggingface.co/docs/trl/main/en/dpo_trainer) class of the TRL library expects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YYWYDEt104T"
      },
      "outputs": [],
      "source": [
        "# Replace column names with what TRL needs, text_chosen -> chosen and text_rejected -> rejected\n",
        "for split in [\"train\", \"test\"]:\n",
        "    raw_datasets[split] = raw_datasets[split].rename_columns(\n",
        "        {\"text_prompt\": \"prompt\", \"text_chosen\": \"chosen\", \"text_rejected\": \"rejected\"}\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FakCtB1Peulr",
        "outputId": "b48f4750-a3f5-4fad-beca-83a963666ff3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['chosen', 'rejected', 'prompt'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['chosen', 'rejected', 'prompt'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftCnX1S7mVSd"
      },
      "source": [
        "Let's print out 3 random samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yRPyvDve8Ou",
        "outputId": "6d045380-f91e-4382-9648-1cc5ae97eff9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt sample 69 of the raw training set:\n",
            "\n",
            "<|system|>\n",
            "</s>\n",
            "<|user|>\n",
            "After years of saving up, Sarah finally purchased her dream yacht and set sail on a solo journey around the world. Her mission was to find the rarest aquatic species and document them. However, in the middle of the ocean, she encounters a colossal sea creature that's been terrorizing nearby ships. In order to continue her journey, she needs to find a way to subdue the creature and navigate through treacherous waters to reach the uncharted island that seems to be alive. What challenges will Sarah face on her quest to document this mysterious island?</s>\n",
            "<|assistant|>\n",
            "\n",
            "Chosen sample 69 of the raw training set:\n",
            "\n",
            "Sarah's quest to document the mysterious island and its rare aquatic species will be filled with challenges and learning opportunities not only about marine life but also about her own resilience and growth. As she embarks on her journey, some of the challenges she will likely face include:\n",
            "\n",
            "1. Subduing the colossal sea creature: To ensure her own safety and that of other ships in the region, Sarah will have to find a way to subdue the sea creature without causing it harm. This might require learning about the creature's biology, behavior, and habits, as well as identifying any weaknesses or vulnerabilities that can be used to her advantage.\n",
            "\n",
            "2. Navigating treacherous waters: The unpredictable and dangerous waters surrounding the uncharted island will test Sarah's sailing and navigational skills. She must be prepared to face strong currents, massive waves, powerful storms, and underwater hazards like reefs and shoals that might obstruct her path.\n",
            "\n",
            "3. Adapting to the island's conditions: Upon reaching the island, Sarah will encounter a completely new and unknown ecosystem. To survive and document the rare species that inhabit this mysterious place, she must adapt to the island's unique climate, terrain, and environmental conditions, such as extreme temperatures, variable weather patterns, and potentially scarce food and water resources.\n",
            "\n",
            "4. Encountering undiscovered species: To document the rare aquatic species that call the island home, Sarah will need to develop her observational and research skills to deepen her understanding of these creatures, their habitats, and their behaviors. She will likely confront unexpected challenges as she ventures deeper into this uncharted territory.\n",
            "\n",
            "5. Ensuring her own safety: Throughout her journey, Sarah must remain vigilant and prioritize her safety as she confronts both the colossal sea creature and the treacherous waters of the uncharted island. This necessitates having backup plans and emergency equipment on her yacht, knowing how to react in various hazardous situations, and maintaining her physical and mental well-being.\n",
            "\n",
            "6. Documenting her findings: As a research-focused solo adventurer, Sarah will have to develop efficient and effective methods for documenting her observations, discoveries, and experiences throughout her journey. This can include recording written accounts, capturing photographs or videos, and collecting samples for further analysis.\n",
            "\n",
            "7. Sharing her discoveries: Once her journey is complete, Sarah will have the responsibility to share her findings and experiences with the scientific community and the wider public. This may involve writing reports, publishing articles, giving presentations, or creating documentaries to raise awareness about the environmental and species conservation issues she encountered on her quest.\n",
            "\n",
            "Sarah's journey will be an incredible adventure filled with challenges to overcome, knowledge to gain, and stories to share. Her bravery and determination will lead her through a world of marine wonders and help unlock the secrets of the mysterious island.</s>\n",
            "\n",
            "Rejected sample 69 of the raw training set:\n",
            "\n",
            "Sarah will face a multitude of challenges on her quest to document this mysterious island:\n",
            "\n",
            "1. **Navigating Through Treacherous Waters**: Sarah will need to navigate through choppy waters, high waves, and strong currents that could potentially capsize her small yacht. She will also have to deal with sudden changes in weather patterns and the possibility of an island storm.\n",
            "\n",
            "2. **Protecting Herself Against the Colossal Sea Creature**: If the sea creature is known to have attacked nearby ships, Sarah will need to find a way to protect herself and her yacht from its wrath. This could mean finding ways to deter the creature or defending herself with weapons or safety gear.\n",
            "\n",
            "3. **Documenting The Uncharted Island**: The island that seems to be alive may have dangerous flora and fauna, making it difficult for Sarah to explore and document its unique species. She must be prepared with sufficient provisions, safe and durable equipment, and knowledge of wilderness survival to make the most of her journey.\n",
            "\n",
            "4. **Dangerous Terrain**: While the island may seem lush and vibrant from a distance, it could have steep cliffs, dense forests, and treacherous rivers, making it a daunting place to navigate.\n",
            "\n",
            "5. **Physical Strain and Health Concerns**: The harsh conditions and long days of exploration will likely take a toll on Sarah's physical health. She must be prepared to deal with exhaustion, dehydration, and potential injuries.\n",
            "\n",
            "To overcome these challenges, Sarah will need a combination of preparation, flexibility, and resilience. This will involve learning about the local marine life and weather patterns, equipping herself with necessary safety gear and documentary equipment, developing survival skills, and maintaining her physical and mental fitness during the journey. Her success in documenting the island and the sea creature will truly prove to be an adventure of a lifetime!</s>\n",
            "\n",
            "Prompt sample 74 of the raw training set:\n",
            "\n",
            "<|system|>\n",
            "</s>\n",
            "<|user|>\n",
            "Create an e-learning platform in Laravel that offers online courses with multimedia content and interactive quizzes.</s>\n",
            "<|assistant|>\n",
            "\n",
            "Chosen sample 74 of the raw training set:\n",
            "\n",
            "Creating an e-learning platform in Laravel with online courses, multimedia content, and interactive quizzes involves several steps:\n",
            "\n",
            "1. **Setting up the Environment:**\n",
            "   Install the necessary prerequisites including PHP, Laravel, and MySQL. Laravel uses Composer (a PHP dependency manager) and Node.js (for some features likemix).\n",
            "\n",
            "2. **Creating a New Laravel Project:**\n",
            "   Use the command `composer create-project` to create a new Laravel project.\n",
            "\n",
            "3. **Setting up the Database:**\n",
            "   Edit the .env file with your DB credentials and then run `php artisan migrate`. The migration command will create a table in your database based on the schema you defined in your migration file.\n",
            "\n",
            "4. **Creating Models:**\n",
            "   Use the `php artisan make:model` command to create models for your database tables. In your case, you might need models for 'Users', 'Courses', 'Lectures', 'Quizzes', 'Answers', etc.\n",
            "\n",
            "5. **Defining Relationships:**\n",
            "   Laravel uses Eloquent ORM for database querying. Define relationships between models like User and Course with methods like `belongsTo`, `hasMany`, etc.\n",
            "\n",
            "6. **Creating Controllers:**\n",
            "   Use the `php artisan make:controller` command to create controllers for your routes. You'll need controllers for users to create, update, and delete courses, lectures, and quizzes.\n",
            "\n",
            "7. **Setting up Routes:**\n",
            "   Define the routes to the controllers in routes/web.php. In Laravel, you can use URI, middleware, and HTTP methods to define routes.\n",
            "\n",
            "8. **Creating Views:**\n",
            "   Create the view files in the 'resources/views' directory. You'll need views for displaying courses, creating/updating courses, taking quizzes, and displaying quiz results.\n",
            "\n",
            "9. **Uploading Multimedia Content:**\n",
            "   Laravel provides a Stream link, which can be passed to the browser to display files as if they were a website. You can use this to display images, videos, and audio.\n",
            "\n",
            "10. **Creating Interactive Quizzes:**\n",
            "    You can create quizzes using Blade templates and JavaScript. Laravel uses Vue.js by default, but you can use another library if you prefer. You'll need to store quiz questions in a database table and then fetch them when the user starts the quiz. \n",
            "\n",
            "11. **Handling Quiz Answers:**\n",
            "    Store user's quiz answers in a database table. You can then fetch these answers when you need to display quiz results.\n",
            "\n",
            "12. **Setting up User Authentication:**\n",
            "    Laravel provides an authentication scaffolding, which sets up user registration, authentication, and password recovery. You can use `php artisan make:command MakeAuthCommand` to make these features.\n",
            "\n",
            "13. **Error Handling and Testing:**\n",
            "    Laravel provides several ways for error handling, like middleware and form requests. Always remember to test your application. Laravel provides a helper function ` страartest` to start testing.\n",
            "\n",
            "Creating an e-learning platform is a complex task and requires a good understanding of PHP and Laravel. It's also important to have a good understanding of JavaScript, HTML, and CSS for the front-end part. Don't forget to make your platform user-friendly and engaging by using best practices in UX/UI design.\n",
            "\n",
            "Also, remember that creating an e-learning platform involves several legal and ethical considerations, including data privacy and intellectual property rights. Make sure to comply with all applicable laws and regulations.</s>\n",
            "\n",
            "Rejected sample 74 of the raw training set:\n",
            "\n",
            "Here's a suggested high-level overview of the development workflow for building an e-learning platform with multimedia content and interactive quizzes using the Laravel framework:\n",
            "\n",
            "1. Planning and requirements gathering:\n",
            "\n",
            "\tDefine the scope of the project, including the features, functionality, and requirements for the e-learning platform. This will help you prioritize and plan the development stages.\n",
            "\n",
            "\tDetermine the technology stack, including the programming language, framework, and any libraries or third-party tools that will be used.\n",
            "\n",
            "\tDecide on the database structure for the platform, including the entities and relationships between them.\n",
            "\n",
            "\tPlan the user authentication and authorization process, including the registration and login flows, and the roles and permissions required for different user groups.\n",
            "\n",
            "2. Development:\n",
            "\n",
            "\tImplement the backend functionality, including the courses, multimedia content, and quizzes.\n",
            "\n",
            "\tImplement the user authentication and authorization system.\n",
            "\n",
            "\tImplement the frontend website, including the user dashboard, course pages, and quiz pages.\n",
            "\n",
            "\tIntegrate the multimedia content storage and streaming service, such as Amazon S3 or YouTube.\n",
            "\n",
            "\tImplement the quizzes, including the question bank, question selection, and scoring.\n",
            "\n",
            "3. Testing:\n",
            "\n",
            "\tWrite unit tests for the backend functionality, including the courses, multimedia content, and quizzes.\n",
            "\n",
            "\tWrite integration tests to test the backend interactions with the database and other systems.\n",
            "\n",
            "\tCreate automated tests for the frontend website, including UI tests and end-to-end tests.\n",
            "\n",
            "\tCreate user acceptance tests to test the platform functionality from the user's perspective.\n",
            "\n",
            "4. Deployment:\n",
            "\n",
            "\tPrepare the platform for deployment, including configuration, security, and performance tuning.\n",
            "\n",
            "\tDeploy the platform to the production environment.\n",
            "\n",
            "\tImplement continuous integration and continuous deployment (CI/CD) pipelines to automate the deployment process.\n",
            "\n",
            "5. Maintenance and support:\n",
            "\n",
            "\tContinue to maintain and support the e-learning platform, including bug fixes, enhancements, and feature updates.\n",
            "\n",
            "Note: This is a general overview and the specific stages and details may vary depending on the project scope and requirements. It's always a good idea to break the development process down into smaller, manageable stages, with clear milestones and deliverables, for a more efficient and effective workflow.</s>\n",
            "\n",
            "Prompt sample 31 of the raw training set:\n",
            "\n",
            "<|system|>\n",
            "</s>\n",
            "<|user|>\n",
            "Reasoning: Work done by A in 2 days = 2/5\n",
            "Remaining work = 3/5\n",
            "Work done by both A and B in one day = 1/5 + 1/10 = 3/10\n",
            "Remaining work = 3/5 * 10/3 = 2 days.\n",
            "Ans: (B)\n",
            "Question: A and B can do a work in 5 days and 10 days respectively. A starts the work and B joins him after 2 days. In how many days can they complete the remaining work?\n",
            "Options:\n",
            "(A) 6 days\n",
            "(B) 2 days\n",
            "(C) 8 days\n",
            "(D) 3 days\n",
            "(E) 9 days\n",
            "\n",
            "Rationale: ∴ 6x + 3x + 2x = 132 ⇒ x = 12. Second number = 3x = 3×12 = 36.\n",
            "The answer: (E)\n",
            "Question: Let the second number be 3x, so that the first number is 6x and the third number is 2x.\n",
            "Options:\n",
            "(A) 22\n",
            "(B) 77\n",
            "(C) 98\n",
            "(D) 27\n",
            "(E) 36\n",
            "\n",
            "Reasoning: 4 shirts x 5 pants x 2 belts = 40\n",
            "Ans: (C)\n",
            "Question: A woman has 4 shirts, 5 pants and 2 belts. How many different clothing combinations can she wear?\n",
            "Options:\n",
            "(A) 11\n",
            "(B) 20\n",
            "(C) 40\n",
            "(D) 60\n",
            "(E) 4\n",
            "\n",
            "Rationale: If he draws any combination of 3 socks he will definitely have the matching pair of either colour.\n",
            "The answer: (A)\n",
            "Question: John has 10pairs of dark blue socks and 10pairs of black socks. He keeps them all in the same bag. If he picks out 3socks at random, then what is the probability thathe will get a matching pair?\n",
            "Options:\n",
            "(A) 1\n",
            "(B) 3\n",
            "(C) 4\n",
            "(D) 5\n",
            "(E) 6\n",
            "\n",
            "Reasoning: Total age of players in team = 22 x 11 = 242\n",
            "When coach is included, total members = 12, Avg age increases by one becomes 23\n",
            "Total age when coach is included = 23x12 = 276\n",
            "Age of coach = 276-242 = 34 yrs.\n",
            "Ans: (B)\n",
            "Question: The average age of the district level hockey team of eleven is 22 years.The average age gets increased by 1 year when the coach age is also included.what is the age of the coach ?\n",
            "Options:\n",
            "(A) 22 yrs\n",
            "(B) 34 yrs\n",
            "(C) 27 yrs\n",
            "(D) 28 yrs\n",
            "(E) 19 yrs\n",
            "\n",
            "Reasoning: 91 is divisible by 7. So, it is not a prime number.\n",
            "Ans: (D)\n",
            "</s>\n",
            "<|assistant|>\n",
            "\n",
            "Chosen sample 31 of the raw training set:\n",
            "\n",
            "Question: Is 91 a prime number?\n",
            "Options:\n",
            "(A) Yes\n",
            "(B) No\n",
            "(C) Maybe\n",
            "(D) No, it's divisible by 7\n",
            "(E) Yes, it's divisible by 1 and itself\n",
            "\n",
            "Answer: (D) No, it's divisible by 7\n",
            "\n",
            "Confidence: 95%\n",
            "\n",
            "Reasoning: A prime number is a positive integer greater than 1 that is divisible only by 1 and itself. 91 is divisible by 7, so it is not a prime number.</s>\n",
            "\n",
            "Rejected sample 31 of the raw training set:\n",
            "\n",
            "I am not sure yet, is there any way you could help me formulate questions?</s>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# Print a few random samples from the training set:\n",
        "for index in random.sample(range(len(raw_datasets[\"train\"])), 3):\n",
        "    print(f\"Prompt sample {index} of the raw training set:\\n\\n{raw_datasets['train'][index]['prompt']}\")\n",
        "    print(f\"Chosen sample {index} of the raw training set:\\n\\n{raw_datasets['train'][index]['chosen']}\")\n",
        "    print(f\"Rejected sample {index} of the raw training set:\\n\\n{raw_datasets['train'][index]['rejected']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY8Og84AhyDn"
      },
      "source": [
        "## Load SFT model\n",
        "\n",
        "Here we load the supervised fine-tuned (SFT) model (trained during [step 2](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb)). As we used QLoRa during SFT, the [model repository](https://huggingface.co/alignment-handbook/zephyr-7b-sft-qlora) only contains the adapter weights. Hence we first load the base model in 4-bit using the [BitsAndBytes quantization method](https://huggingface.co/docs/transformers/en/main_classes/quantization#transformers.BitsAndBytesConfig), and then load the SFT adapter on top.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UMGylKDAHUv",
        "outputId": "f51f846c-16af-446c-9632-0f0f6360aa7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adapter weights model repo: alignment-handbook/zephyr-7b-sft-lora\n",
            "Base model weights model repo: mistralai/Mistral-7B-v0.1\n"
          ]
        }
      ],
      "source": [
        "from peft import PeftConfig\n",
        "\n",
        "peft_config = PeftConfig.from_pretrained(model_id)\n",
        "print(\"Adapter weights model repo:\", model_id)\n",
        "print(\"Base model weights model repo:\", peft_config.base_model_name_or_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "8a7c1b0abc304bb08f84c6b932e2c36a",
            "2bc4f4a1dc4a496b878170349ee24857",
            "98655a10387447b2b1413e749dbd2500",
            "ab9fadfdf30640a18866d0340ec2d420",
            "a23db35182454106bec5fb2de4be4498",
            "f2bbd93ded184831bab65718289dcd5c",
            "eed75a78ebac41eebbcd6c34f4edea5d",
            "fbe6675ff58c4c24bec31df2bed11b55",
            "efab1deb3690485e9fdbfd366054b9c9",
            "0389900886ad487a803747474ec47cfa",
            "d4fc75db520146d9a6198caef6ffc76f",
            "802e11c78df0417090022f3722295394",
            "6dfbdfd500354d08a3965eb166d20eca",
            "4d5bd9e0b2b44d2d837df8bddc43e9fd",
            "8212228dbe634f3780d28c0e23e1b1cf",
            "acbc262b413c484798645df7a32e828a",
            "3ba33c5846b2425c8c167485a4828d3f",
            "c90c18e473f24939b69ce9f00c9c29bc",
            "8478eb2c797f4edb8243214da8fcfcbd",
            "919a97a044544bf6a366c1b1f2a8eb64",
            "f2784c3f245d4b43981aa9baba29209f",
            "027d8a48a90d4fa39890b925929fcb65",
            "888d670aacb74d0b81094b0a11fea5d5",
            "d7d9ec66a99c44b0b5d3522d2cd6b21f",
            "b4aa9ff60aa84e80b24d06d588b6036e",
            "74b4150010684b428993970d1a7b3bab",
            "ea27d279f121468493ba41a919338060",
            "78deb38d02f64bdb8ac1d6cb275f1a37",
            "576e56179f4d487c9b5aa8c263d79012",
            "560b83d86b5e45f581e2396a67b5e76c",
            "afa23ade16cd45019fbca660e9bff7a4",
            "8ee78671d88f40149a9f2f7f45f76df5",
            "46a1a48eea904ca8a6c3060c9386f015"
          ]
        },
        "id": "33gqLvEQhxd1",
        "outputId": "a2b08565-470b-4a13-e8a7-da108aa3e53f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a7c1b0abc304bb08f84c6b932e2c36a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "802e11c78df0417090022f3722295394",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "888d670aacb74d0b81094b0a11fea5d5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/83.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n",
        "\n",
        "# specify how to quantize the model\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n",
        "\n",
        "# Step 1: load the base model (Mistral-7B in our case) in 4-bit\n",
        "model_kwargs = dict(\n",
        "    # attn_implementation=\"flash_attention_2\", # set this to True if your GPU supports it (Flash Attention drastically speeds up model computations)\n",
        "    torch_dtype=\"auto\",\n",
        "    use_cache=False,  # set to False as we're going to use gradient checkpointing\n",
        "    device_map=device_map,\n",
        "    quantization_config=quantization_config,\n",
        ")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(peft_config.base_model_name_or_path, **model_kwargs)\n",
        "\n",
        "# Step 2: load base model + SFT adapter weights\n",
        "# notice that only the adapter weights are trainable!\n",
        "model = PeftModel.from_pretrained(base_model, model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "350Pvl1E7TDL"
      },
      "source": [
        "Notice how only the adapter layers are trainable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KigQ_QKi6ng1",
        "outputId": "1c89c1e1-804a-426c-cbd7-83d81bf531c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "base_model.model.model.embed_tokens.weight False\n",
            "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.0.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.0.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.0.input_layernorm.weight False\n",
            "base_model.model.model.layers.0.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.1.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.1.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.1.input_layernorm.weight False\n",
            "base_model.model.model.layers.1.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.2.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.2.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.2.input_layernorm.weight False\n",
            "base_model.model.model.layers.2.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.3.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.3.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.3.input_layernorm.weight False\n",
            "base_model.model.model.layers.3.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.4.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.4.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.4.input_layernorm.weight False\n",
            "base_model.model.model.layers.4.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.5.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.5.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.5.input_layernorm.weight False\n",
            "base_model.model.model.layers.5.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.6.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.6.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.6.input_layernorm.weight False\n",
            "base_model.model.model.layers.6.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.7.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.7.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.7.input_layernorm.weight False\n",
            "base_model.model.model.layers.7.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.8.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.8.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.8.input_layernorm.weight False\n",
            "base_model.model.model.layers.8.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.9.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.9.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.9.input_layernorm.weight False\n",
            "base_model.model.model.layers.9.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.10.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.10.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.10.input_layernorm.weight False\n",
            "base_model.model.model.layers.10.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.11.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.11.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.11.input_layernorm.weight False\n",
            "base_model.model.model.layers.11.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.12.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.12.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.12.input_layernorm.weight False\n",
            "base_model.model.model.layers.12.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.13.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.13.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.13.input_layernorm.weight False\n",
            "base_model.model.model.layers.13.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.14.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.14.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.14.input_layernorm.weight False\n",
            "base_model.model.model.layers.14.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.15.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.15.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.15.input_layernorm.weight False\n",
            "base_model.model.model.layers.15.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.16.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.16.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.16.input_layernorm.weight False\n",
            "base_model.model.model.layers.16.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.17.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.17.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.17.input_layernorm.weight False\n",
            "base_model.model.model.layers.17.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.18.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.18.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.18.input_layernorm.weight False\n",
            "base_model.model.model.layers.18.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.19.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.19.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.19.input_layernorm.weight False\n",
            "base_model.model.model.layers.19.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.20.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.20.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.20.input_layernorm.weight False\n",
            "base_model.model.model.layers.20.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.21.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.21.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.21.input_layernorm.weight False\n",
            "base_model.model.model.layers.21.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.22.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.22.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.22.input_layernorm.weight False\n",
            "base_model.model.model.layers.22.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.23.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.23.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.23.input_layernorm.weight False\n",
            "base_model.model.model.layers.23.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.24.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.24.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.24.input_layernorm.weight False\n",
            "base_model.model.model.layers.24.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.25.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.25.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.25.input_layernorm.weight False\n",
            "base_model.model.model.layers.25.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.26.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.26.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.26.input_layernorm.weight False\n",
            "base_model.model.model.layers.26.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.27.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.27.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.27.input_layernorm.weight False\n",
            "base_model.model.model.layers.27.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.28.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.28.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.28.input_layernorm.weight False\n",
            "base_model.model.model.layers.28.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.29.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.29.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.29.input_layernorm.weight False\n",
            "base_model.model.model.layers.29.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.30.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.30.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.30.input_layernorm.weight False\n",
            "base_model.model.model.layers.30.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.31.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.31.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.31.input_layernorm.weight False\n",
            "base_model.model.model.layers.31.post_attention_layernorm.weight False\n",
            "base_model.model.model.norm.weight False\n",
            "base_model.model.lm_head.weight False\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "  print(name, param.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AlKMUUcfbs1"
      },
      "source": [
        "## Define DPOTrainer\n",
        "\n",
        "Next, we define the training arguments and instantiate a [DPOTrainer](https://huggingface.co/docs/trl/main/en/dpo_trainer) class which will handle fine-tuning for us.\n",
        "\n",
        "Note that in this case, we leverage the [DPO](https://arxiv.org/abs/2305.18290) (direct preference optimization) method, which is one of the best methods for human preference fine-tuning at the time of writing. Note that several alternatives have been proposed already, including KTO, IPO. The `DPOTrainer` [also supports](https://huggingface.co/docs/trl/main/en/dpo_trainer#loss-functions) these. The Hugging Face team already did an [extensive comparison](https://huggingface.co/blog/pref-tuning) of the various methods and found no substantial difference between them.\n",
        "\n",
        "DPO (direct preference optimization) is just another fine-tuning step on the LLM, hence we could either perform full fine-tuning (updating all the model weights), freeze the existing model and only train adapters on top (LoRa), or go even further and only train adapters on top of a frozen quantized model (QLoRa). The same techniques apply as during SFT.\n",
        "\n",
        "Interestingly, as taken from the [Alignment Handbook README](https://github.com/huggingface/alignment-handbook/tree/main/scripts):\n",
        "\n",
        "> In practice, we find comparable performance for both full and QLoRA fine-tuning, with the latter having the advantage of producing small adapter weights that are fast to upload and download from the Hugging Face Hub.\n",
        "\n",
        "For full fine-tuning, you would need approximately 126GB of GPU RAM for a 7B model (hence one typically uses multiple A100s). With QLoRa, you only need about 7GB! In this case, as we're running on an RTX 4090 which has 24GB of RAM, we will use [QLoRa](https://huggingface.co/blog/4bit-transformers-bitsandbytes), which is the most memory efficient.\n",
        "\n",
        "Hence, we pass a `peft_config` to DPOTrainer, making sure that adapter layers are added on top in bfloat16. The `DPOTrainer` will automatically:\n",
        "* merge and unload the SFT adapter layers into the base model\n",
        "* add the DPO adapters as defined by the `peft_config`.\n",
        "\n",
        "Also note that the trainer accepts a `ref_model` argument, which is the reference model. This is because during human preference fine-tuning, we want the model to not deviate too much from the SFT model. Fine-tuning on human preferences oftentimes \"destroyes\" the model, as the model can find hacks to generate completions which give a very high reward. Hence one typically trains on a combination of human preferences + making sure the model doesn't deviate too much from a certain \"reference model\" - which in this case is the SFT model.\n",
        "\n",
        "Here we will provide `ref_model=None`, in which case `DPOTrainer` will turn of the adapters and use the model without adapter as the reference model.\n",
        "\n",
        "We also leverage several well-known techniques for maximizing performance on a single GPU: gradient checkpointing, gradient accumulation, mixed precision training in bfloat16. Refer to [this guide](https://huggingface.co/docs/transformers/v4.20.1/en/perf_train_gpu_one) for all the details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UG5VOPQkfcXi"
      },
      "outputs": [],
      "source": [
        "from trl import DPOTrainer\n",
        "from peft import LoraConfig\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# path where the Trainer will save its checkpoints and logs\n",
        "output_dir = 'data/zephyr-7b-dpo-lora'\n",
        "\n",
        "# based on config\n",
        "training_args = TrainingArguments(\n",
        "    bf16=True,\n",
        "    beta=0.01,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\":False},\n",
        "    hub_model_id=\"zephyr-7b-dpo-qlora\",\n",
        "    learning_rate=5.0e-6,\n",
        "    log_level=\"info\",\n",
        "    logging_steps=10,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    max_length=1024,\n",
        "    max_prompt_length=512,\n",
        "    num_train_epochs=1,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    output_dir=output_dir,  # It is handy to append `hub_model_revision` to keep track of your local experiments\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=8,\n",
        "    # push_to_hub=True,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=1,\n",
        "    seed=42,\n",
        "    warmup_ratio=0.1,\n",
        ")\n",
        "\n",
        "# based on the recipe: https://github.com/huggingface/alignment-handbook/blob/main/recipes/zephyr-7b-beta/dpo/config_qlora.yaml\n",
        "peft_config = LoraConfig(\n",
        "        r=128,\n",
        "        lora_alpha=128,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\",  \"up_proj\",  \"down_proj\"],\n",
        ")\n",
        "\n",
        "trainer = DPOTrainer(\n",
        "        model,\n",
        "        ref_model=None,\n",
        "        model_init_kwargs=None,\n",
        "        ref_model_init_kwargs=None,\n",
        "        args=training_args,\n",
        "        beta=training_args.beta,\n",
        "        train_dataset=raw_datasets[\"train\"],\n",
        "        eval_dataset=raw_datasets[\"test\"],\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=training_args.max_length,\n",
        "        max_prompt_length=training_args.max_prompt_length,\n",
        "        peft_config=peft_config,\n",
        "        loss_type=training_args.loss_type,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGldALxQIwYu"
      },
      "source": [
        "## Train!\n",
        "\n",
        "Finally, training is as simple as calling trainer.train()!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgEnI5KMIwyt"
      },
      "outputs": [],
      "source": [
        "train_result = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xxjryHNBKD6"
      },
      "source": [
        "## Saving the model\n",
        "\n",
        "Next, we save the Trainer's state. We also add the number of training samples to the logs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ai5jXhJBMsj"
      },
      "outputs": [],
      "source": [
        "metrics = train_result.metrics\n",
        "max_train_samples = training_args.max_train_samples if training_args.max_train_samples is not None else len(raw_datasets[\"train\"])\n",
        "metrics[\"train_samples\"] = min(max_train_samples, len(raw_datasets[\"train\"]))\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tCZxj1tBNAc"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Let's generate some new texts with our trained model.\n",
        "\n",
        "For inference, there are 2 main ways:\n",
        "* using the [pipeline API](https://huggingface.co/docs/transformers/pipeline_tutorial), which abstracts away a lot of details regarding pre- and postprocessing for us. [This model card](https://huggingface.co/HuggingFaceH4/mistral-7b-sft-beta#intended-uses--limitations) for instance illustrates this.\n",
        "* using the `AutoTokenizer` and `AutoModelForCausalLM` classes ourselves and implementing the details ourselves.\n",
        "\n",
        "Let us do the latter, so that we understand what's going on.\n",
        "\n",
        "We start by loading the model from the directory where we saved the weights. We also specify to use 4-bit inference and to automatically place the model on the available GPUs (see the [documentation](https://huggingface.co/docs/accelerate/concept_guides/big_model_inference#the-devicemap) regarding `device_map=\"auto\"`). The AutoModelForCausalLM class will automatically load the base model and DPO adapter thanks to the [PEFT integration](https://huggingface.co/docs/peft/tutorial/peft_integrations#transformers) in the Transformers library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiRvmsSkyubH"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "model = AutoModelForCausalLM.from_pretrained(output_dir, load_in_4bit=True, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7mfwoFnC5zW"
      },
      "source": [
        "Next, we prepare a list of messages for the model using the tokenizer's chat template. Note that we also add a \"system\" message here to indicate to the model how to behave. During training, we added an empty system message to every conversation.\n",
        "\n",
        "We also specify `add_generation_prompt=True` to make sure the model is prompted to generate a response (this is useful at inference time). We specify \"cuda\" to move the inputs to the GPU. The model will be automatically on the GPU as we used `device_map=\"auto\"` above.\n",
        "\n",
        "Next, we use the [generate()](https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to autoregressively generate the next token IDs, one after the other. Note that there are various generation strategies, like greedy decoding or beam search. Refer to [this blog post](https://huggingface.co/blog/how-to-generate) for all details. Here we use sampling.\n",
        "\n",
        "Finally, we use the batch_decode method of the tokenizer to turn the generated token IDs back into strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hkacv5PvBOvE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
        "]\n",
        "\n",
        "# prepare the messages for the model\n",
        "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# inference\n",
        "outputs = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95\n",
        ")\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "027d8a48a90d4fa39890b925929fcb65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0389900886ad487a803747474ec47cfa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bc4f4a1dc4a496b878170349ee24857": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2bbd93ded184831bab65718289dcd5c",
            "placeholder": "​",
            "style": "IPY_MODEL_eed75a78ebac41eebbcd6c34f4edea5d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3ba33c5846b2425c8c167485a4828d3f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46a1a48eea904ca8a6c3060c9386f015": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d5bd9e0b2b44d2d837df8bddc43e9fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8478eb2c797f4edb8243214da8fcfcbd",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_919a97a044544bf6a366c1b1f2a8eb64",
            "value": 116
          }
        },
        "560b83d86b5e45f581e2396a67b5e76c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "576e56179f4d487c9b5aa8c263d79012": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6dfbdfd500354d08a3965eb166d20eca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ba33c5846b2425c8c167485a4828d3f",
            "placeholder": "​",
            "style": "IPY_MODEL_c90c18e473f24939b69ce9f00c9c29bc",
            "value": "generation_config.json: 100%"
          }
        },
        "74b4150010684b428993970d1a7b3bab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ee78671d88f40149a9f2f7f45f76df5",
            "placeholder": "​",
            "style": "IPY_MODEL_46a1a48eea904ca8a6c3060c9386f015",
            "value": " 83.9M/83.9M [00:00&lt;00:00, 209MB/s]"
          }
        },
        "78deb38d02f64bdb8ac1d6cb275f1a37": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "802e11c78df0417090022f3722295394": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6dfbdfd500354d08a3965eb166d20eca",
              "IPY_MODEL_4d5bd9e0b2b44d2d837df8bddc43e9fd",
              "IPY_MODEL_8212228dbe634f3780d28c0e23e1b1cf"
            ],
            "layout": "IPY_MODEL_acbc262b413c484798645df7a32e828a"
          }
        },
        "8212228dbe634f3780d28c0e23e1b1cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2784c3f245d4b43981aa9baba29209f",
            "placeholder": "​",
            "style": "IPY_MODEL_027d8a48a90d4fa39890b925929fcb65",
            "value": " 116/116 [00:00&lt;00:00, 10.3kB/s]"
          }
        },
        "8478eb2c797f4edb8243214da8fcfcbd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "888d670aacb74d0b81094b0a11fea5d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7d9ec66a99c44b0b5d3522d2cd6b21f",
              "IPY_MODEL_b4aa9ff60aa84e80b24d06d588b6036e",
              "IPY_MODEL_74b4150010684b428993970d1a7b3bab"
            ],
            "layout": "IPY_MODEL_ea27d279f121468493ba41a919338060"
          }
        },
        "8a7c1b0abc304bb08f84c6b932e2c36a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2bc4f4a1dc4a496b878170349ee24857",
              "IPY_MODEL_98655a10387447b2b1413e749dbd2500",
              "IPY_MODEL_ab9fadfdf30640a18866d0340ec2d420"
            ],
            "layout": "IPY_MODEL_a23db35182454106bec5fb2de4be4498"
          }
        },
        "8ee78671d88f40149a9f2f7f45f76df5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "919a97a044544bf6a366c1b1f2a8eb64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "98655a10387447b2b1413e749dbd2500": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbe6675ff58c4c24bec31df2bed11b55",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_efab1deb3690485e9fdbfd366054b9c9",
            "value": 2
          }
        },
        "a23db35182454106bec5fb2de4be4498": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab9fadfdf30640a18866d0340ec2d420": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0389900886ad487a803747474ec47cfa",
            "placeholder": "​",
            "style": "IPY_MODEL_d4fc75db520146d9a6198caef6ffc76f",
            "value": " 2/2 [00:09&lt;00:00,  4.34s/it]"
          }
        },
        "acbc262b413c484798645df7a32e828a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afa23ade16cd45019fbca660e9bff7a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4aa9ff60aa84e80b24d06d588b6036e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_560b83d86b5e45f581e2396a67b5e76c",
            "max": 83946192,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_afa23ade16cd45019fbca660e9bff7a4",
            "value": 83946192
          }
        },
        "c90c18e473f24939b69ce9f00c9c29bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4fc75db520146d9a6198caef6ffc76f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7d9ec66a99c44b0b5d3522d2cd6b21f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78deb38d02f64bdb8ac1d6cb275f1a37",
            "placeholder": "​",
            "style": "IPY_MODEL_576e56179f4d487c9b5aa8c263d79012",
            "value": "adapter_model.safetensors: 100%"
          }
        },
        "ea27d279f121468493ba41a919338060": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eed75a78ebac41eebbcd6c34f4edea5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efab1deb3690485e9fdbfd366054b9c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f2784c3f245d4b43981aa9baba29209f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2bbd93ded184831bab65718289dcd5c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbe6675ff58c4c24bec31df2bed11b55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
