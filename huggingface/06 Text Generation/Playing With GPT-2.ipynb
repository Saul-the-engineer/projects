{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Models with GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's play with gpt2 and gpt2-xl\n",
    "Note that we can use the Auto functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2'\n",
    "model_name = 'gpt2-xl'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input sequence: \n",
      "Machine learning with PyTorch can do amazing\n"
     ]
    }
   ],
   "source": [
    "seq = \"Machine learning with PyTorch can do amazing\"\n",
    "print(\"\\nInput sequence: \")\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized input data structure: \n",
      "{'input_ids': tensor([[37573,  4673,   351,  9485, 15884,   354,   460,   466,  4998]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(seq, return_tensors=\"pt\").to(device)\n",
    "print(\"\\nTokenized input data structure: \")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token IDs and their words: \n",
      "tensor(37573, device='cuda:0') Machine\n",
      "tensor(4673, device='cuda:0')  learning\n",
      "tensor(351, device='cuda:0')  with\n",
      "tensor(9485, device='cuda:0')  Py\n",
      "tensor(15884, device='cuda:0') Tor\n",
      "tensor(354, device='cuda:0') ch\n",
      "tensor(460, device='cuda:0')  can\n",
      "tensor(466, device='cuda:0')  do\n",
      "tensor(4998, device='cuda:0')  amazing\n"
     ]
    }
   ],
   "source": [
    "input_ids = inputs[\"input_ids\"]  # just IDS, no attn mask\n",
    "print(\"\\nToken IDs and their words: \")\n",
    "for id in input_ids[0]:\n",
    "  word = tokenizer.decode(id)\n",
    "  print(id, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's run it through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All logits for next word: \n",
      "tensor([[ 3.5669,  1.4470, -3.1922,  ..., -6.6968, -7.5917,  0.0593]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  logits = model(**inputs).logits[:, -1, :]\n",
    "print(\"\\nAll logits for next word: \")\n",
    "print(logits)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All probabilities: \n",
      "tensor([[2.5635e-05, 3.0775e-06, 2.9744e-08,  ..., 8.9411e-10, 3.6535e-10,\n",
      "         7.6823e-07]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "probs = torch.softmax(logits, dim=-1)\n",
    "print(\"\\nAll probabilities: \")\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Token ID</th>\n",
       "      <td>1243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logits</th>\n",
       "      <td>tensor(14.0662)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Probability</th>\n",
       "      <td>tensor(0.9303)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted Word</th>\n",
       "      <td>things</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Value\n",
       "Token ID                   1243\n",
       "Logits          tensor(14.0662)\n",
       "Probability      tensor(0.9303)\n",
       "Predicted Word           things"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_id = torch.argmax(logits).item()\n",
    "pred_word = tokenizer.decode(pred_id)\n",
    "pd.DataFrame([pred_id, logits[0, pred_id].cpu(), probs[0, pred_id].cpu(), pred_word], \n",
    "              index=['Token ID', 'Logits', 'Probability', 'Predicted Word'], columns =['Value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look a bit closer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Choice 1</th>\n",
       "      <th>Choice 2</th>\n",
       "      <th>Choice 3</th>\n",
       "      <th>Choice 4</th>\n",
       "      <th>Choice 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transformers are the</td>\n",
       "      <td>(15.07%)</td>\n",
       "      <td>ills (14.52%)</td>\n",
       "      <td>________ (7.28%)</td>\n",
       "      <td>icky (4.74%)</td>\n",
       "      <td>_____ (4.51%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transformers are the</td>\n",
       "      <td>most (7.08%)</td>\n",
       "      <td>ultimate (4.51%)</td>\n",
       "      <td>original (2.48%)</td>\n",
       "      <td>\" (1.87%)</td>\n",
       "      <td>main (1.79%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transformers are the  most</td>\n",
       "      <td>popular (17.39%)</td>\n",
       "      <td>common (5.30%)</td>\n",
       "      <td>powerful (5.28%)</td>\n",
       "      <td>famous (4.39%)</td>\n",
       "      <td>successful (2.82%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transformers are the  most popular</td>\n",
       "      <td>toy (10.29%)</td>\n",
       "      <td>toys (6.68%)</td>\n",
       "      <td>Transformers (6.46%)</td>\n",
       "      <td>of (5.72%)</td>\n",
       "      <td>and (5.04%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transformers are the  most popular toy</td>\n",
       "      <td>line (50.22%)</td>\n",
       "      <td>in (9.74%)</td>\n",
       "      <td>of (7.60%)</td>\n",
       "      <td>lines (5.07%)</td>\n",
       "      <td>line (4.07%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transformers are the  most popular toy line</td>\n",
       "      <td>in (36.55%)</td>\n",
       "      <td>of (14.28%)</td>\n",
       "      <td>, (5.79%)</td>\n",
       "      <td>on (3.88%)</td>\n",
       "      <td>. (3.41%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Transformers are the  most popular toy line in</td>\n",
       "      <td>the (71.44%)</td>\n",
       "      <td>history (7.64%)</td>\n",
       "      <td>America (4.29%)</td>\n",
       "      <td>Japan (2.48%)</td>\n",
       "      <td>all (1.15%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Transformers are the  most popular toy line in...</td>\n",
       "      <td>world (68.34%)</td>\n",
       "      <td>US (4.86%)</td>\n",
       "      <td>history (3.50%)</td>\n",
       "      <td>universe (3.18%)</td>\n",
       "      <td>United (2.54%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Transformers are the  most popular toy line in...</td>\n",
       "      <td>. (32.81%)</td>\n",
       "      <td>, (32.00%)</td>\n",
       "      <td>and (10.73%)</td>\n",
       "      <td>( (2.07%)</td>\n",
       "      <td>with (1.94%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Transformers are the  most popular toy line in...</td>\n",
       "      <td>(11.34%)</td>\n",
       "      <td>They (10.70%)</td>\n",
       "      <td>(8.73%)</td>\n",
       "      <td>The (5.84%)</td>\n",
       "      <td>I (4.05%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Input           Choice 1  \\\n",
       "0                              Transformers are the            (15.07%)   \n",
       "1                             Transformers are the         most (7.08%)   \n",
       "2                         Transformers are the  most   popular (17.39%)   \n",
       "3                 Transformers are the  most popular       toy (10.29%)   \n",
       "4             Transformers are the  most popular toy      line (50.22%)   \n",
       "5        Transformers are the  most popular toy line        in (36.55%)   \n",
       "6     Transformers are the  most popular toy line in       the (71.44%)   \n",
       "7  Transformers are the  most popular toy line in...     world (68.34%)   \n",
       "8  Transformers are the  most popular toy line in...         . (32.81%)   \n",
       "9  Transformers are the  most popular toy line in...           (11.34%)   \n",
       "\n",
       "           Choice 2               Choice 3           Choice 4  \\\n",
       "0     ills (14.52%)       ________ (7.28%)       icky (4.74%)   \n",
       "1  ultimate (4.51%)       original (2.48%)          \" (1.87%)   \n",
       "2    common (5.30%)       powerful (5.28%)     famous (4.39%)   \n",
       "3      toys (6.68%)   Transformers (6.46%)         of (5.72%)   \n",
       "4        in (9.74%)             of (7.60%)      lines (5.07%)   \n",
       "5       of (14.28%)              , (5.79%)         on (3.88%)   \n",
       "6   history (7.64%)        America (4.29%)      Japan (2.48%)   \n",
       "7        US (4.86%)        history (3.50%)   universe (3.18%)   \n",
       "8        , (32.00%)           and (10.73%)          ( (2.07%)   \n",
       "9     They (10.70%)                (8.73%)        The (5.84%)   \n",
       "\n",
       "              Choice 5  \n",
       "0        _____ (4.51%)  \n",
       "1         main (1.79%)  \n",
       "2   successful (2.82%)  \n",
       "3          and (5.04%)  \n",
       "4         line (4.07%)  \n",
       "5            . (3.41%)  \n",
       "6          all (1.15%)  \n",
       "7       United (2.54%)  \n",
       "8         with (1.94%)  \n",
       "9            I (4.05%)  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#input_txt = \"Transformers are the\"\n",
    "input_txt = \"Transformers are the \"                    # This one is interesting to see how things change\n",
    "#input_txt = \"Transformers are built using the\"         # Note the word pieces\n",
    "\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "iterations = []\n",
    "n_steps = 10\n",
    "choices_per_step = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_steps):\n",
    "        iteration = dict()\n",
    "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "        output = model(input_ids=input_ids)\n",
    "        \n",
    "        # Select logits of the first batch and the last token and apply softmax to get the probability\n",
    "        next_token_logits = output.logits[0, -1, :]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "        \n",
    "        # Store tokens with highest probabilities in our little table\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "            token_choice = (\n",
    "                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n",
    "            )\n",
    "            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "        iterations.append(iteration)\n",
    "\n",
    "            \n",
    "        # Append predicted next token to input\n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "\n",
    "pd.DataFrame(iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The generate method runs the transformer several steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are the  most popular toy line in the world. \n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The researchers, from the University of California, Davis, and the University of Colorado, Boulder, were conducting a study on the Andean cloud forest, which is home to the rare species of cloud forest trees.\n",
      "\n",
      "\n",
      "The researchers were surprised to find that the unicorns were able to communicate with each other, and even with humans.\n",
      "\n",
      "\n",
      "The researchers were surprised to find that the unicorns were able\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "input_txt = \"\"\"In a shocking finding, scientist discovered \\\n",
    "a herd of unicorns living in a remote, previously unexplored \\\n",
    "valley, in the Andes Mountains. Even more surprising to the \\\n",
    "researchers was the fact that the unicorns spoke perfect English.\\n\\n\n",
    "\"\"\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play around with sampling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some scoring functions\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def log_probs_from_logits(logits, labels):\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logp_label\n",
    "\n",
    "def sequence_logprob(model, labels, input_len=0):\n",
    "    with torch.no_grad():\n",
    "        output = model(labels)\n",
    "        log_probs = log_probs_from_logits(\n",
    "            output.logits[:, :-1, :], labels[:, 1:])\n",
    "        seq_log_prob = torch.sum(log_probs[:, input_len:])\n",
    "    return seq_log_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = model.generate(input_ids, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy walking with my cute dog, and I love to read. I'm a big fan of the Harry Potter series, and I'm a huge fan of the Harry Potter movies. I'm a huge fan of the Harry Potter books, and I\n",
      "\n",
      "log-prob: -51.03\n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, greedy_output, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(greedy_output[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "beam_output = model.generate(\n",
    "    input_ids, \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy walking with my cute dog, but I don't want to walk with him in the rain. I don't want to walk in the rain. I don't want to walk in the rain. I don't want to walk in the rain\n",
      "\n",
      "log-prob: -31.37\n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, beam_output, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(beam_output[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids, \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy walking with my cute dog, but I don't want to walk with him in the rain.\"\n",
      "\n",
      "\"I'm not sure if it's a good idea to take my dog to the park,\" said one woman. \"I think it\n",
      "\n",
      "log-prob: -57.65\n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, beam_output, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(beam_output[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling within the probability distribution of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "torch.random.manual_seed(42)\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy walking with my cute dog, Sophia, and protagonist Alex Miller, and had the opportunity to see the original Mathers series with my son on the Auctus trail.\n",
      "\n",
      "Trivia Edit\n",
      "\n",
      "A prequel called Ashes was released\n",
      "\n",
      "log-prob: -159.56\n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, sample_output, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(sample_output[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change the temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=0, \n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy walking with my cute dog and seeing the different types of plants close to me,\" said Deen, who lives in the Guadalupe Heights neighborhood. \"I enjoyed it at the Festival of Trees, too.\"\n",
      "\n",
      "The festival, which\n",
      "\n",
      "log-prob: -98.73\n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, sample_output, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(sample_output[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top-K sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy walking with my cute dog in the summer!\n",
      "\n",
      "Is my place warm?\n",
      "\n",
      "Please don't hesitate to contact me at any point if you have any questions about my furniture, or would like a quote on a new piece. I\n",
      "\n",
      "log-prob: -85.71\n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, sample_output, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(sample_output[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top-P sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# deactivate top_k sampling and sample only from 92% most likely words\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_p=0.92, \n",
    "    top_k=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy walking with my cute dog and cat, and biking. I never miss a moment of the summer, because you should feel good about your body and love the sun. When I am eating healthy, I never feel hungry. I find balance by\n",
      "\n",
      "log-prob: -122.94\n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, sample_output, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(sample_output[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining Top-K and Top-P Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=25, \n",
    "    top_p=0.95, \n",
    "    num_return_sequences=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   40,  2883,  6155,   351,   616, 13779,  3290,   357,    64,  3498,\n",
       "          5022,     8,   290, 13226,   262,  4167,   290,  5897,   286,   262,\n",
       "          8222,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
       "        [   40,  2883,  6155,   351,   616, 13779,  3290,   319,   616,  2166,\n",
       "         33179,    13,   383,  6232,   318,  2407,  5897,    13,   314,   423,\n",
       "           587,  6155,  1363,   422,   670,   351,   616,  3290,   329,   262,\n",
       "           938,  1178, 12513,   290,   523,  1290,    11,  2147,   503,   286,\n",
       "           262,  8850,    13,   198,   198,    40,  2107,   287,   262,  1748],\n",
       "        [   40,  2883,  6155,   351,   616, 13779,  3290,    11,   475,   314,\n",
       "           836,   470,   588,  2491,    11,   780,   340,  1838,   616,  7405,\n",
       "         19597,    13,   314,   588,  2491,   319,   257, 49246,   290,  2712,\n",
       "          9669,    11,   290,   314,   588,  6155,   319,   257,  7161,    13,\n",
       "           887,   314,   836,   470,   588,   852,   503,   287,   262,  4252]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog (a Lab mix) and enjoying the peace and quiet of the forest.\n",
      "\n",
      "log-prob: -502.72\n",
      "\n",
      "1: I enjoy walking with my cute dog on my front porch. The neighborhood is quite quiet. I have been walking home from work with my dog for the last few nights and so far, nothing out of the ordinary.\n",
      "\n",
      "I live in the city\n",
      "\n",
      "log-prob: -159.52\n",
      "\n",
      "2: I enjoy walking with my cute dog, but I don't like running, because it makes my legs sore. I like running on a treadmill and playing basketball, and I like walking on a bike. But I don't like being out in the sun\n",
      "\n",
      "log-prob: -79.76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
    "    logp = sequence_logprob(model, sample_outputs, input_len=len(input_ids[0]))\n",
    "    sample_outputs = sample_outputs[1:, :]\n",
    "    print(f\"\\nlog-prob: {logp:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
