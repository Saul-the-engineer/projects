{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7296263",
   "metadata": {},
   "source": [
    "# Comprehensive Guide to Building Reward Models for RLHF\n",
    "\n",
    "This notebook provides an **exhaustive** implementation of a GPT-2-based reward model for Reinforcement Learning from Human Feedback (RLHF). Unlike simpler approaches like GRPO (Group Relative Policy Optimization) that eliminate the need for explicit reward models, this implementation focuses on the traditional **PPO + Reward Model** paradigm that has proven successful in systems like ChatGPT, Claude, and other state-of-the-art language models.\n",
    "\n",
    "## üß† Theoretical Foundation\n",
    "\n",
    "### Why Reward Models?\n",
    "While recent methods like GRPO show promise by comparing outputs within groups and eliminating separate reward models, the **reward model approach** remains the gold standard for several critical reasons:\n",
    "\n",
    "1. **Scalability**: Reward models can be trained once and reused across multiple policy training runs\n",
    "2. **Interpretability**: Explicit reward scores provide clear signals for debugging and analysis  \n",
    "3. **Flexibility**: Can incorporate diverse preference data from multiple sources and modalities\n",
    "4. **Proven Track Record**: Powers the most successful deployed language models in production\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "#### The Bradley-Terry Model for Preferences\n",
    "Human preferences can be modeled using the **Bradley-Terry model**, which assumes that the probability of preferring completion $y_w$ (winner) over completion $y_l$ (loser) given prompt $x$ follows:\n",
    "\n",
    "$P(y_w \\succ y_l | x) = \\frac{\\exp(r_\\theta(x, y_w))}{\\exp(r_\\theta(x, y_w)) + \\exp(r_\\theta(x, y_l))} = \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l))$\n",
    "\n",
    "Where:\n",
    "- $r_\\theta(x, y)$ is our learned reward function parameterized by $\\theta$\n",
    "- $\\sigma(\\cdot)$ is the sigmoid function: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "- $y_w \\succ y_l$ denotes that $y_w$ is preferred over $y_l$\n",
    "\n",
    "#### Loss Function Derivation\n",
    "The negative log-likelihood loss for the Bradley-Terry model becomes:\n",
    "\n",
    "$\\mathcal{L}_{RM}(\\theta) = -\\mathbb{E}_{(x,y_w,y_l) \\sim \\mathcal{D}} \\left[ \\log \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l)) \\right]$\n",
    "\n",
    "This can be rewritten as:\n",
    "$\\mathcal{L}_{RM}(\\theta) = \\mathbb{E}_{(x,y_w,y_l) \\sim \\mathcal{D}} \\left[ \\log(1 + \\exp(r_\\theta(x, y_l) - r_\\theta(x, y_w))) \\right]$\n",
    "\n",
    "#### Architecture Design\n",
    "Our reward model $r_\\theta(x, y)$ is constructed as:\n",
    "\n",
    "$r_\\theta(x, y) = \\text{Linear}(\\text{GPT-2}_\\theta(\\text{concat}(x, y))_{[\\text{EOS}]})$\n",
    "\n",
    "Where:\n",
    "- $\\text{concat}(x, y)$ represents the concatenation of prompt and completion\n",
    "- $[\\text{EOS}]$ indicates we extract the final token's hidden state\n",
    "- $\\text{Linear}(\\cdot)$ is a single linear layer: $\\mathbb{R}^{d_{\\text{model}}} \\rightarrow \\mathbb{R}$\n",
    "\n",
    "### Comparison with Alternative Approaches\n",
    "\n",
    "#### GRPO vs Reward Models\n",
    "**GRPO (Group Relative Policy Optimization)** eliminates reward models by using within-group comparisons:\n",
    "\n",
    "$A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\ldots, r_G\\})}$\n",
    "\n",
    "**Advantages of GRPO:**\n",
    "- No separate reward model training phase\n",
    "- Reduced computational overhead\n",
    "- Built-in variance reduction through group normalization\n",
    "\n",
    "**Why We Still Use Reward Models:**\n",
    "- **Reusability**: Train once, use for multiple policy iterations\n",
    "- **Data Efficiency**: Can leverage large-scale preference datasets\n",
    "- **Robustness**: More stable training dynamics for complex tasks\n",
    "- **Interpretability**: Explicit reward values enable better analysis\n",
    "\n",
    "## üèóÔ∏è Implementation Strategy\n",
    "\n",
    "### Architecture Components\n",
    "1. **Base Model**: GPT-2 transformer for sequence encoding\n",
    "2. **Value Head**: Single linear layer for scalar reward prediction  \n",
    "3. **Training Loop**: Bradley-Terry loss optimization with preference pairs\n",
    "4. **Evaluation**: Ranking correlation and preference accuracy metrics\n",
    "\n",
    "### Training Pipeline\n",
    "1. **Data Preparation**: Convert preference annotations to training pairs\n",
    "2. **Model Training**: Optimize Bradley-Terry loss with Adam/AdamW\n",
    "3. **Validation**: Test ranking ability on held-out preference data\n",
    "4. **Integration**: Export for use in PPO policy training\n",
    "\n",
    "This implementation provides the foundation for building production-ready reward models that can scale to large preference datasets and integrate seamlessly with PPO-based policy optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa1e198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saul/.pyenv/versions/ppo_test/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    GPT2Model,\n",
    "    GPT2PreTrainedModel,\n",
    "    GPT2Config,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aab1e275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbc1f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "## üß± Step 1: Define GPT-2 Reward Model\n",
    "class GPT2RewardModel(GPT2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.gpt2 = GPT2Model(config)\n",
    "        self.value_head = nn.Linear(config.n_embd, 1)\n",
    "\n",
    "        # Initialize the value head with small weights\n",
    "        nn.init.normal_(self.value_head.weight, std=0.02)\n",
    "        nn.init.zeros_(self.value_head.bias)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass that returns a scalar reward for the input sequence.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Token IDs of shape (batch_size, seq_len)\n",
    "            attention_mask: Attention mask of shape (batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            rewards: Scalar rewards of shape (batch_size,)\n",
    "        \"\"\"\n",
    "        # Get GPT-2 outputs\n",
    "        outputs = self.gpt2(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_states = (\n",
    "            outputs.last_hidden_state\n",
    "        )  # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        # Get the hidden state of the last non-padding token for each sequence\n",
    "        if attention_mask is not None:\n",
    "            # Find the position of the last non-padding token\n",
    "            seq_lengths = attention_mask.sum(dim=1) - 1  # -1 for 0-indexing\n",
    "        else:\n",
    "            seq_lengths = torch.full(\n",
    "                (input_ids.shape[0],), input_ids.shape[1] - 1, device=input_ids.device\n",
    "            )\n",
    "\n",
    "        # Extract the final token's hidden state for each sequence in the batch\n",
    "        batch_indices = torch.arange(\n",
    "            last_hidden_states.shape[0], device=last_hidden_states.device\n",
    "        )\n",
    "        final_hidden_states = last_hidden_states[batch_indices, seq_lengths]\n",
    "\n",
    "        # Pass through value head to get scalar reward\n",
    "        rewards = self.value_head(final_hidden_states)\n",
    "        return rewards.squeeze(-1)  # Shape: (batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66989cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## üìö Step 2: Initialize Model and Tokenizer\n",
    "\n",
    "\n",
    "def initialize_model():\n",
    "    \"\"\"Initialize the reward model and tokenizer.\"\"\"\n",
    "    # Load GPT-2 config and create reward model\n",
    "    config = GPT2Config.from_pretrained(\"gpt2\")\n",
    "    model = GPT2RewardModel(config)\n",
    "\n",
    "    # Load tokenizer and set pad token\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eae261",
   "metadata": {},
   "outputs": [],
   "source": [
    "## üìä Step 3: Create Preference Dataset\n",
    "\n",
    "\n",
    "def create_preference_dataset(tokenizer, num_samples=1000, max_length=256):\n",
    "    \"\"\"\n",
    "    Create a simulated preference dataset for training.\n",
    "    In practice, you would load real preference data here.\n",
    "    \"\"\"\n",
    "    # Load IMDB dataset as base text\n",
    "    raw_dataset = load_dataset(\"imdb\", split=\"train[:5%]\")\n",
    "\n",
    "    preference_data = []\n",
    "\n",
    "    for i in tqdm(\n",
    "        range(min(num_samples, len(raw_dataset))), desc=\"Creating preference pairs\"\n",
    "    ):\n",
    "        base_text = raw_dataset[i][\"text\"][:200]  # Truncate for demo\n",
    "\n",
    "        # Create two versions - one \"better\" than the other\n",
    "        chosen_text = base_text + \" This is a great movie with excellent acting.\"\n",
    "        rejected_text = base_text + \" This movie is terrible and boring.\"\n",
    "\n",
    "        # Tokenize both versions\n",
    "        chosen_encoded = tokenizer(\n",
    "            chosen_text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        rejected_encoded = tokenizer(\n",
    "            rejected_text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        preference_data.append(\n",
    "            {\n",
    "                \"chosen_input_ids\": chosen_encoded[\"input_ids\"],\n",
    "                \"chosen_attention_mask\": chosen_encoded[\"attention_mask\"],\n",
    "                \"rejected_input_ids\": rejected_encoded[\"input_ids\"],\n",
    "                \"rejected_attention_mask\": rejected_encoded[\"attention_mask\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return preference_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8482236d",
   "metadata": {},
   "source": [
    "## üéØ Step 4: Fine-Tune GPT-2 for Scalar Reward Regression Pairwise Preference Loss\n",
    "\n",
    "We train the reward model to give higher scores to the 'chosen' than to the 'rejected' outputs:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{RM}} = -\\log\\left(\\frac{\\exp(r_\\text{chosen})}{\\exp(r_\\text{chosen}) + \\exp(r_\\text{rejected})}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdb46da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## üéØ Step 4: Define Loss Function\n",
    "\n",
    "\n",
    "def pairwise_preference_loss(chosen_rewards, rejected_rewards):\n",
    "    \"\"\"\n",
    "    Bradley-Terry model loss for preference learning.\n",
    "    Loss = -log(sigmoid(r_chosen - r_rejected))\n",
    "    \"\"\"\n",
    "    return -torch.log(torch.sigmoid(chosen_rewards - rejected_rewards)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5a6532",
   "metadata": {},
   "outputs": [],
   "source": [
    "## üîÅ Step 5: Training Loop\n",
    "\n",
    "\n",
    "def train_reward_model(\n",
    "    model, preference_dataset, tokenizer, num_epochs=3, batch_size=4, lr=1e-5\n",
    "):\n",
    "    \"\"\"Train the reward model on preference data.\"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Setup optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    total_steps = len(preference_dataset) * num_epochs // batch_size\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=total_steps // 10, num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # Create DataLoader\n",
    "    dataloader = DataLoader(preference_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    print(f\"Training on {device}\")\n",
    "    print(f\"Total training steps: {total_steps}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            # Move batch to device\n",
    "            chosen_input_ids = batch[\"chosen_input_ids\"].squeeze(1).to(device)\n",
    "            chosen_attention_mask = batch[\"chosen_attention_mask\"].squeeze(1).to(device)\n",
    "            rejected_input_ids = batch[\"rejected_input_ids\"].squeeze(1).to(device)\n",
    "            rejected_attention_mask = (\n",
    "                batch[\"rejected_attention_mask\"].squeeze(1).to(device)\n",
    "            )\n",
    "\n",
    "            # Forward pass\n",
    "            chosen_rewards = model(chosen_input_ids, chosen_attention_mask)\n",
    "            rejected_rewards = model(rejected_input_ids, rejected_attention_mask)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = pairwise_preference_loss(chosen_rewards, rejected_rewards)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix(\n",
    "                {\n",
    "                    \"loss\": f\"{loss.item():.4f}\",\n",
    "                    \"avg_loss\": f\"{total_loss / (batch_idx + 1):.4f}\",\n",
    "                    \"lr\": f\"{scheduler.get_last_lr()[0]:.2e}\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "        avg_epoch_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch + 1} completed. Average Loss: {avg_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f0a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## üî¨ Step 6: Evaluation and Testing\n",
    "\n",
    "\n",
    "def evaluate_reward_model(model, tokenizer, test_texts):\n",
    "    \"\"\"Evaluate the reward model on test texts.\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text in test_texts:\n",
    "            encoded = tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=256,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(device)\n",
    "\n",
    "            reward = model(encoded[\"input_ids\"], encoded[\"attention_mask\"])\n",
    "            print(f\"Text: {text[:50]}...\")\n",
    "            print(f\"Reward: {reward.item():.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a4cf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## üöÄ Step 7: Main Training Script\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training pipeline.\"\"\"\n",
    "    print(\"üéØ Initializing GPT-2 Reward Model for PPO\")\n",
    "\n",
    "    # Initialize model and tokenizer\n",
    "    model, tokenizer = initialize_model()\n",
    "    print(\n",
    "        f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\"\n",
    "    )\n",
    "\n",
    "    # Create preference dataset\n",
    "    print(\"üìä Creating preference dataset...\")\n",
    "    preference_dataset = create_preference_dataset(tokenizer, num_samples=500)\n",
    "    print(f\"Created {len(preference_dataset)} preference pairs\")\n",
    "\n",
    "    # Train the model\n",
    "    print(\"üîÅ Starting training...\")\n",
    "    train_reward_model(model, preference_dataset, tokenizer, num_epochs=3, batch_size=2)\n",
    "\n",
    "    # Evaluate on test examples\n",
    "    print(\"üî¨ Evaluating model...\")\n",
    "    test_texts = [\n",
    "        \"This movie is absolutely fantastic with great acting and plot!\",\n",
    "        \"This movie is terrible and boring with bad acting.\",\n",
    "        \"The film has decent cinematography but lacks emotional depth.\",\n",
    "    ]\n",
    "    evaluate_reward_model(model, tokenizer, test_texts)\n",
    "\n",
    "    # Save the model\n",
    "    print(\"üíæ Saving model...\")\n",
    "    model.save_pretrained(\"./gpt2_reward_model\")\n",
    "    tokenizer.save_pretrained(\"./gpt2_reward_model\")\n",
    "    print(\"Model saved successfully!\")\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8970e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## üéÆ Step 8: Usage with PPO\n",
    "\n",
    "\n",
    "def use_with_ppo_example(model, tokenizer, prompt, response):\n",
    "    \"\"\"\n",
    "    Example of how to use the reward model with PPO.\n",
    "    This function shows the expected interface for PPO integration.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Combine prompt and response\n",
    "    full_text = prompt + response\n",
    "\n",
    "    # Tokenize\n",
    "    encoded = tokenizer(\n",
    "        full_text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "    # Get reward\n",
    "    with torch.no_grad():\n",
    "        reward = model(encoded[\"input_ids\"], encoded[\"attention_mask\"])\n",
    "\n",
    "    return reward.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909a6c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Initializing GPT-2 Reward Model for PPO\n",
      "Model initialized with 124440577 parameters\n",
      "üìä Creating preference dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating preference pairs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 1957.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 500 preference pairs\n",
      "üîÅ Starting training...\n",
      "Training on cuda\n",
      "Total training steps: 750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:16<00:00, 15.25it/s, loss=-0.0000, avg_loss=0.1960, lr=7.41e-06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Average Loss: 0.1960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:12<00:00, 20.22it/s, loss=0.0000, avg_loss=0.0002, lr=3.70e-06]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed. Average Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:15<00:00, 16.45it/s, loss=-0.0000, avg_loss=0.0000, lr=0.00e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 completed. Average Loss: 0.0000\n",
      "üî¨ Evaluating model...\n",
      "Text: This movie is absolutely fantastic with great acti...\n",
      "Reward: 10.0591\n",
      "\n",
      "Text: This movie is terrible and boring with bad acting....\n",
      "Reward: -7.3091\n",
      "\n",
      "Text: The film has decent cinematography but lacks emoti...\n",
      "Reward: -1.1237\n",
      "\n",
      "üíæ Saving model...\n",
      "Model saved successfully!\n",
      "üéÆ PPO Integration Example:\n",
      "Reward for response: 10.3318\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Run the main training pipeline\n",
    "    trained_model, trained_tokenizer = main()\n",
    "\n",
    "    # Example PPO usage\n",
    "    print(\"üéÆ PPO Integration Example:\")\n",
    "    sample_prompt = \"What do you think about this movie? \"\n",
    "    sample_response = \"I think it's a great film with excellent storytelling and character development.\"\n",
    "\n",
    "    reward_score = use_with_ppo_example(\n",
    "        trained_model, trained_tokenizer, sample_prompt, sample_response\n",
    "    )\n",
    "    print(f\"Reward for response: {reward_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppo_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
