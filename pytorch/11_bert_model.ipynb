{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT's masked pretraining, often referred to as the cloze procedure, is a pivotal aspect of its training methodology. In this phase, a random subset of words in each training instance is replaced with a special [MASK] token. The model is then tasked with predicting the original words that were masked out, turning the training into a masked language model (MLM) objective. This approach is instrumental in training a bidirectional understanding of context, as the model must consider both preceding and succeeding words to accurately predict the masked tokens. Additionally, 15% of the words are randomly chosen for masking, and some are kept unchanged to avoid the model overfitting to the [MASK] token. This strategy ensures that the model learns a robust contextual representation of words, capturing intricate semantic relationships within sentences.\n",
    "\n",
    "BERT incorporates several ingenious tricks to enhance its training efficiency. One such technique is the use of segment embeddings, where each token is assigned a segment ID to distinguish between sentences in the input. This allows the model to understand the relationships between tokens in different segments, reinforcing its grasp of sentence-level context. Another key trick involves the use of a positional embedding to convey the order of words in a sentence, as BERT doesn't inherently account for word order. These techniques contribute to the creation of rich, context-aware representations during masked pretraining.\n",
    "\n",
    "During the fine-tuning phase, the [MASK] tokens play a crucial role in adapting BERT to specific tasks. In the task-specific datasets, a small fraction of the tokens is masked, and the model is fine-tuned to predict these masked tokens, similar to the pretraining phase. However, in fine-tuning, only a fraction of the masked tokens is replaced with the [MASK] token, while the rest are replaced with the actual words. This approach prevents the model from solely relying on the [MASK] token during fine-tuning, ensuring that it retains a nuanced understanding of the task-specific data.\n",
    "\n",
    "In summary, BERT's training methodology involves the cloze procedure during masked pretraining, where [MASK] tokens are used for predicting masked words bidirectionally. Ingenious tricks like segment embeddings and positional embeddings enhance the model's contextual understanding. During fine-tuning, the [MASK] tokens continue to be crucial, but with modifications to prevent over-reliance on them. These strategies collectively contribute to BERT's remarkable ability to capture intricate contextual relationships in natural language, making it a pioneering model in the realm of natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT model works and how it was trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, embed_dim: int, vocab_size: int):\n",
    "        \"\"\"\n",
    "        Initialize the InputEmbedding module.\n",
    "\n",
    "        Args:\n",
    "            embed_dim (int): The dimensionality of the input embedding.\n",
    "            vocab_size (int): The size of the vocabulary.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Store the dimensionality and vocabulary size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Create an embedding layer that maps the vocabulary to a embed_dim-dimensional space\n",
    "        # The embedding layer should have shape (vocab_size, embed_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the InputEmbedding module.\n",
    "\n",
    "        Args:\n",
    "            x (tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            tensor: The embedded input tensor after scaling it by the square root of the dimensionality.\n",
    "\n",
    "        \"\"\"\n",
    "        # Embed the input tensor using the embedding layer\n",
    "        # Shape: (batch_size, seq_len) -> (batch_size, seq_len, embed_dim)\n",
    "        embedded_input = self.embedding(x)\n",
    "        # Scale the embedded input tensor by the square root of the dimensionality\n",
    "        # Shape: (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n",
    "        scaled_embedded_input = embedded_input * torch.sqrt(torch.tensor(self.embed_dim))\n",
    "        return scaled_embedded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim: int = 512, max_seq_len: int = 100, dropout: float = 0.1,):\n",
    "        \"\"\"Initialize the PositionalEncoding module.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Precompute the positional encoding matrix\n",
    "        self.positional_encoding = self._precompute_positional_encoding(max_seq_len, embed_dim)\n",
    "\n",
    "    def _precompute_positional_encoding(self, max_seq_len, embed_dim):\n",
    "        \"\"\"Precompute the positional encoding matrix.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Create a positional encoding matrix of shape (max_seq_len, embed_dim)\n",
    "            positional_encoding = torch.zeros(max_seq_len, embed_dim)\n",
    "            # Create a tensor 'pos' with values [0, 1, 2, ..., max_seq_len - 1] (max_seq_len, 1)\n",
    "            position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "            # Compute the positional encoding matrix\n",
    "            division_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / embed_dim))\n",
    "            positional_encoding[:, 0::2] = torch.sin(position * division_term)\n",
    "            positional_encoding[:, 1::2] = torch.cos(position * division_term)\n",
    "            # Shape (max_seq_len, embed_dim) -> (1, max_seq_len, embed_dim)\n",
    "            positional_encoding = positional_encoding.unsqueeze(0)\n",
    "\n",
    "        return positional_encoding\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform the forward pass of the PositionalEncoding module.\"\"\"\n",
    "        # Add the positional encoding matrix to the input tensor\n",
    "        x = x + self.positional_encoding[:, : x.size(1)].to(x.device)\n",
    "        # Apply dropout to the input tensor\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, embed_dim: int, eps: float = 1e-6):\n",
    "        \"\"\"Initialize the LayerNormalization module.\"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gain = nn.Parameter(torch.Tensor(embed_dim).uniform_())\n",
    "        self.bias = nn.Parameter(torch.Tensor(embed_dim).normal_())\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform the forward pass of the LayerNormalization module.\"\"\"\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return (x - mean) / (std + self.eps) * self.gain + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, embed_dim: int, intermediate_size: int, dropout: float = 0.1):\n",
    "        \"\"\"Initialize the FeedForwardBlock module.\n",
    "        embed_dim is the hidden size of the transformer model functions as input and output size of the FeedForwardBlock\n",
    "        intermediate_size is the hidden size of the intermediate layer in the FeedForwardBlock\n",
    "        dropout is the dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, intermediate_size)\n",
    "        self.fc2 = nn.Linear(intermediate_size, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform the forward pass of the FeedForwardBlock module.\"\"\"\n",
    "        # (Batch, Seq_len, embed_dim) -> (Batch, Seq_len, intermediate_size) -> (Batch, Seq_len, embed_dim)\n",
    "        x_intermediate = self.dropout(F.relu(self.fc1(x)))\n",
    "        x_output = self.fc2(x_intermediate)\n",
    "        return x_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int = 512, num_heads: int = 8, attn_dropout: float = 0.1, ff_dropout: float = 0.1, max_len: int = 512):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        assert embed_dim % self.num_heads == 0, \"invalid heads and embedding dimension configuration\"\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
    "        self.proj_dropout = nn.Dropout(ff_dropout)\n",
    "    \n",
    "    def forward(self, q:torch.Tensor, k:torch.Tensor, v:torch.Tensor, mask=None,):\n",
    "        batch_size, seq_len, _ = q.size()\n",
    "        q = self.query(q).view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
    "        k = self.key(k).view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
    "        v = self.value(v).view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
    "\n",
    "        attention = torch.einsum('bhid,bhjd->bhij', q, k) / math.sqrt(q.size(-1))\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        attention = self.attn_dropout(F.softmax(attention, dim=-1))\n",
    "        \n",
    "        y = torch.einsum('bhij,bhjd->bhid', attention, v)\n",
    "        y = y.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "        return self.proj_dropout(self.proj(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout: float = 0.1):\n",
    "        \"\"\"Initialize the ResidualConnection module.\"\"\"\n",
    "        super().__init__()\n",
    "        self.layer_norm = LayerNormalization(embed_dim=embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"Perform the forward pass of the ResidualConnection module.\"\"\"\n",
    "        sublayer_output = sublayer(x)\n",
    "        normalized_x = self.layer_norm(x)\n",
    "        residual_output = normalized_x + sublayer_output\n",
    "        return self.dropout(residual_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLanguageModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    predicting origin token from masked input sequence\n",
    "    n-class classification problem, n-class = vocab_size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, vocab_size):\n",
    "        \"\"\"\n",
    "        :param hidden: output size of BERT model\n",
    "        :param vocab_size: total vocab size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(embed_dim, vocab_size)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextSentencePrediction(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    2-class classification model : is_next, is_not_next\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim):\n",
    "        \"\"\"\n",
    "        :param hidden: BERT model output size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(embed_dim, 1)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # use only the first token which is the [CLS]\n",
    "        return self.softmax(self.linear(x[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, embed_dim: int, vocab_size: int):\n",
    "        \"\"\"Initialize the ProjectionHead module.\"\"\"\n",
    "        super().__init__()\n",
    "        self.masked_lm = MaskedLanguageModel(embed_dim, vocab_size)\n",
    "        self.next_sentence = NextSentencePrediction(embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform the forward pass of the ProjectionHead module.\"\"\"\n",
    "        # (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, vocab_size)\n",
    "        return self.masked_lm(x), self.next_sentence(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        embed_dim: int = 512, \n",
    "        num_heads: int = 8, \n",
    "        ff_dim: int = 2048, \n",
    "        attn_dropout: float = 0.1, \n",
    "        ff_dropout: float = 0.1, \n",
    "        max_len: int = 512,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Initialize multi-head self-attention mechanism\n",
    "        self.MultiHeadAttention = MultiHeadAttention(\n",
    "            embed_dim=embed_dim, \n",
    "            num_heads=num_heads, \n",
    "            attn_dropout=attn_dropout, \n",
    "            ff_dropout=ff_dropout,\n",
    "            max_len=max_len,\n",
    "            )\n",
    "        # Initialize feed-forward block\n",
    "        self.feed_forward = FeedForwardBlock(\n",
    "            embed_dim=embed_dim, \n",
    "            intermediate_size=ff_dim, \n",
    "            dropout=ff_dropout,\n",
    "            )\n",
    "        # Initialize residual connections\n",
    "        self.residual_connection1 = ResidualConnection(embed_dim=embed_dim, dropout=ff_dropout)\n",
    "        self.residual_connection2 = ResidualConnection(embed_dim=embed_dim, dropout=ff_dropout)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # Apply self-attention mechanism with residual connection\n",
    "        x_with_attention = self.residual_connection1(x, lambda x: self.MultiHeadAttention(x, x, x, mask=attention_mask))\n",
    "        # Apply feed-forward block with residual connection\n",
    "        x_with_ff = self.residual_connection2(x_with_attention, self.feed_forward)\n",
    "        return x_with_ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim:int = 512,\n",
    "        vocab_size:int = 10000,\n",
    "        num_segments:int = 3,\n",
    "        max_seq_len:int = 100,\n",
    "        attn_dropout:float = 0.1,\n",
    "        ff_dropout:float = 0.1,\n",
    "        n_blocks:int = 6,\n",
    "        num_heads:int = 8,\n",
    "        ff_dim:int = 2048,\n",
    "    ):\n",
    "        \"\"\"Initialize the Encoder module.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        # (768, 30_000) Uses Wordpiece Embedding\n",
    "        self.embedding = InputEmbedding(embed_dim=embed_dim, vocab_size=vocab_size)\n",
    "        # (768 30_000) Positional Embedding max_length is the context window size, model won't be able to see beyond this\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            embed_dim=embed_dim, \n",
    "            max_seq_len=max_seq_len, \n",
    "            dropout=ff_dropout,\n",
    "        )\n",
    "        # distinguish between 3 segments: segment A, segment B, and padding\n",
    "        # the forward method ends up as (batch_size, seq_len, embed_dim)\n",
    "        self.segment_embed = nn.Embedding(num_segments, embed_dim)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            EncoderBlock(\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                ff_dim=ff_dim,\n",
    "                attn_dropout=attn_dropout,\n",
    "                ff_dropout=ff_dropout,\n",
    "            ) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.projection_head = ProjectionHead(embed_dim=embed_dim, vocab_size=vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, src_mask: torch.Tensor = None):\n",
    "        # (batch_size, seq_len) -> (batch_size, seq_len, embed_dim)\n",
    "        x = self.embedding(input_ids)\n",
    "        # (batch_size, seq_len, embed_dim) + (batch_size, seq_len, embed_dim)\n",
    "        x = x + self.segment_embed(src_mask)\n",
    "        # (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n",
    "        x = self.positional_encoding(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.projection_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 512\n",
    "vocab_size = 10000\n",
    "max_seq_len = 100\n",
    "n_blocks = 1\n",
    "num_heads = 8\n",
    "ff_dim = 2048\n",
    "\n",
    "# Create an instance of the Encoder class\n",
    "sample_model = Bert(\n",
    "    embed_dim=embed_dim, \n",
    "    vocab_size=vocab_size, \n",
    "    max_seq_len=max_seq_len,\n",
    "    n_blocks=n_blocks, \n",
    "    num_heads=num_heads, \n",
    "    ff_dim=ff_dim,\n",
    ")\n",
    "\n",
    "# Define your input tensor\n",
    "input_ids = torch.tensor([[1, 2, 3, 4, 0], [1, 2, 3, 0, 0]])  # Input token IDs\n",
    "segment_ids = torch.tensor([[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]])  # Segment IDs\n",
    "\n",
    "# Pass the input tensor through the Encoder\n",
    "output = sample_model(input_ids, src_mask=segment_ids)\n",
    "print(f\"Masked Lanaguage Output: {output[0].shape}\")\n",
    "print(f\"Next Sentence Output: {output[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, data_pair, tokenizer, seq_len=64):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        self.corpus_lines = len(data_pair)\n",
    "        self.lines = data_pair\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.corpus_lines\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "\n",
    "        # Step 1: get random sentence pair, either negative or positive (saved as is_next_label)\n",
    "        t1, t2, is_next_label = self.get_sent(item)\n",
    "\n",
    "        # Step 2: replace random words in sentence with mask / random words\n",
    "        t1_random, t1_label = self.random_word(t1)\n",
    "        t2_random, t2_label = self.random_word(t2)\n",
    "\n",
    "        # Step 3: Adding CLS and SEP tokens to the start and end of sentences\n",
    "         # Adding PAD token for labels\n",
    "        t1 = [self.tokenizer.vocab['[CLS]']] + t1_random + [self.tokenizer.vocab['[SEP]']]\n",
    "        t2 = t2_random + [self.tokenizer.vocab['[SEP]']]\n",
    "        t1_label = [self.tokenizer.vocab['[PAD]']] + t1_label + [self.tokenizer.vocab['[PAD]']] # label is 1/0 for each token\n",
    "        t2_label = t2_label + [self.tokenizer.vocab['[PAD]']]\n",
    "\n",
    "        # Step 4: combine sentence 1 and 2 as one input\n",
    "        # adding PAD tokens to make the sentence same length as seq_len\n",
    "        segment_ids = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len]\n",
    "        input_ids = (t1 + t2)[:self.seq_len]\n",
    "        masked_lm_labels = (t1_label + t2_label)[:self.seq_len]\n",
    "        padding = [self.tokenizer.vocab['[PAD]'] for _ in range(self.seq_len - len(input_ids))]\n",
    "        input_ids.extend(padding), masked_lm_labels.extend(padding), segment_ids.extend(padding)\n",
    "\n",
    "        # Converting to tensor\n",
    "        input_ids = torch.tensor(input_ids).long()\n",
    "        segment_ids = torch.tensor(segment_ids).long()\n",
    "        masked_lm_labels = torch.tensor(masked_lm_labels).long()\n",
    "        is_next_label = torch.tensor(is_next_label).long()\n",
    "\n",
    "        return input_ids, segment_ids, masked_lm_labels, is_next_label\n",
    "\n",
    "    def random_word(self, sentence):\n",
    "        tokens = sentence.split()\n",
    "        output_label = []\n",
    "        output = []\n",
    "\n",
    "        # 15% of the tokens would be replaced\n",
    "        for i, token in enumerate(tokens):\n",
    "            prob = random.random()\n",
    "\n",
    "            # remove cls and sep token\n",
    "            token_id = self.tokenizer(token)['input_ids'][1:-1]\n",
    "\n",
    "            if prob < 0.15:\n",
    "                prob /= 0.15\n",
    "\n",
    "                # 80% chance change token to mask token\n",
    "                if prob < 0.8:\n",
    "                    for i in range(len(token_id)):\n",
    "                        output.append(self.tokenizer.vocab['[MASK]'])\n",
    "\n",
    "                # 10% chance change token to random token\n",
    "                elif prob < 0.9:\n",
    "                    for i in range(len(token_id)):\n",
    "                        output.append(random.randrange(len(self.tokenizer.vocab)))\n",
    "\n",
    "                # 10% chance change token to current token\n",
    "                else:\n",
    "                    output.append(token_id)\n",
    "\n",
    "                output_label.append(token_id)\n",
    "\n",
    "            else:\n",
    "                output.append(token_id)\n",
    "                for i in range(len(token_id)):\n",
    "                    output_label.append(0)\n",
    "\n",
    "        # flattening\n",
    "        output = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output]))\n",
    "        output_label = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output_label]))\n",
    "        assert len(output) == len(output_label)\n",
    "        return output, output_label\n",
    "\n",
    "    def get_sent(self, index):\n",
    "        '''return random sentence pair'''\n",
    "        t1, t2 = self.get_corpus_line(index)\n",
    "\n",
    "        # negative or positive pair, for next sentence prediction\n",
    "        if random.random() > 0.5:\n",
    "            return t1, t2, 1\n",
    "        else:\n",
    "            return t1, self.get_random_line(), 0\n",
    "\n",
    "    def get_corpus_line(self, item):\n",
    "        '''return sentence pair'''\n",
    "        return self.lines[item][0], self.lines[item][1]\n",
    "\n",
    "    def get_random_line(self):\n",
    "        '''return random single sentence'''\n",
    "        return self.lines[random.randrange(len(self.lines))][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "sample_data = [\n",
    "    [\"Mary had a little lamb\", \"Its fleece was white as snow\",],\n",
    "    [\"Everywhere that Mary went\", \"The lamb was sure to go\",],\n",
    "    [\"Well, I thought we'd start with pronunciation, if that's okay with you.\", 'Not the hacking and gagging and spitting part. Please.'], \n",
    "    ['Not the hacking and gagging and spitting part. Please.', \"Okay... then how 'bout we try out some French cuisine. Saturday? Night?\"], \n",
    "    [\"You're asking me out. That's so cute. What's your name again?\", 'Forget it.'], \n",
    "    [\"No, no, it's my fault -- we didn't have a proper introduction ---\", 'Cameron.']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 768\n",
    "vocab_size = tokenizer.vocab_size # 30522\n",
    "max_seq_len = 100\n",
    "n_blocks = 2\n",
    "num_heads = 12\n",
    "ff_dim = 2048\n",
    "\n",
    "device = \"cpu\" #torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr = 5e-5\n",
    "batch_size = 2\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BERTDataset(\n",
    "   sample_data,\n",
    "   seq_len=max_seq_len,\n",
    "   tokenizer=tokenizer,\n",
    "   )\n",
    "\n",
    "train_loader = DataLoader(\n",
    "   train_dataset,\n",
    "   batch_size=batch_size, \n",
    "   shuffle=True, \n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Encoder class\n",
    "model = Bert(\n",
    "    embed_dim=embed_dim, \n",
    "    vocab_size=vocab_size, \n",
    "    max_seq_len=max_seq_len,\n",
    "    n_blocks=n_blocks, \n",
    "    num_heads=num_heads, \n",
    "    ff_dim=ff_dim,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion1 = nn.NLLLoss(ignore_index=0)\n",
    "criterion2 = nn.BCEWithLogitsLoss()\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,)\n",
    "\n",
    "for batch in train_loader:\n",
    "    # Step 1: Move batch to device\n",
    "    input_ids, segment_ids, masked_lm_labels, is_next = batch\n",
    "    input_ids = input_ids.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    masked_lm_labels = masked_lm_labels.to(device)\n",
    "    is_next = is_next.unsqueeze(1).float().to(device)\n",
    "\n",
    "    # Step 2: Forward pass (batch_size, seq_len, embed_dim)\n",
    "    output = model(input_ids, segment_ids)\n",
    "\n",
    "    # Step 3: Loss for masked LM\n",
    "    loss_mask = criterion1(output[0].view(-1, vocab_size), masked_lm_labels.view(-1))\n",
    "\n",
    "    # Step 4: Loss for next sentence prediction\n",
    "    next_sentence_loss = criterion2(output[1], is_next)\n",
    "\n",
    "    # Step 5: Total loss\n",
    "    loss = loss_mask + next_sentence_loss\n",
    "\n",
    "    # Step 6: Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Step 7: Print loss\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "input_ids, segment_ids, masked_lm_labels, is_next = batch\n",
    "output = model(input_ids, segment_ids)\n",
    "print(f\"Masked Language Model Output: {output[0].shape}\")\n",
    "print(f\"Next Sentence Prediction Output: {output[1].shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
