{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work on showing how the BERT model works and how it was trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT's masked pretraining, often referred to as the cloze procedure, is a pivotal aspect of its training methodology. In this phase, a random subset of words in each training instance is replaced with a special [MASK] token. The model is then tasked with predicting the original words that were masked out, turning the training into a masked language model (MLM) objective. This approach is instrumental in training a bidirectional understanding of context, as the model must consider both preceding and succeeding words to accurately predict the masked tokens. Additionally, 15% of the words are randomly chosen for masking, and some are kept unchanged to avoid the model overfitting to the [MASK] token. This strategy ensures that the model learns a robust contextual representation of words, capturing intricate semantic relationships within sentences.\n",
    "\n",
    "BERT incorporates several ingenious tricks to enhance its training efficiency. One such technique is the use of segment embeddings, where each token is assigned a segment ID to distinguish between sentences in the input. This allows the model to understand the relationships between tokens in different segments, reinforcing its grasp of sentence-level context. Another key trick involves the use of a positional embedding to convey the order of words in a sentence, as BERT doesn't inherently account for word order. These techniques contribute to the creation of rich, context-aware representations during masked pretraining.\n",
    "\n",
    "During the fine-tuning phase, the [MASK] tokens play a crucial role in adapting BERT to specific tasks. In the task-specific datasets, a small fraction of the tokens is masked, and the model is fine-tuned to predict these masked tokens, similar to the pretraining phase. However, in fine-tuning, only a fraction of the masked tokens is replaced with the [MASK] token, while the rest are replaced with the actual words. This approach prevents the model from solely relying on the [MASK] token during fine-tuning, ensuring that it retains a nuanced understanding of the task-specific data.\n",
    "\n",
    "In summary, BERT's training methodology involves the cloze procedure during masked pretraining, where [MASK] tokens are used for predicting masked words bidirectionally. Ingenious tricks like segment embeddings and positional embeddings enhance the model's contextual understanding. During fine-tuning, the [MASK] tokens continue to be crucial, but with modifications to prevent over-reliance on them. These strategies collectively contribute to BERT's remarkable ability to capture intricate contextual relationships in natural language, making it a pioneering model in the realm of natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Embedding module that combines token, positional, and segment embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, num_segments: int, max_sequence_length: int,\n",
    "                 hidden_size: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize BERT Embedding module.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary.\n",
    "            num_segments (int): Number of segments for segment embeddings.\n",
    "            max_sequence_length (int): Maximum sequence length.\n",
    "            hidden_size (int): Size of the hidden layer.\n",
    "            dropout (float): Dropout rate (default: 0.1).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_size) # (30_000, 768) Uses Wordpiece Embedding\n",
    "        self.positional_embedding = nn.Embedding(max_sequence_length, hidden_size) # (512, 768) Positional Embedding max_length is the context window size, model won't be able to see beyond this\n",
    "        self.segment_embedding = nn.Embedding(num_segments, hidden_size) # (3, 768) Segment Embedding for distinguishing between sentences [CLS], [SEP], [PAD]\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"positional_input\", torch.arange(max_sequence_length).unsqueeze(0))\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, segment_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the BERT Embedding module.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Input token IDs.\n",
    "            segment_ids (torch.Tensor): Segment IDs for distinguishing different segments.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Combined embeddings.\n",
    "        \"\"\"\n",
    "        embeddings = (self.token_embedding(input_ids)\n",
    "                      + self.positional_embedding(self.positional_input)\n",
    "                      + self.segment_embedding(segment_ids))\n",
    "        return self.dropout(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTTransformerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Transformer model for sequence classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, num_segments: int, max_sequence_length: int,\n",
    "                 hidden_size: int, num_attention_heads: int, num_layers: int,\n",
    "                 dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize BERT Transformer model.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary.\n",
    "            num_segments (int): Number of segments for segment embeddings.\n",
    "            max_sequence_length (int): Maximum sequence length.\n",
    "            hidden_size (int): Size of the hidden layer.\n",
    "            num_attention_heads (int): Number of attention heads.\n",
    "            num_layers (int): Number of transformer layers.\n",
    "            dropout (float): Dropout rate (default: 0.1).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding = BERTEmbedding(vocab_size, num_segments, max_sequence_length,\n",
    "                                       hidden_size, dropout=dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(hidden_size, num_attention_heads,\n",
    "                                                   dropout=dropout)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, segment_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the BERT Transformer model.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Input token IDs.\n",
    "            segment_ids (torch.Tensor): Segment IDs for distinguishing different segments.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits.\n",
    "        \"\"\"\n",
    "        x = self.embedding(input_ids, segment_ids)\n",
    "        x = self.encoder(x)\n",
    "        x = self.fc(x[:, 0])  # Assuming classification based on the [CLS] token\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data (replace with your actual data)\n",
    "input_ids = torch.tensor([[1, 2, 3, 4, 0], [1, 2, 3, 0, 0]])  # Input token IDs\n",
    "segment_ids = torch.tensor([[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]])  # Segment IDs\n",
    "labels = torch.tensor([1, 0])  # Example labels (binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = 30000\n",
    "num_segments = 3\n",
    "max_sequence_length = 5\n",
    "hidden_size = 768\n",
    "num_attention_heads = 12\n",
    "num_layers = 6\n",
    "dropout = 0.1\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 10\n",
    "\n",
    "# Instantiate model\n",
    "model = BERTTransformerModel(vocab_size, num_segments, max_sequence_length, hidden_size,\n",
    "                             num_attention_heads, num_layers, dropout)\n",
    "\n",
    "model(input_ids, segment_ids)  # Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saul/.pyenv/versions/3.11.8/envs/llm/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7733625173568726\n",
      "Epoch 2, Loss: 1.1450214385986328\n",
      "Epoch 3, Loss: 0.7164051532745361\n",
      "Epoch 4, Loss: 0.9264397621154785\n",
      "Epoch 5, Loss: 0.6959352493286133\n",
      "Epoch 6, Loss: 0.7335981130599976\n",
      "Epoch 7, Loss: 0.7223803997039795\n",
      "Epoch 8, Loss: 0.7208968997001648\n",
      "Epoch 9, Loss: 0.6882054805755615\n",
      "Epoch 10, Loss: 0.7581155896186829\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader\n",
    "dataset = TensorDataset(input_ids, segment_ids, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Instantiate model\n",
    "model = BERTTransformerModel(vocab_size, num_segments, max_sequence_length, hidden_size,\n",
    "                             num_attention_heads, num_layers, dropout)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary cross-entropy loss with logits\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for input_ids_batch, segment_ids_batch, labels_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids_batch, segment_ids_batch)\n",
    "        loss = criterion(outputs.squeeze(), labels_batch.float())  # Squeeze to remove extra dimension\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
