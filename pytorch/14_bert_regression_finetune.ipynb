{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the embeddings from the dataframe\n",
    "train_embeddings_text = train_df_raw[\"text\"]\n",
    "valid_embeddings_text = valid_df_raw[\"text\"]\n",
    "test_embeddings_text = test_df_raw[\"text\"]\n",
    "\n",
    "target_train = train_df_raw[[\"labels\"]]\n",
    "target_valid = valid_df_raw[[\"labels\"]]\n",
    "target_test = test_df_raw[[\"labels\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    def __init__(self, x, y, tokenizer, max_length):\n",
    "        super(BertDataset, self).__init__()\n",
    "        self.x = x\n",
    "        self.y = torch.from_numpy(y).float().view(-1, 1)\n",
    "        self.tokenizer=tokenizer\n",
    "        self.max_length=max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.x[index])\n",
    "        # encode returns a dictionary with keys: input_ids, token_type_ids, attention_mask\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text=text,\n",
    "            text_pair=None,\n",
    "            padding='max_length',\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "        ids = inputs[\"input_ids\"][:self.max_length]\n",
    "        token_type_ids = inputs[\"token_type_ids\"][:self.max_length]\n",
    "        mask = inputs[\"attention_mask\"][:self.max_length]\n",
    "        items = {\n",
    "            'input_ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'target': self.y[index],\n",
    "            }\n",
    "\n",
    "        return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = BertDataset(\n",
    "    x=train_embeddings_text.values, \n",
    "    y=target_train.values, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=100,\n",
    "    )\n",
    "dataloader_train = DataLoader(\n",
    "    dataset=dataset_train, \n",
    "    batch_size=32, \n",
    "    shuffle=True,\n",
    "    )\n",
    "\n",
    "dataset_valid = BertDataset(\n",
    "    x=valid_embeddings_text.values, \n",
    "    y=target_valid.values, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=100,\n",
    "    )\n",
    "dataloader_valid = DataLoader(\n",
    "    dataset=dataset_valid, \n",
    "    batch_size=32, \n",
    "    shuffle=True,\n",
    "    )\n",
    "\n",
    "dataset_test = BertDataset(\n",
    "    x=test_embeddings_text.values, \n",
    "    y=target_test.values, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=100,\n",
    "    )\n",
    "dataloader_test = DataLoader(dataset=dataset_test, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertRegressor, self).__init__()\n",
    "        self.bert_model = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.fc1 = nn.Linear(768, 512)\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(512)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(256)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.out = nn.Linear(256, 1)\n",
    "        nn.init.xavier_normal_(self.out.weight)\n",
    "        \n",
    "    def forward(self,input_ids, attention_mask, token_type_ids):\n",
    "        # Feed input to BERT\n",
    "        _, pooled_output = self.bert_model(\n",
    "            input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids, \n",
    "            return_dict=False,\n",
    "            )\n",
    "        \n",
    "        # Regression head\n",
    "        x = self.dropout1(F.relu(self.batch_norm1(self.fc1(pooled_output))))\n",
    "        x = self.dropout2(F.relu(self.batch_norm2(self.fc2(x))))\n",
    "        output = F.sigmoid(self.out(x))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regression(model, train_loader, optimizer, objective, device):\n",
    "    model.train()\n",
    "    train_batch_loss = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        ids = batch[\"input_ids\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        y = batch[\"target\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_hat = model(\n",
    "            input_ids=ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            )\n",
    "\n",
    "        loss = objective(y_hat, y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_batch_loss.append(loss.item())\n",
    "\n",
    "    train_loss = sum(train_batch_loss) / len(train_batch_loss)\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def validate_regression(model, val_loader, objective, device):\n",
    "    model.eval()\n",
    "    val_batch_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            ids = batch[\"input_ids\"].to(device)\n",
    "            mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            y_val = batch[\"target\"].to(device)\n",
    "\n",
    "            y_hat_val = model(\n",
    "                input_ids=ids,\n",
    "                attention_mask=mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                )\n",
    "\n",
    "            loss_val = objective(y_hat_val, y_val)\n",
    "            \n",
    "            val_batch_loss.append(loss_val.item())\n",
    "\n",
    "    val_loss = sum(val_batch_loss) / len(val_batch_loss)\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_test, y_test in data_loader:\n",
    "            x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "            y_hat = model(x_test)\n",
    "            predictions.append(y_hat)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def predict(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            ids = batch[\"input_ids\"].to(device)\n",
    "            mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            y_hat = model(\n",
    "                input_ids=ids,\n",
    "                attention_mask=mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                )\n",
    "            predictions.append(y_hat.cpu())\n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0).squeeze()\n",
    "    return predictions\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 1\n",
    "        self.best_validation_loss = float('inf')\n",
    "        self.best_model_weights = None\n",
    "\n",
    "    def early_stop(self, validation_loss, model):\n",
    "        if validation_loss < self.best_validation_loss - self.min_delta:\n",
    "            self.best_validation_loss = validation_loss\n",
    "            self.counter = 1\n",
    "            self.save_best_weights(model)\n",
    "            print(f\"Early Stopping counter: {self.counter} out of {self.patience}\")\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f\"Early Stopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def save_best_weights(self, model):\n",
    "        self.best_model_weights = deepcopy(model.state_dict())\n",
    "\n",
    "    def restore_best_weights(self, model):\n",
    "        model.load_state_dict(self.best_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists to store loss and accuracy\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "# training lists and parameters\n",
    "n_epochs = 100\n",
    "patience = 5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "# instantiate model, loss function and optimizer\n",
    "model = BertRegressor().to(device)\n",
    "objective = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.01)\n",
    "early_stopper = EarlyStopper(patience=patience, min_delta=0)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    tr_loss = train_regression(model, dataloader_train, optimizer, objective, device)\n",
    "    train_loss.append(tr_loss)\n",
    "\n",
    "    eva_loss = validate_regression(model, dataloader_valid, objective, device)\n",
    "    val_loss.append(eva_loss)\n",
    "    print(f'[{epoch+1}/{n_epochs}] Train loss: {tr_loss:.4f} - Val loss: {eva_loss:.4f}')\n",
    "    if early_stopper.early_stop(eva_loss, model):\n",
    "        early_stopper.save_best_weights(model)\n",
    "        print(\"Patience Depleated: Early Stopping triggered.\")\n",
    "        break\n",
    "print(\"Restoring best weights\")\n",
    "early_stopper.restore_best_weights(model)\n",
    "print(\"Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = predict(model, dataloader_train, device)\n",
    "y_pred_valid = predict(model, dataloader_valid, device)\n",
    "y_pred_test = predict(model, dataloader_test, device)\n",
    "\n",
    "\n",
    "mse_train = mean_squared_error(target_train, y_pred_train, squared=True)\n",
    "mse_valid = mean_squared_error(target_valid, y_pred_valid, squared=True)\n",
    "mse_test = mean_squared_error(target_test, y_pred_test, squared=True)\n",
    "\n",
    "mae_train = mean_absolute_error(target_train, y_pred_train)\n",
    "mae_valid = mean_absolute_error(target_valid, y_pred_valid)\n",
    "mae_test = mean_absolute_error(target_test, y_pred_test)\n",
    "\n",
    "# create single dataframe to compare results\n",
    "results_df = pd.DataFrame()\n",
    "results_df[\"train\"] = [mse_train, mae_train]\n",
    "results_df[\"valid\"] = [mse_valid, mae_valid]\n",
    "results_df[\"test\"] = [mse_test, mae_test]\n",
    "results_df.index = [\"mse\", \"mae\"]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results_df = pd.DataFrame({\n",
    "    \"input_text\": train_embeddings_text,\n",
    "    \"target\": target_train.values.flatten(),\n",
    "    \"predicted\": y_pred_train\n",
    "    })\n",
    "\n",
    "valid_results_df = pd.DataFrame({\n",
    "    \"input_text\": valid_embeddings_text,\n",
    "    \"target\": target_valid.values.flatten(),\n",
    "    \"predicted\": y_pred_valid\n",
    "    })\n",
    "\n",
    "test_results_df = pd.DataFrame({\n",
    "    \"input_text\": test_embeddings_text,\n",
    "    \"target\": target_test.values.flatten(),\n",
    "    \"predicted\": y_pred_test\n",
    "    })\n",
    "\n",
    "\n",
    "train_results_df[\"residual\"] = train_results_df[\"target\"] - train_results_df[\"predicted\"]\n",
    "valid_results_df[\"residual\"] = valid_results_df[\"target\"] - valid_results_df[\"predicted\"]\n",
    "test_results_df[\"residual\"] = test_results_df[\"target\"] - test_results_df[\"predicted\"]\n",
    "\n",
    "train_results_df[\"residual_abs\"] = abs(train_results_df[\"target\"] - train_results_df[\"predicted\"])\n",
    "valid_results_df[\"residual_abs\"] = abs(valid_results_df[\"target\"] - valid_results_df[\"predicted\"])\n",
    "test_results_df[\"residual_abs\"] = abs(test_results_df[\"target\"] - test_results_df[\"predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residuals\n",
    "plt.scatter(test_results_df.index, test_results_df[\"residual\"])\n",
    "plt.title(\"Residuals vs. True Values\")\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top n residuals\n",
    "n = 25\n",
    "sample_test = test_results_df.nlargest(n, \"residual_abs\")\n",
    "sample_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-pp-similarity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
