{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings With Sentence-Transformers\n",
    "\n",
    "I will work through two examples of using the sentence-transformer, the first time I will use the `sentence-transformer` library then only the `transformers` library to do the same approach from scratch.\n",
    "\n",
    "The purpose is to show that it's not too difficult to create the sentence-transformer funcationality from scratch and that the BERT model can be fine-tuned and applied to similarity tasks relatively easily.\n",
    "\n",
    "* Inspired from this video: https://www.youtube.com/watch?v=Ey81KfQ3PQU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Background:\n",
    "* import tokenizer, size 512\n",
    "    * first token is ['CLS'], empty tokkens are assigned ['PAD']\n",
    "* 12 Encoder Blocks\n",
    "    * Each block has an input of (1x768x512)\n",
    "* Output is 512x1 (flattens out the 767)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using transformer library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Three years later, the coffin was still full of Jello.\",\n",
    "    \"The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.\",\n",
    "    \"The person box was packed with jelly many dozens of months later.\",\n",
    "    \"Standing on one's head at job interviews forms a lasting impression.\",\n",
    "    \"It took him a month to finish the meal.\",\n",
    "    \"He found a leprechaun in his walnut shell.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saulramirez/.pyenv/versions/3.11.2/envs/eureka-trad/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 399/399 [00:00<00:00, 2.02MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 625/625 [00:00<00:00, 5.04MB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 4.37MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 2.96MB/s]\n",
      "Downloading (…)in/added_tokens.json: 100%|██████████| 2.00/2.00 [00:00<00:00, 5.97kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 521kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 438M/438M [00:05<00:00, 80.7MB/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "\n",
    "# initialize dictionary that will contain tokenized sentences\n",
    "tokens = {'input_ids': [], 'attention_mask': []}\n",
    "\n",
    "for sentence in sentences:\n",
    "    # tokenize sentence and append to dictionary lists max_length=128 because it's a BERT destill\n",
    "    # returns dictionary of lists of tensors\n",
    "    # we need to pull input and attention mask from dictionary\n",
    "    new_tokens = tokenizer.encode_plus(sentence, max_length=128, truncation=True,\n",
    "                                       padding='max_length', return_tensors='pt')\n",
    "    # append new tokens to dictionary\n",
    "    tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
    "    # append new attention mask to dictionary\n",
    "    tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
    "\n",
    "# reformat list of tensors into single tensor\n",
    "tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
    "tokens['attention_mask'] = torch.stack(tokens['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6 sentences, 128 tokens per sentence\n",
    "tokens['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**tokens)\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 128, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-6.9230e-02,  6.2300e-01,  3.5369e-02,  ...,  8.0334e-01,\n",
       "           1.6314e+00,  3.2812e-01],\n",
       "         [ 3.6729e-02,  6.8419e-01,  1.9460e-01,  ...,  8.4759e-02,\n",
       "           1.4747e+00, -3.0080e-01],\n",
       "         [-1.2140e-02,  6.5431e-01, -7.2718e-02,  ..., -3.2600e-02,\n",
       "           1.7717e+00, -6.8121e-01],\n",
       "         ...,\n",
       "         [ 1.9532e-01,  1.1085e+00,  3.3905e-01,  ...,  1.2826e+00,\n",
       "           1.0114e+00, -7.2754e-02],\n",
       "         [ 9.0217e-02,  1.0288e+00,  3.2973e-01,  ...,  1.2940e+00,\n",
       "           9.8651e-01, -1.1125e-01],\n",
       "         [ 1.2404e-01,  9.7365e-01,  3.9329e-01,  ...,  1.1359e+00,\n",
       "           8.7685e-01, -1.0435e-01]],\n",
       "\n",
       "        [[-3.2124e-01,  8.2512e-01,  1.0554e+00,  ..., -1.8555e-01,\n",
       "           1.5169e-01,  3.9366e-01],\n",
       "         [-7.1457e-01,  1.0297e+00,  1.1217e+00,  ...,  3.3118e-02,\n",
       "           2.3820e-01, -1.5632e-01],\n",
       "         [-2.3522e-01,  1.1353e+00,  8.5941e-01,  ..., -4.3096e-01,\n",
       "          -2.7241e-02, -2.9676e-01],\n",
       "         ...,\n",
       "         [-5.4000e-01,  3.2365e-01,  7.8392e-01,  ...,  2.1861e-03,\n",
       "          -2.9941e-01,  2.6594e-01],\n",
       "         [-5.6429e-01,  3.1867e-01,  9.5759e-01,  ...,  3.4248e-02,\n",
       "          -3.0299e-01,  1.8783e-01],\n",
       "         [-5.1719e-01,  3.5987e-01,  9.3357e-01,  ...,  2.4326e-02,\n",
       "          -2.2319e-01,  1.6717e-01]],\n",
       "\n",
       "        [[-7.5756e-01,  8.3988e-01, -3.7922e-01,  ...,  1.2708e-01,\n",
       "           1.2514e+00,  1.3652e-01],\n",
       "         [-6.5908e-01,  7.6135e-01, -4.6619e-01,  ...,  2.2593e-01,\n",
       "           1.1289e+00, -3.6105e-01],\n",
       "         [-9.0070e-01,  6.7913e-01, -3.7775e-01,  ...,  1.1418e-01,\n",
       "           9.0801e-01, -1.8305e-01],\n",
       "         ...,\n",
       "         [-2.1578e-01,  5.4630e-01,  3.1171e-01,  ...,  1.8021e-01,\n",
       "           7.1693e-01, -6.7160e-02],\n",
       "         [-3.0920e-01,  4.8334e-01,  3.0211e-01,  ...,  2.2885e-01,\n",
       "           6.6559e-01, -9.3169e-02],\n",
       "         [-2.9402e-01,  4.6784e-01,  3.0949e-01,  ...,  2.7821e-01,\n",
       "           5.1436e-01, -1.0211e-01]],\n",
       "\n",
       "        [[-1.0246e-01,  9.7842e-01,  1.4798e+00,  ..., -6.7322e-01,\n",
       "          -1.3459e+00, -1.5414e-01],\n",
       "         [ 1.6459e-01,  1.1261e+00,  9.7448e-01,  ..., -8.2403e-01,\n",
       "          -1.5562e+00, -6.0396e-01],\n",
       "         [ 4.7917e-01,  9.7228e-01,  1.3746e+00,  ..., -9.8250e-01,\n",
       "          -1.3523e+00, -5.8834e-01],\n",
       "         ...,\n",
       "         [ 6.3124e-02,  3.3896e-01,  1.2718e+00,  ..., -3.9970e-01,\n",
       "          -1.1031e+00, -1.3408e-01],\n",
       "         [ 1.3678e-01,  4.4807e-01,  1.2677e+00,  ..., -3.7586e-01,\n",
       "          -1.0867e+00, -2.6922e-01],\n",
       "         [ 1.4712e-01,  3.7091e-01,  1.2411e+00,  ..., -3.6103e-01,\n",
       "          -1.1337e+00, -2.6628e-01]],\n",
       "\n",
       "        [[-6.9432e-02,  1.3936e-01,  7.9762e-01,  ...,  1.1904e-01,\n",
       "           9.8823e-01,  2.6582e-01],\n",
       "         [ 5.1373e-03, -5.3534e-02,  8.8652e-01,  ..., -2.0870e-01,\n",
       "           7.9596e-01,  2.9188e-02],\n",
       "         [-1.5181e-01,  1.4075e-02,  7.6035e-01,  ..., -2.6414e-01,\n",
       "           6.3991e-01, -1.5048e-01],\n",
       "         ...,\n",
       "         [-1.6340e-01, -5.6690e-02,  7.4140e-01,  ...,  2.4665e-01,\n",
       "           7.6735e-01,  7.6984e-02],\n",
       "         [-2.2222e-01,  1.7150e-03,  7.0698e-01,  ...,  2.1065e-01,\n",
       "           7.1550e-01,  7.8734e-02],\n",
       "         [-1.9339e-01,  2.5327e-02,  7.8219e-01,  ...,  1.7633e-01,\n",
       "           6.4733e-01,  5.0552e-02]],\n",
       "\n",
       "        [[-2.3620e-01,  8.5513e-01, -8.0395e-01,  ...,  6.1217e-01,\n",
       "           3.0030e-01, -1.4919e-01],\n",
       "         [-8.6804e-02,  9.5311e-01, -6.4188e-01,  ...,  7.8669e-01,\n",
       "           2.9603e-01, -7.3501e-01],\n",
       "         [-3.0156e-01,  1.0148e+00, -3.3798e-01,  ...,  8.6336e-01,\n",
       "           4.6253e-02, -3.6234e-01],\n",
       "         ...,\n",
       "         [-1.0904e-01,  6.3199e-01, -8.4330e-01,  ...,  7.4846e-01,\n",
       "           1.0252e-01,  1.4869e-02],\n",
       "         [ 7.2192e-03,  7.3466e-01, -7.6890e-01,  ...,  6.0643e-01,\n",
       "           1.2874e-01,  3.3142e-02],\n",
       "         [-1.1083e-01,  7.6055e-01, -4.4468e-01,  ...,  6.7188e-01,\n",
       "           1.0593e-01, -3.4445e-03]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last_hidden_state is the last layer of the model\n",
    "embeddings = outputs.last_hidden_state\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6 sentences, 128 tokens per sentence, 768 features per token\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention mask size is:  torch.Size([6, 128])\n",
      "mask size is:  torch.Size([6, 128, 768])\n"
     ]
    }
   ],
   "source": [
    "# remove padding tokens with attention mask\n",
    "# need to add the 768 features per token\n",
    "attention_mask = tokens['attention_mask']\n",
    "print('attention mask size is: ', attention_mask.shape)\n",
    "mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "print('mask size is: ', mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked embeddings size:  torch.Size([6, 128, 768])\n",
      "we need to sum acros the 128 tokens to get a single vector for each sentence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0692,  0.6230,  0.0354,  ...,  0.8033,  1.6314,  0.3281],\n",
       "         [ 0.0367,  0.6842,  0.1946,  ...,  0.0848,  1.4747, -0.3008],\n",
       "         [-0.0121,  0.6543, -0.0727,  ..., -0.0326,  1.7717, -0.6812],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "        [[-0.3212,  0.8251,  1.0554,  ..., -0.1855,  0.1517,  0.3937],\n",
       "         [-0.7146,  1.0297,  1.1217,  ...,  0.0331,  0.2382, -0.1563],\n",
       "         [-0.2352,  1.1353,  0.8594,  ..., -0.4310, -0.0272, -0.2968],\n",
       "         ...,\n",
       "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
       "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
       "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.7576,  0.8399, -0.3792,  ...,  0.1271,  1.2514,  0.1365],\n",
       "         [-0.6591,  0.7614, -0.4662,  ...,  0.2259,  1.1289, -0.3611],\n",
       "         [-0.9007,  0.6791, -0.3778,  ...,  0.1142,  0.9080, -0.1830],\n",
       "         ...,\n",
       "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "        [[-0.1025,  0.9784,  1.4798,  ..., -0.6732, -1.3459, -0.1541],\n",
       "         [ 0.1646,  1.1261,  0.9745,  ..., -0.8240, -1.5562, -0.6040],\n",
       "         [ 0.4792,  0.9723,  1.3746,  ..., -0.9825, -1.3523, -0.5883],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000]],\n",
       "\n",
       "        [[-0.0694,  0.1394,  0.7976,  ...,  0.1190,  0.9882,  0.2658],\n",
       "         [ 0.0051, -0.0535,  0.8865,  ..., -0.2087,  0.7960,  0.0292],\n",
       "         [-0.1518,  0.0141,  0.7603,  ..., -0.2641,  0.6399, -0.1505],\n",
       "         ...,\n",
       "         [-0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.2362,  0.8551, -0.8040,  ...,  0.6122,  0.3003, -0.1492],\n",
       "         [-0.0868,  0.9531, -0.6419,  ...,  0.7867,  0.2960, -0.7350],\n",
       "         [-0.3016,  1.0148, -0.3380,  ...,  0.8634,  0.0463, -0.3623],\n",
       "         ...,\n",
       "         [-0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000, -0.0000]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply mask to embeddings to only get non-padded tokens\n",
    "masked_embeddings = embeddings * mask\n",
    "print('masked embeddings size: ', masked_embeddings.shape)\n",
    "print('we need to sum acros the 128 tokens to get a single vector for each sentence')\n",
    "masked_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to get mean pooling of non-padded tokens\n",
    "summed = torch.sum(masked_embeddings, 1)\n",
    "summed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counts of features that are not padding (1 if not padding, 0 if padding)\n",
    "counts = torch.clamp(mask.sum(1), min=1e-9)\n",
    "counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 15., 15.,  ..., 15., 15., 15.],\n",
       "        [22., 22., 22.,  ..., 22., 22., 22.],\n",
       "        [15., 15., 15.,  ..., 15., 15., 15.],\n",
       "        [16., 16., 16.,  ..., 16., 16., 16.],\n",
       "        [12., 12., 12.,  ..., 12., 12., 12.],\n",
       "        [14., 14., 14.,  ..., 14., 14., 14.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 768])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooled = summed / counts\n",
    "mean_pooled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0745,  0.8637,  0.1795,  ...,  0.7734,  1.7247, -0.1803],\n",
       "        [-0.3715,  0.9729,  1.0840,  ..., -0.2552, -0.2759,  0.0358],\n",
       "        [-0.5030,  0.7950, -0.1240,  ...,  0.1441,  0.9704, -0.1791],\n",
       "        [-0.0132,  0.9773,  1.4516,  ..., -0.8462, -1.4004, -0.4118],\n",
       "        [-0.2019,  0.0597,  0.8603,  ..., -0.0100,  0.8431, -0.0841],\n",
       "        [-0.2131,  1.0175, -0.8833,  ...,  0.7371,  0.1947, -0.3011]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3308892 , 0.7219259 , 0.17475471, 0.44709635, 0.5548363 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# convert from PyTorch tensor to numpy array\n",
    "mean_pooled = mean_pooled.detach().numpy()\n",
    "\n",
    "# calculate\n",
    "cosine_similarity(\n",
    "    [mean_pooled[0]],\n",
    "    mean_pooled[1:]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These similarities translate to:\n",
    "\n",
    "| Index | Sentence | Similarity |\n",
    "| --- | --- | --- |\n",
    "| 1 | \"The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.\" | 0.3309 |\n",
    "| 2 | \"The person box was packed with jelly many dozens of months later.\" | 0.7219 |\n",
    "| 3 | \"Standing on one's head at job interviews forms a lasting impression.\" | 0.1748 |\n",
    "| 4 | \"It took him a month to finish the meal.\" | 0.4471 |\n",
    "| 5 | \"He found a leprechaun in his walnut shell.\" | 0.5548 |\n",
    "\n",
    "\n",
    "So, as intended, the most similar sentence is that in index **2** - which contains the same meaning as our first sentence, without using the same words:\n",
    "\n",
    "`\"Three years later, the coffin was still full of Jello.\"`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the sentence-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)821d1/.gitattributes: 100%|██████████| 391/391 [00:00<00:00, 3.31MB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 717kB/s]\n",
      "Downloading (…)8d01e821d1/README.md: 100%|██████████| 3.95k/3.95k [00:00<00:00, 29.2MB/s]\n",
      "Downloading (…)d1/added_tokens.json: 100%|██████████| 2.00/2.00 [00:00<00:00, 11.4kB/s]\n",
      "Downloading (…)01e821d1/config.json: 100%|██████████| 625/625 [00:00<00:00, 5.00MB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 122/122 [00:00<00:00, 836kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 438M/438M [00:05<00:00, 79.2MB/s] \n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 345kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 932kB/s]\n",
      "Downloading (…)821d1/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 8.70MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 399/399 [00:00<00:00, 5.06MB/s]\n",
      "Downloading (…)8d01e821d1/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 4.54MB/s]\n",
      "Downloading (…)1e821d1/modules.json: 100%|██████████| 229/229 [00:00<00:00, 2.44MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.07446156,  0.86369616,  0.17946291, ...,  0.77344   ,\n",
       "         1.7247493 , -0.1802747 ],\n",
       "       [-0.37146357,  0.97290134,  1.0839922 , ..., -0.25521314,\n",
       "        -0.27593705,  0.03575896],\n",
       "       [-0.50298285,  0.79498583, -0.12402609, ...,  0.14406338,\n",
       "         0.9703752 , -0.179116  ],\n",
       "       [-0.01324293,  0.97728604,  1.4515941 , ..., -0.84616524,\n",
       "        -1.4004319 , -0.41184407],\n",
       "       [-0.20192575,  0.05970386,  0.8602744 , ..., -0.01000801,\n",
       "         0.84306234, -0.08407753],\n",
       "       [-0.21311863,  1.017493  , -0.88327694, ...,  0.7371028 ,\n",
       "         0.1946914 , -0.30111343]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "sentence_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3308892 , 0.7219259 , 0.17475471, 0.44709635, 0.5548363 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(\n",
    "    [sentence_embeddings[0]],\n",
    "    sentence_embeddings[1:]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These similarities translate to almost the exact same values as we calculated before:\n",
    "\n",
    "| Index | Sentence | Similarity (before) | New similarity |\n",
    "| --- | --- | --- | --- |\n",
    "| 1 | \"The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.\" | 0.3309 | 0.3309 |\n",
    "| 2 | \"The person box was packed with jelly many dozens of months later.\" | 0.7219 | 0.7219 |\n",
    "| 3 | \"Standing on one's head at job interviews forms a lasting impression.\" | 0.1748 | 0.174**7** |\n",
    "| 4 | \"It took him a month to finish the meal.\" | 0.4471 | 0.447**2** |\n",
    "| 5 | \"He found a leprechaun in his walnut shell.\" | 0.5548 | 0.554**7** |\n",
    "\n",
    "So, using `sentence-transformers` can make life much easier. But either option produces the same outcome."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eureka-trad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
