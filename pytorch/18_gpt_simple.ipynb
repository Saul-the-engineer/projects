{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "References:\n",
    "\n",
    "https://jaketae.github.io/study/gpt/\n",
    "https://medium.com/@sntaus/building-a-mini-gpt-like-language-model-from-scratch-27257bf5c145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTConfig:\n",
    "    attn_dropout = 0.1\n",
    "    embed_dropout = 0.1\n",
    "    ff_dropout = 0.1\n",
    "    \n",
    "    def __init__(\n",
    "        self, vocab_size, max_len, **kwargs\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "class GPT1Config(GPTConfig):\n",
    "    num_heads = 12\n",
    "    num_blocks = 12\n",
    "    embed_dim = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.embed_dim\n",
    "        self.num_heads = config.num_heads\n",
    "        assert embed_dim % self.num_heads == 0, \"invalid heads and embedding dimension configuration\"\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.attn_dropout = nn.Dropout(config.attn_dropout)\n",
    "        self.proj_dropout = nn.Dropout(config.ff_dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\", \n",
    "            torch.tril(torch.ones(config.max_len, config.max_len))\n",
    "            .unsqueeze(0).unsqueeze(0)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        # x.shape == (batch_size, seq_len, embed_dim)\n",
    "        k_t = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
    "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
    "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
    "        # shape == (batch_size, num_heads, seq_len, head_dim)\n",
    "        \n",
    "        attn = torch.matmul(q, k_t) / math.sqrt(q.size(-1))\n",
    "        # attn.shape == (batch_size, num_heads, seq_len, seq_len)\n",
    "        mask = self.mask[:, :, :seq_len, :seq_len]\n",
    "        attn = attn.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        attn = self.attn_dropout(attn)\n",
    "        # attn.shape == (batch_size, num_heads, seq_len, seq_len)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        y = torch.matmul(attn, v)\n",
    "        # y.shape == (batch_size, num_heads, seq_len, head_dim)\n",
    "        y = y.transpose(1, 2)\n",
    "        # y.shape == (batch_size, seq_len, num_heads, head_dim)\n",
    "        y = y.reshape(batch_size, seq_len, -1)\n",
    "        # y.shape == (batch_size, seq_len, embed_dim)\n",
    "        y = self.proj_dropout(self.proj(y))\n",
    "        return y\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.embed_dim\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiheadAttention(config)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim),\n",
    "            nn.Dropout(config.ff_dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.embed_dim\n",
    "        self.max_len = config.max_len\n",
    "        self.tok_embed = nn.Embedding(\n",
    "            config.vocab_size, embed_dim\n",
    "        )\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, config.max_len, embed_dim)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(config.embed_dropout)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(config) for _ in range(config.num_blocks)]\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, config.vocab_size)\n",
    "    \n",
    "    def forward(self, x, target=None):\n",
    "        # batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        assert seq_len <= self.max_len, \"sequence longer than model capacity\"\n",
    "        \n",
    "        tok_embedding = self.tok_embed(x)\n",
    "        # tok_embedding.shape == (batch_size, seq_len, embed_dim)\n",
    "        pos_embedding = self.pos_embed[:, :seq_len, :]\n",
    "        # pos_embedding.shape == (1, seq_len, embed_dim)\n",
    "        x = self.dropout(tok_embedding + pos_embedding)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "        x = self.fc(x)\n",
    "        # x.shape == (batch_size, seq_len, vocab_size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 0., 0.],\n",
       "          [1., 1., 0.],\n",
       "          [1., 1., 1.]]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 5\n",
    "mask = torch.tril(torch.ones(max_len, max_len)).unsqueeze(0).unsqueeze(0)\n",
    "mask\n",
    "\n",
    "seq_len = 3\n",
    "mask = mask[:, :, :seq_len, :seq_len]\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 3, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 3\n",
    "num_heads = 2\n",
    "\n",
    "attn = torch.randn(batch_size, num_heads, seq_len, seq_len)\n",
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.7825,    -inf,    -inf],\n",
       "          [-0.5122,  0.0047,    -inf],\n",
       "          [-0.2257, -1.3988,  2.2008]],\n",
       "\n",
       "         [[ 0.9375,    -inf,    -inf],\n",
       "          [ 1.4302, -1.0021,    -inf],\n",
       "          [-0.8016, -0.4134, -0.2736]]],\n",
       "\n",
       "\n",
       "        [[[ 1.8246,    -inf,    -inf],\n",
       "          [ 0.7566, -0.4946,    -inf],\n",
       "          [ 1.1573,  0.4111,  0.5795]],\n",
       "\n",
       "         [[ 1.7281,    -inf,    -inf],\n",
       "          [ 2.3234,  0.6134,    -inf],\n",
       "          [ 0.9229,  1.3463,  1.5307]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0044,    -inf,    -inf],\n",
       "          [-0.2688, -1.2587,    -inf],\n",
       "          [-0.0407, -0.6177,  1.4859]],\n",
       "\n",
       "         [[-0.3037,    -inf,    -inf],\n",
       "          [ 1.1637, -1.4806,    -inf],\n",
       "          [-0.4594, -0.7926,  1.6997]]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = attn.masked_fill(mask == 0, float(\"-inf\"))\n",
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0000, 0.0000, 0.0000],\n",
       "          [0.3736, 0.6264, 0.0000],\n",
       "          [0.0792, 0.0245, 0.8963]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000],\n",
       "          [0.9193, 0.0807, 0.0000],\n",
       "          [0.2398, 0.3536, 0.4066]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 0.0000, 0.0000],\n",
       "          [0.7775, 0.2225, 0.0000],\n",
       "          [0.4913, 0.2330, 0.2757]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000],\n",
       "          [0.8468, 0.1532, 0.0000],\n",
       "          [0.2292, 0.3500, 0.4208]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 0.0000, 0.0000],\n",
       "          [0.7291, 0.2709, 0.0000],\n",
       "          [0.1622, 0.0911, 0.7467]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000],\n",
       "          [0.9337, 0.0663, 0.0000],\n",
       "          [0.0963, 0.0690, 0.8346]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(attn, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (tok_embed): Embedding(10, 768)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (proj_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (proj_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (proj_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (proj_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (proj_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (proj_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (proj_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (proj_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (proj_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (proj_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (proj_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (proj_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 10\n",
    "max_len = 12\n",
    "\n",
    "config = GPT1Config(vocab_size, max_len)\n",
    "model = GPT(config)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence longer than model capacity\n"
     ]
    }
   ],
   "source": [
    "seq_len = 15\n",
    "\n",
    "test_input = torch.randint(high=vocab_size, size=(batch_size, seq_len))\n",
    "try:\n",
    "    model(test_input).shape\n",
    "except AssertionError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 12, 10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test_input[:, :max_len]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: PyTorch model, input tensor (batch_size x max_token_count) \n",
    "# and max_output_token_count (when to stop if <end> not encountered)\n",
    "# Function to perform inference recursively for each sequence in a batch\n",
    "def infer_recursive(model, input_vectors, max_output_token_count=10):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    outputs = []\n",
    "\n",
    "    # Loop over sequences in the batch\n",
    "    for i in range(input_vectors.shape[0]):\n",
    "        print(f\"Infering sequence {i}\")\n",
    "        input_vector = input_vectors[i].reshape(1, input_vectors.shape[1])\n",
    "        predicted_sequence = []\n",
    "        wc = 0  # Initialize word count\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            while True:\n",
    "                output = model(input_vector)  # Pass current input through model\n",
    "                predicted_index = output[0, :].argmax().item()  # Get index of predicted token\n",
    "                predicted_sequence.append(predicted_index)  # Append predicted index to sequence\n",
    "                # Stop when <end> token is predicted or the maximum output length is reached\n",
    "                if predicted_index == word_to_ix['<end>'] or wc > max_output_token_count:\n",
    "                    break\n",
    "                # Append predicted token to input and increment word count\n",
    "                input_vector = torch.cat([input_vector, torch.tensor([[predicted_index]])], dim=1)\n",
    "                wc += 1\n",
    "        outputs.append(torch.tensor(predicted_sequence))  # Append predicted sequence to outputs\n",
    "    outputs = pad_tensors(outputs)  # Pad predicted sequences to the same length\n",
    "    return outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
