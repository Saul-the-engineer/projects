{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int = 100, dropout: float = 0.1):\n",
    "        \"\"\"Initialize the PositionalEncoding module.\"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        positional_encoding = torch.zeros(seq_len, d_model)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        division_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        positional_encoding[:, 0::2] = torch.sin(position * division_term)\n",
    "        positional_encoding[:, 1::2] = torch.cos(position * division_term)\n",
    "        positional_encoding = positional_encoding.unsqueeze(0)\n",
    "        self.register_buffer(\"positional_encoding\", positional_encoding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform the forward pass of the PositionalEncoding module.\"\"\"\n",
    "        x = x + self.positional_encoding[:, : x.size(1)].requires_grad_(False)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size:int,\n",
    "        tgt_vocab_size:int,\n",
    "        src_seq_len:int,\n",
    "        tgt_seq_len:int,\n",
    "        d_model:int=512, \n",
    "        nhead:int=8,\n",
    "        num_encoder_layers:int=6, \n",
    "        num_decoder_layers:int=6, \n",
    "        dim_feedforward:int=2048, \n",
    "        dropout:float=0.1,\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_type=\"Transformer\"\n",
    "        self.d_model = d_model\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        self.src_positional_encoding = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "        self.tgt_positional_encoding = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True, # (batch, seq, d_model)\n",
    "            )\n",
    "        self.linear = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src:torch.Tensor, tgt:torch.Tensor) -> torch.Tensor:\n",
    "        src = self.src_embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.src_positional_encoding(src)\n",
    "        tgt = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt = self.tgt_positional_encoding(tgt)\n",
    "        x = self.transformer(src, tgt)\n",
    "        x = self.linear(x)\n",
    "        x = F.log_softmax(x, dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=0\n",
    "d_model=512\n",
    "\n",
    "src_vocab_size=10000\n",
    "tgt_vocab_size=10000\n",
    "\n",
    "src_seq_length=100\n",
    "tgt_seq_length=100\n",
    "\n",
    "model = TransformerModel(\n",
    "    src_vocab_size= src_vocab_size,\n",
    "    tgt_vocab_size = tgt_vocab_size,\n",
    "    src_seq_len = src_seq_length,\n",
    "    tgt_seq_len = tgt_seq_length,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0, 100, 10000])\n"
     ]
    }
   ],
   "source": [
    "src = torch.rand(batch_size, src_seq_length).long()\n",
    "tgt = torch.rand(batch_size, tgt_seq_length).long()\n",
    "\n",
    "output = model(src, tgt)\n",
    "print(output.shape) # torch.Size([batch, tgt_seq_len, tgt_vocab_size]) Seq2Seq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
