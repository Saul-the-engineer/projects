{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspiration: https://www.youtube.com/watch?v=ISNdQcPhsts&ab_channel=UmarJamil\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchmetrics\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the English to Spanish sentence pairs from Hugging Face datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilingualDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer_src, tokenizer_tgt, lang_src, lang_tgt, seq_len):\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.lang_src = lang_src\n",
    "        self.lang_tgt = lang_tgt\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64) #Special token 'SOS': tensor([2])\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64) #Special tokens 'EOS': tensor([3])\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64) #Special tokens 'PAD': tensor([1])\n",
    "\n",
    "        # Filter out data points that do not meet the required sequence length\n",
    "        self.processed_data = self._filter_dataset(dataset)\n",
    "\n",
    "    def _filter_dataset(self, dataset):\n",
    "        processed_data = []\n",
    "        for idx, src_target_pair in enumerate(dataset):\n",
    "            src_text = src_target_pair[\"translation\"][self.lang_src]\n",
    "            tgt_text = src_target_pair[\"translation\"][self.lang_tgt]\n",
    "\n",
    "            encode_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "            encode_target_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "            encode_padding_tokens = self.seq_len - len(encode_input_tokens) - 2\n",
    "            decode_padding_tokens = self.seq_len - len(encode_target_tokens) - 1\n",
    "\n",
    "            if encode_padding_tokens >= 0 and decode_padding_tokens >= 0:\n",
    "                encoder_input = torch.cat([\n",
    "                    self.sos_token,\n",
    "                    torch.tensor(encode_input_tokens, dtype=torch.int64),\n",
    "                    self.eos_token,\n",
    "                    torch.tensor([self.pad_token] * encode_padding_tokens, dtype=torch.int64)\n",
    "                ])\n",
    "\n",
    "                decoder_input = torch.cat([\n",
    "                    self.sos_token,\n",
    "                    torch.tensor(encode_target_tokens),\n",
    "                    torch.tensor([self.pad_token] * decode_padding_tokens, dtype=torch.int64)\n",
    "                ])\n",
    "\n",
    "                label = torch.cat([\n",
    "                    torch.tensor(encode_target_tokens),\n",
    "                    self.eos_token,\n",
    "                    torch.tensor([self.pad_token] * decode_padding_tokens, dtype=torch.int64)\n",
    "                ])\n",
    "\n",
    "                assert encoder_input.size(0) == self.seq_len, f\"Encoder input length does not match sequence length at index {idx}\"\n",
    "                assert decoder_input.size(0) == self.seq_len, f\"Decoder input length does not match sequence length at index {idx}\"\n",
    "                assert label.size(0) == self.seq_len, f\"Label length does not match sequence length at index {idx}\"\n",
    "\n",
    "                processed_data.append(\n",
    "                    {\n",
    "                        \"encoder_input\": encoder_input,\n",
    "                        \"decoder_input\": decoder_input,\n",
    "                        \"label\": label\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                print(f\"Skipping data point at index {idx} due to insufficient sequence length.\")\n",
    "\n",
    "        return processed_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.processed_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.processed_data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_build_tokenizer(config, dataset, language):\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(language))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(dataset, language), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n",
    "\n",
    "def get_all_sentences(dataset, language):\n",
    "    for sentence_pair in dataset:\n",
    "        yield sentence_pair[\"translation\"][language]\n",
    "\n",
    "def get_dataset(config):\n",
    "    # download dataset\n",
    "    dataset = load_dataset('opus_books', f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split='train')\n",
    "\n",
    "    # build tokenizer\n",
    "    tokenizer_src = get_or_build_tokenizer(config, dataset, config['lang_src'])\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, dataset, config['lang_tgt'])\n",
    "\n",
    "    # split dataset\n",
    "    train_size = int(len(dataset) * 0.8)\n",
    "    val_size = int(len(dataset) - train_size)\n",
    "\n",
    "    train_data_raw, val_data_raw = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_dataset = BilingualDataset(train_data_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "    valid_dataset = BilingualDataset(val_data_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "\n",
    "    # Find the maximum length of each sentence in the source and target sentence\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "\n",
    "    for item in dataset:\n",
    "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
    "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "    print(f'Max length of source sentence: {max_len_src}')\n",
    "    print(f'Max length of target sentence: {max_len_tgt}')\n",
    "    \n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    return {\n",
    "        \"batch_size\": 16,\n",
    "        \"num_epochs\": 2,\n",
    "        \"lr\": 1e-4,\n",
    "        \"seq_len\": 350,\n",
    "        \"d_model\": 512,\n",
    "        \"lang_src\": \"en\",\n",
    "        \"lang_tgt\": \"es\",\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"tmodel_\",\n",
    "        \"preload\": None,\n",
    "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
    "        \"experiment_name\": \"runs/transformer_seq2seq\",\n",
    "    }\n",
    "\n",
    "def get_weights_file_path(config, epoch: str):\n",
    "    model_folder = config[\"model_folder\"]\n",
    "    model_basename = config[\"model_basename\"]\n",
    "    model_filename = f\"{model_basename}{epoch}.pt\"\n",
    "    return str(Path(\".\") / model_folder / model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Build a Transformer model to translate English to Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int = 100, dropout: float = 0.1):\n",
    "        \"\"\"Initialize the PositionalEncoding module.\"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        positional_encoding = torch.zeros(seq_len, d_model)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        division_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        positional_encoding[:, 0::2] = torch.sin(position * division_term)\n",
    "        positional_encoding[:, 1::2] = torch.cos(position * division_term)\n",
    "        positional_encoding = positional_encoding.unsqueeze(0)\n",
    "        self.register_buffer(\"positional_encoding\", positional_encoding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform the forward pass of the PositionalEncoding module.\"\"\"\n",
    "        x = x + self.positional_encoding[:, : x.size(1)].requires_grad_(False)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size:int,\n",
    "        tgt_vocab_size:int,\n",
    "        src_seq_len:int,\n",
    "        tgt_seq_len:int,\n",
    "        d_model:int=512, \n",
    "        nhead:int=8,\n",
    "        num_encoder_layers:int=6, \n",
    "        num_decoder_layers:int=6, \n",
    "        dim_feedforward:int=2048, \n",
    "        dropout:float=0.1,\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_type=\"Transformer\"\n",
    "        self.d_model = d_model\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        self.src_positional_encoding = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "        self.tgt_positional_encoding = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True, # (batch, seq, d_model)\n",
    "            )\n",
    "        self.src_mask = self.transformer.generate_square_subsequent_mask(src_seq_len)\n",
    "        self.tgt_mask = self.transformer.generate_square_subsequent_mask(tgt_seq_len)\n",
    "        print(f\"Source mask size: {self.src_mask.shape}\")\n",
    "        print(f\"Target mask size: {self.tgt_mask.shape}\")\n",
    "        self.linear = nn.Linear(d_model, tgt_vocab_size) \n",
    "\n",
    "    def translate(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        max_length: int = 50,\n",
    "        start_symbol: int = 2,\n",
    "        stop_symbol: int = 3,\n",
    "    ) -> torch.Tensor:\n",
    "        self.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            src = self.src_embedding(src.long()) * torch.sqrt(torch.tensor(self.d_model))\n",
    "            src = self.src_positional_encoding(src)\n",
    "\n",
    "            src_mask = self.transformer.generate_square_subsequent_mask(src.size(1)).to(src.device)\n",
    "            memory = self.transformer.encoder(src, src_mask)\n",
    "\n",
    "            output_sequence  = torch.ones(1, 1, dtype=torch.long).fill_(start_symbol).to(src.device)\n",
    "            for _ in range(max_length - 1):\n",
    "                tgt_mask = self.transformer.generate_square_subsequent_mask(output_sequence.size(1)).to(src.device)\n",
    "                tgt = self.tgt_embedding(output_sequence) * torch.sqrt(torch.tensor(self.d_model))\n",
    "                tgt = self.tgt_positional_encoding(tgt)\n",
    "                out = self.transformer.decoder(tgt, memory, tgt_mask=tgt_mask)\n",
    "                out = self.linear(out)\n",
    "                prob = F.log_softmax(out, dim=-1)\n",
    "                _, next_word = torch.max(prob, dim=-1)\n",
    "                next_word = next_word[0, -1].unsqueeze(0)\n",
    "                output_sequence  = torch.cat([output_sequence, next_word.unsqueeze(0)], dim=1)  # Ensure both tensors have the same number of dimensions\n",
    "                if next_word == stop_symbol:  # Assuming the end token has the index 3\n",
    "                    break\n",
    "        return output_sequence \n",
    "\n",
    "    def get_tgt_mask(self, seq_len:int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        sample output: tensor([[0., -inf, -inf, -inf],\n",
    "                               [0., 0., -inf, -inf],\n",
    "                               [0., 0., 0., -inf],\n",
    "                               [0., 0., 0., 0.]])\n",
    "        \"\"\"\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).type(torch.int)\n",
    "        mask = mask.masked_fill(mask==1, float('-inf'))\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def get_pad_mask(self, matrix:torch.tensor, pad_token:int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        sample input = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
    "        # [False, False, False, True, True, True]\n",
    "        \"\"\"\n",
    "        mask = (matrix == pad_token)\n",
    "        return mask.any(dim=-1)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        src:torch.Tensor,\n",
    "        tgt:torch.Tensor,\n",
    "        ) -> torch.Tensor:\n",
    "        src = self.src_embedding(src) * torch.sqrt(torch.tensor(self.d_model))\n",
    "        src = self.src_positional_encoding(src) # (batch, seq, d_model)\n",
    "        tgt = self.tgt_embedding(tgt) * torch.sqrt(torch.tensor(self.d_model))\n",
    "        tgt = self.tgt_positional_encoding(tgt) # (batch, seq, d_model)\n",
    "        \n",
    "        src_key_padding_mask = self.get_pad_mask(src, 1).to(src.device)\n",
    "        #tgt_key_padding_mask = self.get_pad_mask(tgt, 1).to(src.device)\n",
    "\n",
    "        x = self.transformer(\n",
    "            src=src, \n",
    "            tgt=tgt,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_mask=self.tgt_mask,\n",
    "            ) # (batch, seq, d_model) though originally (seq, batch, d_model)\n",
    "        x = self.linear(x)\n",
    "        x = F.log_softmax(x, dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the English to Spanish sentence pairs from Hugging Face datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, num_examples=2):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "\n",
    "    source_texts = []\n",
    "    expected = []\n",
    "    predicted = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
    "            model_out = model.translate(encoder_input, max_length=max_len)\n",
    "\n",
    "            #print(f\"Encoder input size: {encoder_input}\")\n",
    "            #print(f\"Model output: {model_out}\")\n",
    "\n",
    "            src_input_text = tokenizer_src.decode(batch[\"decoder_input\"][0].tolist())\n",
    "            tgt_output_text = tokenizer_tgt.decode(batch[\"label\"][0].tolist())\n",
    "            model_out_text = tokenizer_tgt.decode(model_out[0].tolist())\n",
    "\n",
    "            source_texts.append(src_input_text)\n",
    "            expected.append(tgt_output_text)\n",
    "            predicted.append(model_out_text)\n",
    "\n",
    "            print(f\"Source: {src_input_text}\")\n",
    "            print(f\"Target: {tgt_output_text}\")\n",
    "            print(f\"Predicted: {model_out_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using {device} device.\")\n",
    "\n",
    "    Path(config[\"model_folder\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"Loading dataset...\")\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_dataset(config)\n",
    "    # subset train_dataloader and val_dataloader to 100 examples\n",
    "    # subset_train_dataloader = []\n",
    "    # subset_val_dataloader = []\n",
    "\n",
    "    # for i, data in enumerate(train_dataloader):\n",
    "    #     subset_train_dataloader.append(data)\n",
    "    #     if i >= 2:\n",
    "    #         break\n",
    "\n",
    "    # for i, data in enumerate(val_dataloader):\n",
    "    #     subset_val_dataloader.append(data)\n",
    "    #     if i >= 2:\n",
    "    #         break\n",
    "\n",
    "    # train_dataloader = subset_train_dataloader\n",
    "    # val_dataloader = subset_val_dataloader\n",
    "        \n",
    "    print(\"Dataset loaded.\")\n",
    "\n",
    "    print(\"Building model...\")\n",
    "    model = TransformerModel(\n",
    "        src_vocab_size= tokenizer_src.get_vocab_size(),\n",
    "        tgt_vocab_size = tokenizer_tgt.get_vocab_size(),\n",
    "        src_seq_len = config[\"seq_len\"],\n",
    "        tgt_seq_len = config[\"seq_len\"],\n",
    "        ).to(device)\n",
    "    print(\"Model built.\")\n",
    "\n",
    "    # Setup Tensorboard\n",
    "    print(\"Setting up hyperparameters...\")\n",
    "    writer = SummaryWriter(config[\"experiment_name\"])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"], eps=1e-09)\n",
    "\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    if config[\"preload\"] is not None:\n",
    "        model_filename = get_weights_file_path(config, config[\"preload\"])\n",
    "        print(f\"Preloading model {model_filename}\")\n",
    "        checkpoint = torch.load(config[\"preload\"])\n",
    "        initial_epoch = checkpoint[\"epoch\"] + 1\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        global_step = checkpoint[\"global_step\"]\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "    criteria = nn.CrossEntropyLoss(ignore_index=tokenizer_tgt.token_to_id(\"[PAD]\"), label_smoothing=0.1)\n",
    "    print(\"Hyperparameters set.\")\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(initial_epoch, config[\"num_epochs\"]):\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "        for batch in batch_iterator:\n",
    "            encoder_input = batch[\"encoder_input\"].to(device) # [batch, seq_len]\n",
    "            decoder_input = batch[\"decoder_input\"].to(device) # [batch, seq_len]\n",
    "            label = batch[\"label\"].to(device) # [batch, seq_len]\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(encoder_input, decoder_input)\n",
    "            loss = criteria(output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            writer.add_scalar(\"Loss/train\", loss.item(), global_step)\n",
    "            writer.flush()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "\n",
    "        # Run validation\n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config[\"seq_len\"], device, num_examples=2)\n",
    "\n",
    "\n",
    "        # Save model\n",
    "        model_filename = get_weights_file_path(config, epoch)\n",
    "        print(f\"Saving model {model_filename}\")\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"global_step\": global_step,\n",
    "            \"loss\": loss.item(),\n",
    "        }, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()\n",
    "train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size=2\n",
    "# d_model=512\n",
    "\n",
    "# src_vocab_size=30000\n",
    "# tgt_vocab_size=30000\n",
    "\n",
    "# src_seq_length=100\n",
    "# tgt_seq_length=100\n",
    "\n",
    "# model = TransformerModel(\n",
    "#     src_vocab_size= src_vocab_size,\n",
    "#     tgt_vocab_size = tgt_vocab_size,\n",
    "#     src_seq_len = src_seq_length,\n",
    "#     tgt_seq_len = tgt_seq_length,\n",
    "#     )\n",
    "\n",
    "# src = torch.rand(batch_size, src_seq_length).long()\n",
    "# tgt = torch.rand(batch_size, tgt_seq_length).long()\n",
    "\n",
    "# output = model(src, tgt)\n",
    "# print(f\"Output shape: {output.shape}\") # torch.Size([batch, tgt_seq_len, tgt_vocab_size]) Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #source_sequence = \"[SOS] Hello world [EOS] [PAD]\"\n",
    "# source_sequence = \"Hello world\"\n",
    "\n",
    "# src_tokenizer = get_or_build_tokenizer(get_config(), \"/home/saul/workspace/projects/pytorch/tokenizer_en.json\", \"en\")\n",
    "# trg_tokenizer = get_or_build_tokenizer(get_config(), \"/home/saul/workspace/projects/pytorch/tokenizer_es.json\", \"es\")\n",
    "# encode_input_tokens = src_tokenizer.encode(source_sequence).ids\n",
    "# encode_padding_tokens = src_seq_length - len(encode_input_tokens) - 2 # -2 for sos and eos token\n",
    "\n",
    "# encoder_input = torch.cat([\n",
    "#     torch.tensor([2], dtype=torch.int64),\n",
    "#     torch.tensor(src_tokenizer.encode(source_sequence).ids, dtype=torch.int64),\n",
    "#     torch.tensor([3], dtype=torch.int64),\n",
    "#     torch.tensor([1] * encode_padding_tokens, dtype=torch.int64)\n",
    "# ])\n",
    "\n",
    "# print(f\"Source sequence: {encoder_input}\")\n",
    "# print(encoder_input.shape)\n",
    "# out = model.translate(encoder_input)\n",
    "# output = trg_tokenizer.decode(out[0].tolist())\n",
    "# print(f\"Output: {output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
