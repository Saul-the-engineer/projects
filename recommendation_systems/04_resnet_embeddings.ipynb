{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings from Resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saul/.pyenv/versions/3.11.8/envs/llm/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/saul/.pyenv/versions/3.11.8/envs/llm/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained ResNet model (resnet18)\n",
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the last fully connected layer to get the embeddings\n",
    "model = nn.Sequential(*list(model.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the image preprocessing transformations\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image from a URL or local path (replace URL with your image path if needed)\n",
    "url = \"https://images.unsplash.com/photo-1593642632823-8f785ba67e45?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwzNjUyOXwwfDF8c2VhcmNofDF8fHRlY2h8ZW58MHx8fHwxNjc5MjA2NzQ2&ixlib=rb-1.2.1&q=80&w=400\"  # Replace with a valid image URL or local file path\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the image and add a batch dimension\n",
    "input_tensor = preprocess(img).unsqueeze(0)  # Shape: [1, 3, 224, 224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([1, 512, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings by passing the input through the model\n",
    "with torch.no_grad():\n",
    "    embedding = model(input_tensor)\n",
    "print(f\"Embedding shape: {embedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vector shape: torch.Size([512])\n",
      "Embedding vector:\n",
      "tensor([2.8576e+00, 1.1922e+00, 3.4011e-01, 6.4422e-01, 2.0134e-01, 2.6692e-01,\n",
      "        4.9765e-01, 1.1040e-01, 1.8118e+00, 8.5407e-01, 1.2696e+00, 1.8353e+00,\n",
      "        7.5778e-01, 1.1389e+00, 3.9937e-02, 1.8792e+00, 5.1544e-02, 1.5950e-01,\n",
      "        6.5171e-01, 1.5792e+00, 5.2563e-02, 2.1139e-01, 1.1726e+00, 1.7703e+00,\n",
      "        1.5273e+00, 1.3297e+00, 5.5108e-01, 2.4302e+00, 1.3671e-01, 2.4126e-01,\n",
      "        1.1358e+00, 7.5909e-01, 4.2556e-01, 4.9568e-01, 1.1930e-01, 6.9830e-01,\n",
      "        7.8944e-01, 3.6795e-01, 2.0914e+00, 6.5689e-01, 1.5763e+00, 2.3488e-01,\n",
      "        4.6677e-01, 1.8104e-01, 1.1846e+00, 1.1247e+00, 5.9191e-01, 1.5561e+00,\n",
      "        1.1378e+00, 1.0383e+00, 5.4494e-01, 1.5300e+00, 2.1667e-01, 1.2570e-01,\n",
      "        6.4365e-01, 6.6463e-01, 0.0000e+00, 6.5498e-01, 5.5730e-01, 2.6462e-01,\n",
      "        3.9504e-01, 1.0762e-02, 1.2542e-02, 7.9144e-01, 1.1295e+00, 7.7308e-02,\n",
      "        3.6684e-01, 3.8991e-01, 5.6734e-02, 4.3304e-01, 6.3290e-01, 1.6323e+00,\n",
      "        4.4338e-02, 1.3664e-01, 6.2402e-01, 4.4050e-01, 1.9422e+00, 6.6589e-01,\n",
      "        8.1000e-01, 8.2657e-02, 3.8288e-01, 4.4980e-01, 3.4914e-01, 8.0084e-02,\n",
      "        5.8294e-02, 6.2644e-01, 2.6956e-01, 3.3211e-01, 1.0410e+00, 1.6159e+00,\n",
      "        5.3038e-01, 1.5781e+00, 6.0019e-02, 2.4709e-01, 8.0925e-01, 8.4253e-02,\n",
      "        4.2806e+00, 1.3500e+00, 5.8220e-01, 2.5613e+00, 7.8993e-03, 1.0964e+00,\n",
      "        5.2404e-01, 2.3409e+00, 1.4730e+00, 1.1269e+00, 5.8408e-03, 7.8097e-01,\n",
      "        2.4168e-01, 2.9479e+00, 5.3270e-01, 9.4963e-01, 1.2434e-01, 1.0858e+00,\n",
      "        4.0356e-01, 6.5936e-01, 4.0244e-01, 1.9387e+00, 6.4475e-01, 1.1710e+00,\n",
      "        6.5417e-01, 3.9338e+00, 7.8445e-01, 1.5835e+00, 1.0550e-01, 7.3959e-02,\n",
      "        4.7435e-01, 2.3559e-01, 2.6464e-01, 4.4259e-01, 4.5127e-01, 1.0283e+00,\n",
      "        4.7587e-02, 2.1157e+00, 1.4014e+00, 1.0674e+00, 6.6995e-01, 1.3084e+00,\n",
      "        3.8182e-02, 1.9157e+00, 5.5865e-01, 3.8057e-01, 6.6984e-01, 2.2913e+00,\n",
      "        8.2559e-01, 7.9080e-01, 3.1126e-01, 1.5280e+00, 3.4720e-02, 5.3860e-02,\n",
      "        8.4664e-01, 8.7193e-02, 6.0005e-01, 2.0345e-01, 2.5172e-01, 2.1499e+00,\n",
      "        1.1128e+00, 3.6509e-01, 7.2766e-01, 8.5124e-01, 2.7433e+00, 3.1915e-01,\n",
      "        4.4421e-01, 2.0737e+00, 6.2827e-01, 1.3669e+00, 1.5621e-01, 5.6626e-01,\n",
      "        3.8742e+00, 1.4091e+00, 1.7773e-01, 4.2908e-01, 9.6860e-01, 8.8264e-01,\n",
      "        3.8012e-01, 1.6815e+00, 1.7175e-02, 5.3618e-01, 6.4211e-01, 1.1185e+00,\n",
      "        1.4732e+00, 2.5260e-02, 2.2571e-01, 1.9955e+00, 5.3895e-01, 2.9157e-02,\n",
      "        8.4293e-02, 1.4057e+00, 1.6032e+00, 2.8870e-01, 2.8235e-01, 6.2891e-01,\n",
      "        2.7292e+00, 8.0738e-01, 4.3490e+00, 1.2813e-01, 2.0454e+00, 2.6520e+00,\n",
      "        1.9091e+00, 2.8907e+00, 8.8275e-01, 4.5297e+00, 7.9481e-01, 2.5004e-01,\n",
      "        0.0000e+00, 1.2575e+00, 3.0108e+00, 1.0822e-01, 9.5627e-01, 1.6668e+00,\n",
      "        3.8915e-01, 3.4450e-01, 6.1118e-01, 1.0927e-01, 9.9980e-01, 1.4432e+00,\n",
      "        4.1016e+00, 2.6458e-01, 7.6137e-01, 4.5432e-01, 7.3013e-01, 5.2987e-02,\n",
      "        2.8358e+00, 8.4516e-01, 5.7175e-01, 1.0717e+00, 1.1813e+00, 6.8009e-01,\n",
      "        2.0263e-02, 9.6500e-02, 5.6733e-01, 2.1262e+00, 1.1237e+00, 5.4550e-01,\n",
      "        9.6938e-02, 3.6066e-01, 1.4933e+00, 1.4081e+00, 6.6818e-01, 1.3259e+00,\n",
      "        7.1721e-01, 5.5342e-01, 3.7281e-01, 1.2689e+00, 1.1904e+00, 1.2859e+00,\n",
      "        2.0076e-01, 5.8452e-01, 9.4003e-01, 4.0189e-02, 7.4563e-01, 8.7742e-01,\n",
      "        8.4623e-03, 7.9574e-02, 7.3350e-01, 3.3794e-01, 5.8580e-01, 1.8547e+00,\n",
      "        7.8387e-01, 1.8326e-01, 2.1829e+00, 5.1356e-01, 1.4769e+00, 1.3050e-01,\n",
      "        5.7610e-03, 2.9725e-01, 2.8971e-01, 4.9713e-01, 1.5972e+00, 1.1513e+00,\n",
      "        3.6887e-01, 1.3053e-01, 2.9035e-02, 1.2582e+00, 1.3474e+00, 1.2452e-01,\n",
      "        6.6044e-01, 2.9945e+00, 7.1173e-01, 1.5403e-01, 2.2982e-01, 2.0561e-01,\n",
      "        8.4170e-01, 7.2103e-01, 1.7632e-01, 6.8486e-01, 1.4381e-01, 2.5910e+00,\n",
      "        8.4159e-01, 5.4135e-01, 1.2172e+00, 2.7053e+00, 5.8598e-01, 3.3268e-01,\n",
      "        1.3395e-01, 2.6187e-02, 1.7074e+00, 1.3318e-01, 7.2945e-01, 9.5852e-03,\n",
      "        1.3623e+00, 5.5719e-02, 1.7095e-01, 3.4222e-01, 4.8566e-02, 1.2154e-01,\n",
      "        5.9874e-01, 3.3947e-01, 4.2133e-01, 5.0650e-01, 8.8414e-01, 9.6541e-01,\n",
      "        2.7120e+00, 7.0343e-01, 1.3017e+00, 4.4051e-01, 5.3165e-02, 1.3308e+00,\n",
      "        1.0329e-01, 6.4947e-01, 1.9440e+00, 2.0039e-01, 1.3002e+00, 8.5265e-02,\n",
      "        1.7761e-01, 1.1129e-01, 1.2616e-01, 7.4811e-01, 2.0297e-01, 1.4449e+00,\n",
      "        1.1174e+00, 6.1902e-01, 0.0000e+00, 5.3933e-01, 7.3367e-01, 7.6896e-01,\n",
      "        7.3322e-01, 3.6320e-02, 1.6985e+00, 3.2736e-01, 3.8299e-01, 2.8080e-01,\n",
      "        7.0973e-01, 1.3072e+00, 1.2592e-01, 2.8287e-01, 1.2326e+00, 1.7497e+00,\n",
      "        6.4090e-02, 5.7718e-01, 9.6715e-01, 1.0353e+00, 9.0882e-01, 3.8125e-01,\n",
      "        2.7187e-01, 5.7441e-02, 1.3657e+00, 1.6207e+00, 1.0694e-01, 2.3504e-01,\n",
      "        3.7643e+00, 7.1175e-01, 1.0414e+00, 6.4897e-01, 1.1630e+00, 5.4842e-01,\n",
      "        4.2100e-02, 1.7263e+00, 2.6868e+00, 6.5002e-01, 2.0058e-03, 1.0660e+00,\n",
      "        6.0977e-01, 4.3363e+00, 6.6744e-02, 1.4867e-01, 2.6488e-01, 1.7431e+00,\n",
      "        1.0222e-01, 1.7326e+00, 1.2492e+00, 1.7386e+00, 4.7462e-01, 7.9697e-01,\n",
      "        2.2636e-02, 1.5056e-01, 1.3003e+00, 7.4238e-02, 9.1994e-02, 2.6305e-01,\n",
      "        1.6268e+00, 1.3378e+00, 1.7238e-01, 5.3775e-02, 9.9479e-02, 8.3103e-01,\n",
      "        1.1811e+00, 1.0712e+00, 1.6420e+00, 5.5827e-01, 2.2708e-01, 4.5715e-01,\n",
      "        1.5381e+00, 2.0752e+00, 1.0194e+00, 1.1733e+00, 1.5903e+00, 8.5940e-06,\n",
      "        1.4302e-01, 1.5338e+00, 6.2866e-01, 6.7360e-01, 1.1069e+00, 3.3848e-01,\n",
      "        3.8552e-01, 2.8132e-01, 7.4529e-01, 2.5920e+00, 1.5778e-01, 4.0722e-01,\n",
      "        1.0511e+00, 8.2445e-02, 2.6790e+00, 1.7135e+00, 9.6819e-01, 4.0761e-01,\n",
      "        1.0170e-01, 1.5288e+00, 4.5056e-01, 9.5713e-01, 1.4800e+00, 1.1284e+00,\n",
      "        1.1605e-01, 1.8357e+00, 3.9117e-01, 2.8037e-01, 2.2654e+00, 8.8412e-01,\n",
      "        5.2213e-01, 7.3252e-02, 1.3694e+00, 7.1785e-01, 3.3124e-03, 1.3116e-01,\n",
      "        6.8067e-01, 7.0761e-02, 8.5803e-01, 7.0622e-01, 1.0306e+00, 1.9122e+00,\n",
      "        1.9909e+00, 9.7337e-01, 1.3078e+00, 2.6389e+00, 1.4764e+00, 3.8958e-01,\n",
      "        1.7768e+00, 1.7959e+00, 1.3398e+00, 1.2311e+00, 2.1991e-01, 2.0952e-01,\n",
      "        1.7929e-01, 6.9301e-01, 4.6103e-01, 1.1217e+00, 2.2245e-01, 1.0537e+00,\n",
      "        2.7438e-01, 0.0000e+00, 2.0805e-01, 1.2180e-01, 2.6798e+00, 1.4007e+00,\n",
      "        2.6453e-01, 7.5118e-02, 6.8406e-01, 1.1739e+00, 2.5440e+00, 1.4763e+00,\n",
      "        4.3713e-01, 6.1937e-01, 1.5065e+00, 1.0489e+00, 2.0575e+00, 2.8157e-01,\n",
      "        4.0416e-01, 9.0805e-01, 6.4194e-02, 1.9850e-01, 8.6052e-01, 6.5432e-01,\n",
      "        1.2542e-01, 3.1730e-02, 1.0027e-01, 6.2488e-01, 1.3200e+00, 7.4185e-01,\n",
      "        4.7807e-01, 1.5070e+00, 3.0232e-01, 4.8539e-01, 3.8652e-02, 1.0114e+00,\n",
      "        9.3444e-02, 1.8773e-01, 9.8507e-01, 3.7309e-01, 1.1326e+00, 2.9274e+00,\n",
      "        1.6043e-01, 7.9019e-02])\n"
     ]
    }
   ],
   "source": [
    "# Remove the extra dimensions to view as a vector\n",
    "embedding_vector = embedding.flatten()\n",
    "\n",
    "print(f\"Embedding vector shape: {embedding_vector.shape}\")\n",
    "print(f\"Embedding vector:\\n{embedding_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
