{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-tower model\n",
    "\n",
    "Labels Used in the Two-Tower Approach\n",
    "Retrieval Labels:\n",
    "\n",
    "Type: Binary (0 or 1)\n",
    "Description: These labels indicate whether a user interacted with an ad. A value of 1 means the user interacted (e.g., clicked or purchased the ad), while 0 indicates no interaction. These labels are typically used during the retrieval stage of training, helping the model learn to rank ads based on their relevance to users.\n",
    "Ranking Labels:\n",
    "\n",
    "Type: Continuous (often in the range of [0, 1])\n",
    "Description: These labels provide a more nuanced measure of the likelihood of interaction. They can represent probabilities or expected values, such as click-through rates (CTR). The ranking labels help fine-tune the model in the ranking stage, allowing it to predict how likely a user is to interact with a given ad more accurately.\n",
    "Advantages of the DeepFM Model\n",
    "Combination of Linear and Deep Learning Models:\n",
    "\n",
    "DeepFM effectively combines Factorization Machines (FM) for capturing feature interactions and Deep Neural Networks (DNN) for modeling complex patterns. This allows it to leverage both the explicit feature interactions (captured by FM) and the deeper latent patterns (captured by DNN) in the data.\n",
    "Handling Sparse Data:\n",
    "\n",
    "The FM component is particularly useful in scenarios with high-dimensional sparse data, as it efficiently captures interactions between features without needing extensive amounts of data.\n",
    "Scalability:\n",
    "\n",
    "DeepFM can handle large-scale datasets efficiently. The architecture is designed to scale well with increasing data size, making it suitable for real-world applications.\n",
    "Flexibility:\n",
    "\n",
    "The model can easily incorporate different types of features, including categorical and numerical, by embedding them appropriately. This flexibility allows it to be used across various domains, such as click prediction and recommendation systems.\n",
    "Interpretability:\n",
    "\n",
    "The FM component provides some level of interpretability, as it directly models feature interactions. This can be helpful for understanding which features contribute to predictions.\n",
    "End-to-End Learning:\n",
    "\n",
    "DeepFM supports end-to-end learning, meaning you can train it directly from raw feature inputs to predictions without needing to separate feature engineering into distinct steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, user_feature_dim, ad_feature_dim, embedding_dim):\n",
    "        super(TwoTowerModel, self).__init__()\n",
    "        # User embedding tower\n",
    "        self.user_embedding = nn.Sequential(\n",
    "            nn.Linear(user_feature_dim, 128), nn.ReLU(), nn.Linear(128, embedding_dim)\n",
    "        )\n",
    "        # Ad embedding tower\n",
    "        self.ad_embedding = nn.Sequential(\n",
    "            nn.Linear(ad_feature_dim, 128), nn.ReLU(), nn.Linear(128, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, user_features, ad_features):\n",
    "        user_emb = self.user_embedding(user_features)\n",
    "        ad_emb = self.ad_embedding(ad_features)\n",
    "\n",
    "        # Normalize for cosine similarity\n",
    "        user_emb = F.normalize(user_emb, p=2, dim=1)\n",
    "        ad_emb = F.normalize(ad_emb, p=2, dim=1)\n",
    "\n",
    "        # Calculate similarity scores\n",
    "        similarity = torch.sum(user_emb * ad_emb, dim=1)\n",
    "\n",
    "        return similarity, user_emb, ad_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_retrieval_stage(model, data_loader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Move criterion inside the function\n",
    "\n",
    "    for batch_idx, (user_features, ad_features, retrieval_labels, _) in enumerate(\n",
    "        data_loader\n",
    "    ):\n",
    "        user_features = user_features.to(device)\n",
    "        ad_features = ad_features.to(device)\n",
    "        retrieval_labels = retrieval_labels.to(\n",
    "            device\n",
    "        ).squeeze()  # Remove extra dimensions\n",
    "\n",
    "        # Get similarity scores\n",
    "        similarity, _, _ = model(user_features, ad_features)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(similarity, retrieval_labels.float())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Retrieval Stage - Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}\"\n",
    "            )\n",
    "\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Factorization Machine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorization Machine Layer\n",
    "class FactorizationMachineLayer(nn.Module):\n",
    "    def __init__(self, feature_dim, embedding_dim):\n",
    "        super(FactorizationMachineLayer, self).__init__()\n",
    "        self.embeddings = nn.Embedding(feature_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed_x = self.embeddings(x)\n",
    "        sum_square = torch.sum(embed_x, dim=1) ** 2\n",
    "        square_sum = torch.sum(embed_x**2, dim=1)\n",
    "        second_order = 0.5 * (sum_square - square_sum).unsqueeze(\n",
    "            1\n",
    "        )  # Ensure correct dimension\n",
    "        return second_order\n",
    "\n",
    "\n",
    "class DeepFM(nn.Module):\n",
    "    def __init__(self, feature_dim, embedding_dim, hidden_dims):\n",
    "        super(DeepFM, self).__init__()\n",
    "        self.fm_layer = FactorizationMachineLayer(feature_dim, embedding_dim)\n",
    "        self.embeddings = nn.Embedding(feature_dim, embedding_dim)\n",
    "\n",
    "        # Deep neural network layers\n",
    "        dnn_layers = []\n",
    "        input_dim = feature_dim * embedding_dim  # Number of inputs to DNN\n",
    "        for hidden_dim in hidden_dims:\n",
    "            dnn_layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            dnn_layers.append(nn.ReLU())\n",
    "            input_dim = hidden_dim\n",
    "        self.dnn = nn.Sequential(*dnn_layers)\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(input_dim + 1, 1)  # +1 for FM output\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        fm_out = self.fm_layer(x.long())\n",
    "        embed_x = self.embeddings(x).view(x.size(0), -1)  # Flatten embeddings\n",
    "        dnn_out = self.dnn(embed_x)\n",
    "        combined = torch.cat([fm_out, dnn_out], dim=1)\n",
    "        output = self.output(combined)\n",
    "        return output  # Sigmoid for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ranking_stage(model, data_loader, optimizer, criterion, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (features, labels) in enumerate(data_loader):\n",
    "        # Convert features to float and normalize\n",
    "        features = features.to(device).float()\n",
    "        # Ensure features are in reasonable range\n",
    "        features = (features - features.mean(dim=0, keepdim=True)) / (\n",
    "            features.std(dim=0, keepdim=True) + 1e-7\n",
    "        )\n",
    "\n",
    "        labels = labels.to(device).float()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping to prevent explosions\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Ranking Stage - Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}\"\n",
    "            )\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "# Modified data preparation for ranking stage\n",
    "def prepare_ranking_data(user_features, ad_features, ranking_labels, batch_size):\n",
    "    # Combine features\n",
    "    combined_features = torch.cat((user_features, ad_features), dim=1)\n",
    "\n",
    "    # Normalize features\n",
    "    combined_features = (\n",
    "        combined_features - combined_features.mean(dim=0, keepdim=True)\n",
    "    ) / (combined_features.std(dim=0, keepdim=True) + 1e-7)\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    ranking_dataset = TensorDataset(combined_features, ranking_labels)\n",
    "    ranking_loader = DataLoader(ranking_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return ranking_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_k(model, data_loader, device, k=100):\n",
    "    \"\"\"\n",
    "    Retrieve top k candidates from retrieval model.\n",
    "    Returns both retrieval and ranking labels for the retrieved items.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_user_features = []\n",
    "    all_ad_features = []\n",
    "    all_similarities = []\n",
    "    all_retrieval_labels = []\n",
    "    all_ranking_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for user_features, ad_features, retrieval_labels, ranking_labels in data_loader:\n",
    "            user_features = user_features.to(device)\n",
    "            ad_features = ad_features.to(device)\n",
    "\n",
    "            similarity, _, _ = model(user_features, ad_features)\n",
    "\n",
    "            # Move everything to CPU for concatenation\n",
    "            all_user_features.append(user_features.cpu())\n",
    "            all_ad_features.append(ad_features.cpu())\n",
    "            all_similarities.append(similarity.cpu())\n",
    "            all_retrieval_labels.append(retrieval_labels)\n",
    "            all_ranking_labels.append(ranking_labels)\n",
    "\n",
    "    # Concatenate all batches on CPU\n",
    "    all_user_features = torch.cat(all_user_features, dim=0)\n",
    "    all_ad_features = torch.cat(all_ad_features, dim=0)\n",
    "    all_similarities = torch.cat(all_similarities, dim=0)\n",
    "    all_retrieval_labels = torch.cat(all_retrieval_labels, dim=0)\n",
    "    all_ranking_labels = torch.cat(all_ranking_labels, dim=0)\n",
    "\n",
    "    # Get top k indices on CPU\n",
    "    top_k_indices = torch.topk(\n",
    "        all_similarities, k=min(k, len(all_similarities))\n",
    "    ).indices\n",
    "\n",
    "    # Gather top k items\n",
    "    retrieved_user_features = all_user_features[top_k_indices]\n",
    "    retrieved_ad_features = all_ad_features[top_k_indices]\n",
    "    retrieved_ranking_labels = all_ranking_labels[top_k_indices]\n",
    "\n",
    "    # Move results back to the specified device\n",
    "    retrieved_user_features = retrieved_user_features.to(device)\n",
    "    retrieved_ad_features = retrieved_ad_features.to(device)\n",
    "    retrieved_ranking_labels = retrieved_ranking_labels.to(device)\n",
    "\n",
    "    return retrieved_user_features, retrieved_ad_features, retrieved_ranking_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data setup\n",
    "\n",
    "User Features\n",
    "For each user, the data might include:\n",
    "\n",
    "Age: Integer value representing age (e.g., 25, 40).\n",
    "Gender: Categorical (encoded as integer, e.g., 0 for male, 1 for female).\n",
    "Country: Categorical, converted to numerical IDs (e.g., 1 for USA, 2 for Canada).\n",
    "Following/Followers: Number of people the user follows and their followers (e.g., 150 follows, 300 followers).\n",
    "\n",
    "Ad Features\n",
    "Each ad might include:\n",
    "\n",
    "Ad Category: Categorical feature describing the ad type (e.g., 0 for electronics, 1 for clothing).\n",
    "Price: Numeric feature for ad item price.\n",
    "Brand Name: Numeric ID representing brand (e.g., 12 for Nike).\n",
    "Conversion Rate: Historical interaction rate, a float (e.g., 0.05 representing 5% conversion rate).\n",
    "Likes and Shares: Historical likes and shares count for the ad.\n",
    "\n",
    "Labels\n",
    "Binary label indicating if a user interacted with the ad:\n",
    "\n",
    "1 for positive interaction (e.g., click or purchase).\n",
    "0 for no interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "num_samples = 10000\n",
    "user_feature_dim = 5\n",
    "ad_feature_dim = 5\n",
    "embedding_dim = 64\n",
    "num_epochs = 2\n",
    "k = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generate synthetic data with two types of labels\n",
    "user_features = torch.randn(num_samples, user_feature_dim)\n",
    "ad_features = torch.randn(num_samples, ad_feature_dim)\n",
    "\n",
    "# Retrieval labels: Binary labels indicating if the ad is relevant (1) or not (0)\n",
    "retrieval_labels = torch.randint(0, 2, (num_samples, 1)).float()\n",
    "\n",
    "# Ranking labels: Continuous values indicating the likelihood of interaction (e.g., CTR)\n",
    "# These could be more fine-grained than retrieval labels\n",
    "ranking_labels = torch.rand(num_samples, 1)  # Values between 0 and 1\n",
    "\n",
    "# Create dataset and dataloader with both types of labels\n",
    "dataset = TensorDataset(user_features, ad_features, retrieval_labels, ranking_labels)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "two_tower_model = TwoTowerModel(user_feature_dim, ad_feature_dim, embedding_dim).to(\n",
    "    device\n",
    ")\n",
    "deepfm_model = DeepFM(\n",
    "    feature_dim=user_feature_dim + ad_feature_dim,\n",
    "    embedding_dim=16,\n",
    "    hidden_dims=[128, 64, 32],\n",
    ").to(device)\n",
    "\n",
    "# Optimizers and losses\n",
    "retrieval_optimizer = torch.optim.Adam(two_tower_model.parameters(), lr=0.001)\n",
    "ranking_optimizer = torch.optim.Adam(deepfm_model.parameters(), lr=0.001)\n",
    "retrieval_criterion = nn.BCEWithLogitsLoss  # Binary cross entropy for retrieval\n",
    "ranking_criterion = nn.BCEWithLogitsLoss()  # Mean squared error for ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/2\n",
      "Retrieval Stage - Epoch 0, Batch 0, Loss: 0.6784\n",
      "Retrieval Stage - Epoch 0, Batch 100, Loss: 0.6775\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mean(): could not infer output dtype. Input dtype must be either a floating point or complex dtype. Got: Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 26\u001b[0m\n\u001b[1;32m     21\u001b[0m ranking_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     22\u001b[0m     ranking_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Train ranking model with ranking labels\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m ranking_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ranking_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeepfm_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mranking_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mranking_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mranking_criterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Retrieval Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretrieval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Ranking Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mranking_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m )\n",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m, in \u001b[0;36mtrain_ranking_stage\u001b[0;34m(model, data_loader, optimizer, criterion, device, epoch)\u001b[0m\n\u001b[1;32m      8\u001b[0m features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Ensure features are in reasonable range\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m features \u001b[38;5;241m=\u001b[39m (features \u001b[38;5;241m-\u001b[39m \u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m) \u001b[38;5;241m/\u001b[39m (\n\u001b[1;32m     11\u001b[0m     features\u001b[38;5;241m.\u001b[39mstd(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-7\u001b[39m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mean(): could not infer output dtype. Input dtype must be either a floating point or complex dtype. Got: Long"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    # Train retrieval model - note we're not passing criterion anymore\n",
    "    retrieval_loss = train_retrieval_stage(\n",
    "        two_tower_model, data_loader, retrieval_optimizer, device, epoch\n",
    "    )\n",
    "\n",
    "    # Retrieve top k items and get corresponding ranking labels\n",
    "    retrieved_user_features, retrieved_ad_features, retrieved_ranking_labels = (\n",
    "        retrieve_top_k(two_tower_model, data_loader, device, k)\n",
    "    )\n",
    "\n",
    "    # Prepare data for ranking stage\n",
    "    if retrieved_user_features is not None:\n",
    "        ranking_features = torch.cat(\n",
    "            (retrieved_user_features, retrieved_ad_features), dim=1\n",
    "        )\n",
    "        ranking_dataset = TensorDataset(ranking_features, retrieved_ranking_labels)\n",
    "        ranking_loader = DataLoader(\n",
    "            ranking_dataset, batch_size=batch_size, shuffle=True\n",
    "        )\n",
    "\n",
    "        # Train ranking model with ranking labels\n",
    "        ranking_loss = train_ranking_stage(\n",
    "            deepfm_model,\n",
    "            ranking_loader,\n",
    "            ranking_optimizer,\n",
    "            ranking_criterion,\n",
    "            device,\n",
    "            epoch,\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1} - Retrieval Loss: {retrieval_loss:.4f}, Ranking Loss: {ranking_loss:.4f}\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
