{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Bag of Words (CBOW) Model\n",
    "\n",
    "The Continuous Bag of Words (CBOW) model is a neural network model that predicts a word given its context. The context is defined as the surrounding words. The model is trained on a large corpus of text and learns to predict the target word based on the context words. The CBOW model is a type of word embedding model that learns a dense vector representation of words in a continuous vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"we are learning natural language processing\",\n",
    "    \"learning embeddings is fun\",\n",
    "    \"we are creating word embeddings with pytorch\",\n",
    "    \"pytorch makes machine learning easy\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['are', 'learning'], 'we'), (['we', 'learning', 'natural'], 'are'), (['we', 'are', 'natural', 'language'], 'learning'), (['are', 'learning', 'language', 'processing'], 'natural'), (['learning', 'natural', 'processing'], 'language'), (['natural', 'language'], 'processing'), (['embeddings', 'is'], 'learning'), (['learning', 'is', 'fun'], 'embeddings'), (['learning', 'embeddings', 'fun'], 'is'), (['embeddings', 'is'], 'fun'), (['are', 'creating'], 'we'), (['we', 'creating', 'word'], 'are'), (['we', 'are', 'word', 'embeddings'], 'creating'), (['are', 'creating', 'embeddings', 'with'], 'word'), (['creating', 'word', 'with', 'pytorch'], 'embeddings'), (['word', 'embeddings', 'pytorch'], 'with'), (['embeddings', 'with'], 'pytorch'), (['makes', 'machine'], 'pytorch'), (['pytorch', 'machine', 'learning'], 'makes'), (['pytorch', 'makes', 'learning', 'easy'], 'machine'), (['makes', 'machine', 'easy'], 'learning'), (['machine', 'learning'], 'easy')]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the corpus to get context-target pairs\n",
    "# In other words what are the words that are surrounding a word within a window size\n",
    "# Similar to the skip-gram model/masked language model in BERT\n",
    "def build_context_target_pairs(corpus, window_size=2):\n",
    "    # Tokenize the sentences as words\n",
    "    tokenized_sentences = [sentence.split() for sentence in corpus]\n",
    "    # Create a vocabulary of unique words\n",
    "    vocabulary = set([word for sentence in tokenized_sentences for word in sentence])\n",
    "    # Create a mapping from words to indices and vice versa\n",
    "    word_to_idx = {word: i for i, word in enumerate(vocabulary)}\n",
    "    # Create a mapping from indices to words\n",
    "    idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
    "\n",
    "    # Create context-target pairs\n",
    "    pairs = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        # For each word in the sentence\n",
    "        for i, word in enumerate(sentence):\n",
    "            context = []\n",
    "            # Parse the context window around the word\n",
    "            for j in range(-window_size, window_size + 1):\n",
    "                if j != 0 and 0 <= i + j < len(sentence):\n",
    "                    context.append(sentence[i + j])\n",
    "            if context:\n",
    "                pairs.append((context, word))\n",
    "\n",
    "    return pairs, word_to_idx, idx_to_word\n",
    "\n",
    "\n",
    "pairs, word_to_idx, idx_to_word = build_context_target_pairs(corpus)\n",
    "print(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW Model Definition\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context_word_idxs):\n",
    "        # Look up embeddings for context words\n",
    "        context_embeddings = self.embeddings(context_word_idxs)\n",
    "        # Average the embeddings along the context dimension\n",
    "        context_mean = context_embeddings.mean(dim=1)\n",
    "        # Predict target word\n",
    "        out = self.linear(context_mean)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 10\n",
    "vocab_size = len(word_to_idx)\n",
    "model = CBOW(vocab_size, embedding_dim)\n",
    "\n",
    "# Loss and Optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 55.8372\n",
      "Epoch 20/100, Loss: 49.9302\n",
      "Epoch 30/100, Loss: 44.9566\n",
      "Epoch 40/100, Loss: 40.7091\n",
      "Epoch 50/100, Loss: 37.0343\n",
      "Epoch 60/100, Loss: 33.8157\n",
      "Epoch 70/100, Loss: 30.9667\n",
      "Epoch 80/100, Loss: 28.4236\n",
      "Epoch 90/100, Loss: 26.1390\n",
      "Epoch 100/100, Loss: 24.0771\n"
     ]
    }
   ],
   "source": [
    "# Training the CBOW Model\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for context, target in pairs:\n",
    "        context_idxs = torch.tensor(\n",
    "            [word_to_idx[word] for word in context], dtype=torch.long\n",
    "        ).unsqueeze(0)\n",
    "        target_idx = torch.tensor([word_to_idx[target]], dtype=torch.long)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(context_idxs)\n",
    "        loss = loss_function(output, target_idx)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acessing the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Embeddings:\n",
      "pytorch: [ 1.4685774  -0.38831982 -1.2889267   0.09289108 -0.14142293  0.22172852\n",
      " -0.7539682   1.6191927  -1.5351536   0.67711204]\n",
      "creating: [-0.8724453   2.1080534  -0.23306686 -0.53616524  0.866615    0.1564897\n",
      " -0.31115413 -1.251447    0.3580911   0.01662499]\n",
      "processing: [ 0.9482841   0.48435536 -0.8159492   1.4652754   0.6811016  -0.87585086\n",
      " -0.12284453 -0.5475216   0.5106665   1.5228214 ]\n",
      "learning: [ 0.6470066   0.811096   -3.0511477  -0.8240166   1.2186054  -0.27947414\n",
      "  0.6884183  -0.70278436  0.8353686  -1.084025  ]\n",
      "easy: [ 1.730673    1.3103616   0.02741366 -1.0480531  -0.2312773   1.2492096\n",
      " -0.34549558  1.1095289  -0.8407805  -0.84121364]\n",
      "language: [ 1.1508207  -0.12721634 -0.395085   -1.00345    -1.7495767  -1.5213605\n",
      " -0.58156365  0.5056273  -0.03067692 -1.1313243 ]\n",
      "makes: [ 0.4294922   0.6526363  -0.02071382 -1.4665662  -0.5823222  -0.08748587\n",
      "  0.12629311  1.453429    2.2575507   0.61133784]\n",
      "we: [-1.5584903  -0.46279067  1.2836462  -1.2402459  -0.13815068  3.1141896\n",
      "  0.5366285  -0.23266892 -1.4584973  -2.191675  ]\n",
      "embeddings: [-0.34448907  0.17196865 -0.3271827   0.25072217 -1.7416956   0.27185377\n",
      " -1.1879513   0.7389292  -0.6088737   1.5496832 ]\n",
      "word: [ 1.1495881   0.42210916 -1.1397645   0.24177943  1.3050904  -0.5454368\n",
      "  0.28908476  0.7777135  -0.078057   -1.9185895 ]\n",
      "is: [-0.71544695 -1.0808632   1.6985259   0.8016914   1.9021417  -0.5233343\n",
      " -1.2115053   1.0825487  -0.850371    0.4625188 ]\n",
      "are: [ 1.2784932   0.3328345   0.5771757   1.6436127   1.4776291   2.1166646\n",
      "  0.70161235 -0.4408809   0.06578923  0.30621982]\n",
      "machine: [-2.354739   -1.3202806   1.1174494   0.38499442  0.5878469   1.4456865\n",
      " -0.8330712  -0.10515606  1.1305854  -0.69712615]\n",
      "with: [-0.92441916 -2.2068741  -1.1307789   0.40912184  1.1356548  -1.5195134\n",
      " -0.4544582   0.719527   -0.46025026  0.2371881 ]\n",
      "fun: [-0.9690477   0.19802186 -1.17731    -0.22699717  0.32861486  1.3501283\n",
      "  0.88942     0.47053984  0.4086686  -0.8093298 ]\n",
      "natural: [-0.6554838   0.11534524  0.63778776 -1.3367985   0.39263132 -1.5256153\n",
      "  1.3868842   0.19304553 -0.7532399   0.5889334 ]\n"
     ]
    }
   ],
   "source": [
    "# Access embeddings\n",
    "embeddings = model.embeddings.weight.data\n",
    "print(\"\\nWord Embeddings:\")\n",
    "for word, idx in word_to_idx.items():\n",
    "    print(f\"{word}: {embeddings[idx].numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token 'learning' is represented by the vector embedding of:\n",
      "tensor([ 0.6470,  0.8111, -3.0511, -0.8240,  1.2186, -0.2795,  0.6884, -0.7028,\n",
      "         0.8354, -1.0840])\n"
     ]
    }
   ],
   "source": [
    "word = \"learning\"\n",
    "print(\n",
    "    f\"The token '{word}' is represented by the vector embedding of:\\n{embeddings[word_to_idx[word]]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
