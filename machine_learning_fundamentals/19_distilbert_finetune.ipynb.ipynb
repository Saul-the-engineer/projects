{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from typing import List, Tuple, Optional\n",
    "from scipy.stats import zscore\n",
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief Data Exploration\n",
    "\n",
    "My goal is to identify:\n",
    "1. Are the topics balanced?\n",
    "2. Is the question or excerpt the the reason for the topic?\n",
    "3. Length of the question and excerpt\n",
    "\n",
    "However based on the assumption that this data is correct we just need to focus on the modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = ...\n",
    "unprocessed = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process dataset with helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the labels to integers for the model\n",
    "class LabelMapper:\n",
    "    def __init__(self, labels: List[str]):\n",
    "        self.labels = labels\n",
    "        self.label_map = {label: i for i, label in enumerate(labels)}\n",
    "        self.inverse_map = {i: label for i, label in enumerate(labels)}\n",
    "    \n",
    "    def map(self, label):\n",
    "        return self.label_map[label]\n",
    "    \n",
    "    def inverse(self, label):\n",
    "        return self.inverse_map[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_question_answer(row: pd.Series):\n",
    "    return f\"Question: {row['question']} Excerpt: {row['excerpt']}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the question and answer into a single string for extra context map the labels to integers\n",
    "processed['combined'] = processed.apply(combine_question_answer, axis=1)\n",
    "processed['length'] = processed['combined'].apply(len)\n",
    "label_mapper = LabelMapper(processed['topic'].unique())\n",
    "processed['label'] = processed['topic'].apply(label_mapper.map)\n",
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max length: {processed['length'].max()} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed['combined'] = unprocessed.apply(combine_question_answer, axis=1)\n",
    "unprocessed['length'] = unprocessed['combined'].apply(len)\n",
    "unprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max length: {unprocessed['length'].max()} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create stratified train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the data into x and y\n",
    "x = processed['combined']\n",
    "y = processed['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, val, and test sets\n",
    "stratified_splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(stratified_splitter.split(x, y))\n",
    "train_idx, val_idx = next(stratified_splitter.split(train_idx, y[train_idx]))\n",
    "\n",
    "# Split the data into train, val, and test sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]\n",
    "x_test, y_test = x[test_idx], y[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the labels in the train, val, and test sets\n",
    "train_topics = y_train.map(label_mapper.inverse)\n",
    "val_topics = y_val.map(label_mapper.inverse) \n",
    "test_topics = y_test.map(label_mapper.inverse)\n",
    "\n",
    "# Calculate proportions for each set\n",
    "train_proportions = train_topics.value_counts() / len(train_topics)\n",
    "val_proportions = val_topics.value_counts() / len(val_topics)\n",
    "test_proportions = test_topics.value_counts() / len(test_topics)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "train_proportions.plot(kind='bar', color='blue', alpha=0.7, label='Train')\n",
    "val_proportions.plot(kind='bar', color='orange', alpha=0.7, label='Validation')\n",
    "test_proportions.plot(kind='bar', color='green', alpha=0.7, label='Test')\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Proportion')\n",
    "plt.title('Distribution of Topics in Different Sets')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x: pd.Series, tokenizer: AutoTokenizer, max_length: int, y: Optional[pd.Series] = None,):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x.iloc[idx]\n",
    "        \n",
    "        # Tokenize the input text\n",
    "        inputs = self.tokenizer(\n",
    "            x,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "            )\n",
    "        input_ids = inputs['input_ids'].squeeze().long()\n",
    "        attention_mask = inputs['attention_mask'].squeeze().long()\n",
    "        \n",
    "        if self.y is not None:\n",
    "            y = self.y.iloc[idx]\n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'labels': torch.tensor(y).long()\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "max_length = 512\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CustomDataset for each set\n",
    "dataset_train = CustomDataset(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    tokenizer, \n",
    "    max_length=max_length,\n",
    "    )\n",
    "\n",
    "dataset_val = CustomDataset(\n",
    "    x_val, \n",
    "    y_val, \n",
    "    tokenizer, \n",
    "    max_length=max_length,\n",
    "    )\n",
    "\n",
    "dataset_test = CustomDataset(\n",
    "    x_test, \n",
    "    y_test, \n",
    "    tokenizer, \n",
    "    max_length=max_length,\n",
    "    )\n",
    "\n",
    "# Create a DataLoader for each set\n",
    "dataloder_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    )\n",
    "\n",
    "dataloader_val = DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    )\n",
    "\n",
    "dataloader_test = DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBERTClassifier(nn.Module):\n",
    "    def __init__(self, n_classes: int):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state_cls = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            return loss, logits\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If we need a custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiRegressionTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"label\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = DistilBERTClassifier(n_classes=len(label_mapper.labels)).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "output_dir = './results'\n",
    "logging_steps = len(dataloder_train) // batch_size\n",
    "num_training_steps = n_epochs * len(dataloder_train)\n",
    "num_warmup_steps = int(num_training_steps * 0.1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # Basic configuration and paths\n",
    "    output_dir = output_dir,\n",
    "    log_level = 'error',\n",
    "\n",
    "    # Training and evaluation configuration\n",
    "    num_train_epochs = n_epochs, # Consider adjusting based on observed convergence\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size = batch_size,\n",
    "    learning_rate = 3e-5,\n",
    "    weight_decay = 0.01,\n",
    "    warmup_steps = num_warmup_steps,\n",
    "    lr_scheduler_type = 'cosine', # Consider adjusting to 'linear' or 'polynomial'\n",
    "\n",
    "    # Logging and Saving configuration\n",
    "    logging_dir = output_dir,\n",
    "    logging_steps = logging_steps,\n",
    "    disable_tqdm = False,\n",
    "    save_strategy = 'steps',\n",
    "    save_steps = logging_steps,\n",
    "    save_total_limit = 1,\n",
    "\n",
    "    # Evaluation configuration\n",
    "    evaluation_strategy = 'steps',\n",
    "    eval_steps = logging_steps,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = 'eval_loss',\n",
    "    greater_is_better = False,\n",
    "\n",
    "    # Optimizer configuration\n",
    "    optim= \"adamw_torch\",\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.999,\n",
    "    adam_epsilon = 1e-8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Trainer\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = dataset_train,\n",
    "    eval_dataset = dataset_val,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trainer.save_model(f\"{output_dir}/model.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "        model: torch.nn.Module,\n",
    "        dataloader: DataLoader,\n",
    "        device: torch.device,\n",
    "        ) -> Tuple[List[int], Optional[List[int]]]:\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    predictions: List[int] = []\n",
    "    all_labels: List[int] = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            \n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            \n",
    "            # Check if labels exist in the batch\n",
    "            if 'labels' in batch:\n",
    "                labels = batch['labels'].to(device)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # If no labels were found, return None for all_labels\n",
    "    if len(all_labels) == 0:\n",
    "        all_labels = None\n",
    "    \n",
    "    return predictions, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Train, Validation, and Test set performance\n",
    "y_train_pred, y_train_true = predict(model, dataloder_train, device)\n",
    "y_val_pred, y_val_true = predict(model, dataloader_val, device)\n",
    "y_test_pred, y_test_true = predict(model, dataloader_test, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalutation Metrics\n",
    "\n",
    "### Precision\n",
    "The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label a negative sample as positive.\n",
    "\n",
    "### Recall\n",
    "The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "\n",
    "### F1-Score\n",
    "The F1-score is the harmonic mean of precision and recall. It provides a single metric that balances both precision and recall. The F1-score reaches its best value at 1 and its worst value at 0. It is a useful metric when you want to seek a balance between precision and recall.\n",
    "\n",
    "### Support\n",
    "Support indicates the number of occurrences of each class in the true labels (y_true). It provides insight into the distribution of classes in the dataset and can help evaluate the significance of the precision, recall, and F1-score for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for each set\n",
    "train_report = classification_report(y_train_true, y_train_pred, target_names=label_mapper.labels, output_dict=True)\n",
    "val_report = classification_report(y_val_true, y_val_pred, target_names=label_mapper.labels, output_dict=True)\n",
    "test_report = classification_report(y_test_true, y_test_pred, target_names=label_mapper.labels, output_dict=True)\n",
    "\n",
    "train_report = pd.DataFrame(train_report).transpose()\n",
    "val_report = pd.DataFrame(val_report).transpose()\n",
    "test_report = pd.DataFrame(test_report).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm.\n",
    "\n",
    "Our results show that the model preforms very well, but has struggled on the topics of 'android', 'apple' and 'unix'. This is likely due to the fact that these topics are very similar in nature and the model has struggled to differentiate between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Confusion Matrix\n",
    "confusion_matrix = confusion_matrix(y_test_true, y_test_pred)\n",
    "\n",
    "# Plot confusion matrix using Seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(font_scale=1.2)  # Adjust font scale if needed\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=label_mapper.labels, yticklabels=label_mapper.labels)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an InferenceDataset for the unprocessed data\n",
    "dataset_inference = CustomDataset(\n",
    "    x=unprocessed['combined'],\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_length,\n",
    "    )\n",
    "\n",
    "# Create a DataLoader for the inference set\n",
    "dataloader_inference = DataLoader(\n",
    "    dataset_inference,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels for the unprocessed data\n",
    "y_inference_pred, _ = predict(model, dataloader_inference, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the predicted labels to the unprocessed data and map them back to the original labels\n",
    "unprocessed['label'] = pd.Series(y_inference_pred)\n",
    "unprocessed['topic'] = unprocessed['label'].map(label_mapper.inverse)\n",
    "unprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed[['topic', 'question', 'excerpt']].to_json('unprocessed-data-with-labels.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization Helper with UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embeddings(\n",
    "        model: torch.nn.Module,\n",
    "        dataloader: DataLoader,\n",
    "        device: torch.device,\n",
    "        ) -> List[np.ndarray]:\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    sentence_embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            # Pass data through the model\n",
    "            hidden_state = model.bert(input_ids, attention_mask).last_hidden_state\n",
    "            # Mask the output to ignore padding tokens\n",
    "            mask = attention_mask.unsqueeze(-1).expand(hidden_state.size()).float()\n",
    "            # Apply the mask and calculate mean pooling\n",
    "            masked_output = hidden_state * mask\n",
    "            embedding = masked_output.sum(1) / mask.sum(1)\n",
    "            # Append the embeddings to the list\n",
    "            sentence_embeddings.extend(embedding.cpu().numpy())\n",
    "\n",
    "    sentence_embeddings = np.array(sentence_embeddings)\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sentence embeddings for the unprocessed data\n",
    "unprocessed_embeddings = get_sentence_embeddings(model, dataloader_inference, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the dimensionality of the embeddings using UMAP\n",
    "umap_model = umap.UMAP(n_neighbors=10, min_dist=0.1, n_components=2)\n",
    "umap_embeddings = umap_model.fit_transform(unprocessed_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed['embedding'] = unprocessed_embeddings.tolist()\n",
    "unprocessed['umap_x'] = umap_embeddings[:, 0]\n",
    "unprocessed['umap_y'] = umap_embeddings[:, 1]\n",
    "#unprocessed.to_parquet('unprocessed_with_embeddings.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of UMAP embeddings with topic labels\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='umap_x', y='umap_y', hue='topic', data=unprocessed, palette='tab20', legend='full', alpha=0.7)\n",
    "\n",
    "# Add labels and other plot elements\n",
    "plt.title('UMAP Embedding with Topic Labels')\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.legend(title='Topic', loc='upper right', bbox_to_anchor=(1.25, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cluster centroids for each topic\n",
    "df_centroids = pd.DataFrame(columns=['topic', 'centroid_x', 'centroid_y'])\n",
    "for label in label_mapper.labels:\n",
    "    centroid_x = unprocessed[unprocessed['topic'] == label]['umap_x'].mean()\n",
    "    centroid_y = unprocessed[unprocessed['topic'] == label]['umap_y'].mean()\n",
    "    df_temp = pd.DataFrame({'topic': [label], 'centroid_x': [centroid_x], 'centroid_y': [centroid_y]})\n",
    "    df_centroids = pd.concat([df_centroids, df_temp], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distances between each point and the respective centroid\n",
    "distances = []\n",
    "for i, row in unprocessed.iterrows():\n",
    "    centroid_x = df_centroids[df_centroids['topic'] == row['topic']]['centroid_x'].values[0]\n",
    "    centroid_y = df_centroids[df_centroids['topic'] == row['topic']]['centroid_y'].values[0]\n",
    "    distance = np.sqrt((row['umap_x'] - centroid_x) ** 2 + (row['umap_y'] - centroid_y) ** 2)\n",
    "    distances.append(distance)\n",
    "\n",
    "unprocessed['distance'] = distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the distances with z-score\n",
    "for i, topic in enumerate(label_mapper.labels):\n",
    "    df_subset = unprocessed[unprocessed['topic'] == topic]\n",
    "    distances = df_subset['distance']\n",
    "    normalized_distances = zscore(distances)\n",
    "    unprocessed.loc[unprocessed['topic'] == topic, 'distance_z_score'] = normalized_distances\n",
    "\n",
    "unprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hexbin plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.hexbin(unprocessed['umap_x'], unprocessed['umap_y'], C=unprocessed['distance_z_score'], gridsize=30, cmap='viridis')\n",
    "plt.colorbar(label='Distance Z-Score')\n",
    "plt.xlabel('UMAP X')\n",
    "plt.ylabel('UMAP Y')\n",
    "plt.title('Hexbin Plot of Distance Z-Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up subplots for each topic\n",
    "num_topics = len(label_mapper.labels)\n",
    "fig, axs = plt.subplots(num_topics, 1, figsize=(8, 6 * num_topics))\n",
    "\n",
    "# Iterate over each topic\n",
    "for i, topic in enumerate(label_mapper.labels):\n",
    "    # Filter data points for the current topic\n",
    "    topic_data = unprocessed[unprocessed['topic'] == topic]\n",
    "    \n",
    "    # Plot histogram of distances\n",
    "    axs[i].hist(topic_data['distance_z_score'], bins=20, color='skyblue', edgecolor='black')\n",
    "    axs[i].set_title(f'Distribution of Distances for Topic: {topic}')\n",
    "    axs[i].set_xlabel('Distance to Centroid')\n",
    "    axs[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming distances are supposed to be normally distributed and the 68-95-99.7 rule applies we subset values further than 2 stds from centroid\n",
    "annomalies = unprocessed[unprocessed['distance_z_score'].abs() > 2]\n",
    "annomalies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gretel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
