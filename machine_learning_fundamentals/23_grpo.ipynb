{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Group Relative Policy Optimization (GRPO)**  \n",
    "\n",
    "**Group Relative Policy Optimization (GRPO)** enhances traditional Proximal Policy Optimization (PPO) by reducing noise and variance during training, especially when dealing with tasks like text generation. Instead of updating the policy using a single response per query, GRPO samples multiple responses and averages their rewards to capture a more representative signal. This approach helps smooth out noise that can arise from individual, potentially suboptimal responses and ensures that policy updates are guided by a more stable and generalizable signal. By reducing the reliance on any one response, GRPO stabilizes the optimization process and prevents the model from overfitting to specific samples.\n",
    "\n",
    "The core reason GRPO reduces variance is through reward aggregation across multiple sampled responses. When relying on a single path (as in vanilla PPO), randomness in sampling or noisy feedback can cause unstable updates. By averaging rewards across multiple responses, GRPO approximates the expected reward, mitigating the effect of outliers and noisy samples. This variance reduction allows the policy to learn robustly, generalizing its behavior across a broader range of possible outputs without being overly sensitive to specific responses.\n",
    "\n",
    "Probability ratio clipping further stabilizes GRPO by preventing excessive policy shifts during updates. The ratio, which compares how much the new policy diverges from the old, is clipped within a defined range to avoid over-updating based on extreme samples. Combined with reward averaging, this mechanism ensures that the model takes gradual, meaningful steps in optimization without erratic behavior. In text generation, this approach prevents the model from overreacting to rare or overly favorable completions, fostering controlled exploration of diverse outputs.\n",
    "\n",
    "GRPO's effectiveness also comes from incorporating techniques like KL divergence regularization, which keeps the policy anchored to its original language generation capabilities. The grouping mechanism—sampling multiple responses per query—is particularly valuable when rewards are derived from human feedback, which can be inconsistent or noisy. By averaging across diverse responses, GRPO reduces the risk of overfitting to specific preferences, resulting in a more generalizable and scalable policy for tasks involving complex and sequential outputs.\n",
    "\n",
    "This group-level aggregation is critical when working with long, complex outputs (like text generation) because the reward signal for a single output might be noisy. By considering multiple completions, we smooth out the noise.\n",
    "\n",
    "## Full GRPO Loss Function\n",
    "\n",
    "$$\n",
    "\\mathcal{J}_{\\text{GRPO}}(\\theta) = \\mathbb{E} \\left[ q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(o | q) \\right] \\Bigg[ \\frac{1}{G} \\sum_{i=1}^G \\min \\Bigg( \\frac{\\pi_{\\theta}(o_i | q)}{\\pi_{\\theta_{\\text{old}}}(o_i | q)} A_i, \\, \\text{clip} \\Bigg( \\frac{\\pi_{\\theta}(o_i | q)}{\\pi_{\\theta_{\\text{old}}}(o_i | q)}, 1 - \\epsilon, 1 + \\epsilon \\Bigg) A_i \\Bigg) - \\beta \\, \\mathcal{D}_{\\text{KL}}(\\pi_{\\theta} \\| \\pi_{\\text{ref}}) \\Bigg]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{D}_{\\text{KL}}(\\pi_{\\theta} \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o_i \\mid q)}{\\pi_{\\theta}(o_i \\mid q)} - \\log \\frac{\\pi_{\\text{ref}}(o_i \\mid q)}{\\pi_{\\theta}(o_i \\mid q)} - 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\dots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\dots, r_G\\})}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $G$ is the number of responses sampled per query.\n",
    "- $\\pi_{\\theta}(o_i \\mid q)$ is the probability of response $o_i$ given query $q$ under the new policy.\n",
    "- $\\pi_{\\theta_{\\text{old}}}(o_i \\mid q)$ is the probability of response $o_i$ given query $q$ under the old policy.\n",
    "- $A_i$ is the advantage of response $o_i$.\n",
    "- $\\epsilon$ is the clipping parameter.\n",
    "- $\\beta$ is the KL divergence regularization weight.\n",
    "\n",
    "## **Step 1: Sampling Queries $q \\sim P(Q)$**\n",
    "We randomly sample queries $q$ from the distribution of possible prompts $P(Q)$. This distribution represents **real-world tasks** or **user inputs** that the model will be trained to handle.\n",
    "\n",
    "**Examples of prompts:**  \n",
    "- \"The best food in the world is\"  \n",
    "- \"Corgis are\"  \n",
    "\n",
    "This step forms the basis of RLHF tasks where the model needs to generate responses aligned with user preferences or reward feedback.\n",
    "\n",
    "## **Step 2: Sampling $G$ Full Responses Per Query**\n",
    "For each query $q$, we sample **G possible responses (sequences of tokens)** from the **old policy** $\\pi_\\theta^{\\text{old}}$. Instead of just predicting a single token, the model generates **entire responses of length $T$** by sampling the next token sequentially until the response is complete.\n",
    "\n",
    "**Example:** If the query is \"The capital of France is,\" the G possible responses could be:  \n",
    "- $o_1$ = \"Paris is a beautiful city with historic landmarks.\"  \n",
    "- $o_2$ = \"Paris, home to the Eiffel Tower and the Louvre.\"  \n",
    "- $o_3$ = \"Paris, the cultural and political hub of France.\"  \n",
    "\n",
    "This process ensures that the model explores **different potential completions** for each prompt, enabling the policy to generalize across a variety of outputs.\n",
    "\n",
    "## **Step 3: Why Are We Sampling $G$ Full Responses?**\n",
    "Sampling **multiple full responses per query** helps reduce variance and ensures that the policy update is stable and generalizable. Instead of relying on a single sampled output (which may be noisy or suboptimal), the model evaluates multiple possible completions, making it robust across different scenarios.\n",
    "\n",
    "**Key benefits of sampling $G$ responses:**\n",
    "- **Variance reduction:** By averaging the PPO objective across $G$ outputs, the model avoids overfitting to any particular sampled response.\n",
    "- **Stable policy updates:** Evaluating multiple responses per query ensures that the updates are consistent across different completions.\n",
    "- **Improved exploration:** The model can explore different possible responses and select the most reward-aligned behavior.\n",
    "\n",
    "## **Step 4: Computing the PPO Objective for Full Responses**\n",
    "The main PPO objective evaluates the **probability ratios** between the new policy $\\pi_\\theta$ and the old policy $\\pi_\\theta^{\\text{old}}$, weighted by the **advantage** of each response.\n",
    "\n",
    "The **probability of a full response** $o_i = [o_1, o_2, ..., o_T]$ under a policy is calculated as:\n",
    "\n",
    "$$\n",
    "\\pi_\\theta(o \\mid q) = \\prod_{t=1}^{T} \\pi_\\theta(o_t \\mid o_{<t}, q)\n",
    "$$\n",
    "\n",
    "In practice, we compute the **log-probability of the entire sequence** to avoid numerical underflow:\n",
    "\n",
    "$$\n",
    "\\log \\pi_\\theta(o \\mid q) = \\sum_{t=1}^{T} \\log \\pi_\\theta(o_t \\mid o_{<t}, q)\n",
    "$$\n",
    "\n",
    "The PPO objective compares the **log-probabilities of the new policy and old policy**:\n",
    "\n",
    "$$\n",
    "\\text{Ratio} = \\exp\\left( \\log \\pi_\\theta(o \\mid q) - \\log \\pi_\\theta^{\\text{old}}(o \\mid q) \\right)\n",
    "$$\n",
    "\n",
    "The **advantage function** $A_i$ quantifies how much better a sampled response $o_i$ is compared to the expected baseline:\n",
    "\n",
    "$$\n",
    "A_i = \\frac{r_i - \\text{mean}(r_1, ..., r_G)}{\\text{std}(r_1, ..., r_G)}\n",
    "$$\n",
    "\n",
    "We then apply **clipping** to the probability ratio to ensure that updates do not deviate too much from the old policy:\n",
    "\n",
    "$$\n",
    "\\text{Clipped objective} = \\min \\left( \\text{Ratio} \\times A_i, \\text{clip}(\\text{Ratio}, 1 - \\epsilon, 1 + \\epsilon) \\times A_i \\right)\n",
    "$$\n",
    "\n",
    "## **Step 5: Averaging the PPO Loss Across Queries and Responses**\n",
    "We compute the expectation $\\mathbb{E}$ by averaging the PPO objective over both the sampled queries and the $G$ sampled responses for each query:\n",
    "\n",
    "$$\n",
    "J_{\\text{GRPO}}(\\theta) = \\mathbb{E}_{q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_\\theta^{\\text{old}}} \\left[ \\frac{1}{G} \\sum_{i=1}^G \\text{Clipped objective} \\right]\n",
    "$$\n",
    "\n",
    "This ensures that the policy update is guided by the **average performance across multiple responses** rather than being sensitive to individual samples.\n",
    "\n",
    "## **Step 6: KL Regularization to Control Response Length and Safety**\n",
    "The KL divergence penalty $D_{KL}(\\pi_\\theta \\parallel \\pi_{\\text{ref}})$ prevents the new policy from deviating too far from a **reference policy** $\\pi_{\\text{ref}}$, which could be the original pre-trained model or a simpler model like DistilGPT-2.\n",
    "\n",
    "$$\n",
    "D_{KL}(\\pi_\\theta \\parallel \\pi_{\\text{ref}}) = \\sum_{o_i} \\pi_{\\text{ref}}(o_i \\mid q) \\log \\frac{\\pi_{\\text{ref}}(o_i \\mid q)}{\\pi_\\theta(o_i \\mid q)}\n",
    "$$\n",
    "\n",
    "### **Why is KL regularization important?**\n",
    "- **Maintains fluency and coherence:** Penalizing large deviations ensures that the model retains its general language generation ability while optimizing for rewards.\n",
    "- **Controls response length:** A larger KL weight $\\beta$ penalizes long or risky completions, encouraging the model to generate shorter, safer responses.\n",
    "- **Balances exploration and exploitation:** By tuning $\\beta$, we can allow the model to explore creative outputs while staying anchored to its pre-trained knowledge.\n",
    "\n",
    "## **Step 7: When to Use a Reference Policy**\n",
    "The reference policy is essential in cases where:\n",
    "- **We are fine-tuning a pre-trained model:** To ensure that the model doesn’t lose its general knowledge.\n",
    "- **In RLHF settings:** To balance maximizing rewards with preserving fluency and coherence.\n",
    "- **To control output length and verbosity:** Increasing $\\beta$ encourages shorter, safer responses, while decreasing it allows for more diverse and creative outputs.\n",
    "\n",
    "\n",
    "## **Final Summary:**  \n",
    "1. **Sample queries** from the distribution P(Q) \n",
    "2. **Generate G full responses** per query using the old policy.  \n",
    "3. **Compute the PPO objective** by comparing the new policy to the old policy, weighted by advantages.  \n",
    "4. **Apply KL regularization** using the reference policy to balance exploration and safety.  \n",
    "5. **Update the model parameters** by averaging the PPO loss across queries and responses.\n",
    "\n",
    "This process ensures that the model improves consistently, remains stable during training, and balances reward maximization with coherence and safety. 😊\n",
    "\n",
    "# References:\n",
    "[DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948)\n",
    "\n",
    "[DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/pdf/2402.03300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TopKLogitsWarper\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequences(\n",
    "    model, tokenizer, inputs, max_length=50, top_k=50, n_sample_sequences=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Sample n_sample_sequences per prompt using top-k sampling.\n",
    "\n",
    "    Args:\n",
    "        model: The model used for generation.\n",
    "        tokenizer: The tokenizer for the model.\n",
    "        inputs: A dict with keys \"input_ids\" and \"attention_mask\" (as returned by the tokenizer).\n",
    "        max_length: Maximum number of new tokens to generate.\n",
    "        top_k: The top-k value used in sampling.\n",
    "        n_sample_sequences: Number of sequences to sample per prompt.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (batch_size, n_sample_sequences, total_sequence_length)\n",
    "        where total_sequence_length = prompt_length + generated tokens.\n",
    "    \"\"\"\n",
    "    G = n_sample_sequences\n",
    "    input_ids = inputs[\"input_ids\"]  # shape: (batch_size, prompt_length)\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    batch_size = input_ids.size(0)\n",
    "\n",
    "    # Initialize top-k warper\n",
    "    top_k_warper = TopKLogitsWarper(top_k=top_k)\n",
    "\n",
    "    # Expand inputs so that each prompt is repeated G times.\n",
    "    # New shape: (batch_size * G, prompt_length)\n",
    "    input_ids = (\n",
    "        input_ids.unsqueeze(1).expand(batch_size, G, -1).reshape(batch_size * G, -1)\n",
    "    )\n",
    "    attention_mask = (\n",
    "        attention_mask.unsqueeze(1)\n",
    "        .expand(batch_size, G, -1)\n",
    "        .reshape(batch_size * G, -1)\n",
    "    )\n",
    "\n",
    "    # We'll build generated sequences starting from the prompt.\n",
    "    generated_sequences = input_ids.clone()  # shape: (batch_size * G, current_length)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=generated_sequences, attention_mask=attention_mask\n",
    "            )\n",
    "            # Get logits for the last token in each sequence\n",
    "            logits = outputs.logits[:, -1, :]  # shape: (batch_size*G, vocab_size)\n",
    "\n",
    "            # Apply top-k filtering\n",
    "            filtered_logits = top_k_warper(None, logits)\n",
    "            probs = F.softmax(filtered_logits, dim=-1)\n",
    "\n",
    "            # Sample next token\n",
    "            next_tokens = torch.multinomial(\n",
    "                probs, num_samples=1\n",
    "            )  # shape: (batch_size*G, 1)\n",
    "\n",
    "            # If all sequences have generated an EOS token, stop early.\n",
    "            if (next_tokens == tokenizer.eos_token_id).all():\n",
    "                break\n",
    "\n",
    "            # Append sampled token to sequences.\n",
    "            generated_sequences = torch.cat([generated_sequences, next_tokens], dim=1)\n",
    "\n",
    "            # Extend the attention mask accordingly.\n",
    "            new_mask = torch.ones(\n",
    "                (attention_mask.size(0), 1),\n",
    "                device=attention_mask.device,\n",
    "                dtype=attention_mask.dtype,\n",
    "            )\n",
    "            attention_mask = torch.cat([attention_mask, new_mask], dim=1)\n",
    "\n",
    "    # Reshape back to (batch_size, G, total_sequence_length)\n",
    "    generated_sequences = generated_sequences.view(batch_size, G, -1)\n",
    "    return generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sequence_log_probs(model, sequences, prompt_length, pad_token_id):\n",
    "    \"\"\"\n",
    "    Compute the log–probability of the generated part (i.e. tokens after the prompt)\n",
    "    for each sequence.\n",
    "\n",
    "    Args:\n",
    "        model: The language model.\n",
    "        sequences: Tensor of shape (batch_size, G, total_sequence_length) where\n",
    "                   the first prompt_length tokens are the prompt.\n",
    "        prompt_length: Length of the prompt (number of tokens).\n",
    "        pad_token_id: Token ID used for padding.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (batch_size, G) containing the summed log–probability\n",
    "        of the generated tokens (i.e. excluding the prompt).\n",
    "    \"\"\"\n",
    "    batch_size, G, total_seq_len = sequences.size()\n",
    "    # Flatten the first two dimensions.\n",
    "    flat_sequences = sequences.view(batch_size * G, total_seq_len)\n",
    "    # Create an attention mask (assumes pad_token_id marks padded tokens)\n",
    "    attention_mask = (flat_sequences != pad_token_id).long()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=flat_sequences, attention_mask=attention_mask).logits\n",
    "    # Shift logits and labels so that the probability for token t is given by logits[t-1]\n",
    "    shift_logits = logits[:, :-1, :]  # shape: (B, total_seq_len-1, vocab_size)\n",
    "    shift_labels = flat_sequences[:, 1:]  # shape: (B, total_seq_len-1)\n",
    "\n",
    "    # We want the log–probabilities for generated tokens only.\n",
    "    # For a prompt of length L, the generated tokens are from index L to end in flat_sequences.\n",
    "    # Their probabilities are predicted at positions L-1 to (total_seq_len-1)-1.\n",
    "    L = prompt_length\n",
    "    N = total_seq_len - L  # number of generated tokens\n",
    "    if N <= 0:\n",
    "        raise ValueError(\"No generated tokens to compute log–probs for.\")\n",
    "\n",
    "    # Slice out only the generated portion.\n",
    "    # The first generated token is predicted at position L-1 in shift_logits, corresponding to label at index L.\n",
    "    gen_logits = shift_logits[:, L - 1 : L - 1 + N, :]  # shape: (B, N, vocab_size)\n",
    "    gen_labels = shift_labels[:, L - 1 : L - 1 + N]  # shape: (B, N)\n",
    "\n",
    "    # Compute log probabilities for each token.\n",
    "    log_probs_all = F.log_softmax(gen_logits, dim=-1)  # shape: (B, N, vocab_size)\n",
    "    token_log_probs = log_probs_all.gather(2, gen_labels.unsqueeze(-1)).squeeze(\n",
    "        -1\n",
    "    )  # shape: (B, N)\n",
    "    # Sum log–probs over the generated tokens.\n",
    "    seq_log_probs = token_log_probs.sum(dim=1)  # shape: (B,)\n",
    "\n",
    "    return seq_log_probs.view(batch_size, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(sequences: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Generate rewards for the sampled sequences.\n",
    "\n",
    "    At the moment this will be random rewards between 0 and 1 in the shape of the input tensor.\n",
    "\n",
    "    Args:\n",
    "        response_text: The tensor of sampled sequences.\n",
    "\n",
    "    Returns:\n",
    "        A tensor of rewards in the shape of the input tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size, G, _ = sequences.shape\n",
    "    rewards = torch.rand(batch_size, G, device=sequences.device)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantages(rewards):\n",
    "    \"\"\"\n",
    "    Normalize rewards per query across the group.\n",
    "\n",
    "    Args:\n",
    "        rewards: Tensor of shape (batch_size, G)\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (batch_size, G) of normalized advantages.\n",
    "    \"\"\"\n",
    "    mean_reward = rewards.mean(dim=1, keepdim=True)\n",
    "    std_reward = rewards.std(dim=1, unbiased=False, keepdim=True)\n",
    "    advantages = (rewards - mean_reward) / (std_reward + 1e-8)\n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_loss(\n",
    "    new_log_probs,\n",
    "    old_log_probs,\n",
    "    advantages,\n",
    "    epsilon=0.2,\n",
    "    beta=0.01,\n",
    "    reference_log_probs=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the PPO-style loss with a KL penalty term.\n",
    "\n",
    "    Args:\n",
    "        new_log_probs: Tensor (batch_size, G) – log–probs under the new policy.\n",
    "        old_log_probs: Tensor (batch_size, G) – log–probs under the old policy.\n",
    "        advantages: Tensor (batch_size, G) – normalized advantages.\n",
    "        epsilon: Clipping parameter.\n",
    "        beta: KL penalty coefficient.\n",
    "        reference_log_probs: Tensor (batch_size, G) – log–probs under a reference policy.\n",
    "\n",
    "    Returns:\n",
    "        A scalar loss.\n",
    "    \"\"\"\n",
    "    # Compute probability ratios: exp(new - old)\n",
    "    ratios = torch.exp(new_log_probs - old_log_probs)\n",
    "    clipped_ratios = torch.clamp(ratios, 1 - epsilon, 1 + epsilon)\n",
    "    objective = torch.min(ratios * advantages, clipped_ratios * advantages)\n",
    "\n",
    "    # KL penalty using the formula: (pi_ref/pi_new) - log(pi_ref/pi_new) - 1.\n",
    "    if reference_log_probs is not None:\n",
    "        ratio_ref = torch.exp(reference_log_probs - new_log_probs)\n",
    "        kl_div = ratio_ref - torch.log(ratio_ref) - 1\n",
    "        kl_div = kl_div.mean()\n",
    "    else:\n",
    "        kl_div = 0.0\n",
    "\n",
    "    loss = -objective.mean() + beta * kl_div\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking individual components of GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and add PAD token\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Initialize Models\n",
    "new_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gpt2\"\n",
    ")  # New policy model (to be optimized)\n",
    "old_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gpt2\"\n",
    ")  # Old policy model (before update)\n",
    "reference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"distilgpt2\"\n",
    ")  # Optional reference policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample batch of prompts (queries)\n",
    "prompts = [\n",
    "    \"The best food in the world is\",\n",
    "    \"Corgis are\",\n",
    "    \"The meaning of life is\",\n",
    "    \"I am\",\n",
    "    \"The most important thing in the world is\",\n",
    "]\n",
    "\n",
    "# Tokenize prompts and get logits\n",
    "inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample G sequences per query using each policy\n",
    "new_policy_sequences = sample_sequences(\n",
    "    new_model,\n",
    "    tokenizer,\n",
    "    inputs,\n",
    "    max_length=10,\n",
    "    top_k=50,\n",
    "    n_sample_sequences=5,\n",
    ")\n",
    "old_policy_sequences = sample_sequences(\n",
    "    old_model,\n",
    "    tokenizer,\n",
    "    inputs,\n",
    "    max_length=10,\n",
    "    top_k=50,\n",
    "    n_sample_sequences=5,\n",
    ")\n",
    "reference_policy_sequences = sample_sequences(\n",
    "    reference_model,\n",
    "    tokenizer,\n",
    "    inputs,\n",
    "    max_length=10,\n",
    "    top_k=50,\n",
    "    n_sample_sequences=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New policy log-probs: tensor([[-37.0535, -24.5370, -20.2848, -33.6020, -30.9356],\n",
      "        [-31.7608, -34.2596, -35.6612, -26.7157, -33.7347],\n",
      "        [-33.5248, -34.9668, -43.7043, -38.8953, -29.9209],\n",
      "        [-31.5192, -31.2574, -20.3213, -43.9429, -27.3535],\n",
      "        [-25.6883, -26.4976, -25.9713, -33.7142, -29.4448]])\n",
      "Old policy log-probs: tensor([[-39.1005, -29.6932, -30.6909, -29.8440, -30.2227],\n",
      "        [-31.7843, -37.6550, -30.7647, -36.2704, -33.3683],\n",
      "        [-28.6793, -10.1107, -28.9684, -37.1744, -25.7923],\n",
      "        [-28.0612, -26.5376, -37.6537, -29.1060, -35.9013],\n",
      "        [-22.7740, -22.2103, -26.1856, -26.3483, -31.9297]])\n",
      "Reference policy log-probs: tensor([[-33.8827, -20.7227, -27.5098, -32.8465, -36.5514],\n",
      "        [-13.8102, -27.4110, -38.8218, -33.9706, -17.7042],\n",
      "        [-32.3322, -29.1900, -35.0639, -38.1682,  -5.4566],\n",
      "        [-44.2237, -29.9062, -43.3479, -33.0118, -28.1428],\n",
      "        [-32.6752, -31.0077, -28.6882, -24.5660, -33.0195]])\n"
     ]
    }
   ],
   "source": [
    "# The prompt length (assumed the same for all examples)\n",
    "prompt_length = inputs[\"input_ids\"].size(1)\n",
    "pad_token_id = (\n",
    "    tokenizer.pad_token_id\n",
    "    if tokenizer.pad_token_id is not None\n",
    "    else tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Compute log–probabilities for the generated (i.e. non–prompt) part.\n",
    "new_log_probs = compute_sequence_log_probs(\n",
    "    new_model, new_policy_sequences, prompt_length, pad_token_id\n",
    ")\n",
    "old_log_probs = compute_sequence_log_probs(\n",
    "    old_model, old_policy_sequences, prompt_length, pad_token_id\n",
    ")\n",
    "if reference_policy_sequences is not None:\n",
    "    reference_log_probs = compute_sequence_log_probs(\n",
    "        reference_model, reference_policy_sequences, prompt_length, pad_token_id\n",
    "    )\n",
    "else:\n",
    "    reference_log_probs = None\n",
    "\n",
    "\n",
    "print(\"New policy log-probs:\", new_log_probs)\n",
    "print(\"Old policy log-probs:\", old_log_probs)\n",
    "print(\"Reference policy log-probs:\", reference_log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards: tensor([[0.6701, 0.8050, 0.0647, 0.9789, 0.1421],\n",
      "        [0.8898, 0.0237, 0.8894, 0.1515, 0.1535],\n",
      "        [0.0225, 0.6304, 0.3054, 0.0383, 0.7066],\n",
      "        [0.8884, 0.7723, 0.4005, 0.9084, 0.9065],\n",
      "        [0.9083, 0.8879, 0.1165, 0.0722, 0.8682]])\n"
     ]
    }
   ],
   "source": [
    "# Mock rewards for demonstration (one reward per sampled token)\n",
    "rewards = compute_reward(new_policy_sequences)\n",
    "print(\"Rewards:\", rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute normalized advantages\n",
    "def compute_advantages(rewards: torch.Tensor) -> torch.Tensor:\n",
    "    mean_reward = rewards.mean(dim=1, keepdim=True)\n",
    "    std_reward = rewards.std(dim=1, unbiased=False, keepdim=True)\n",
    "    advantages = (rewards - mean_reward) / (std_reward + 1e-8)\n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advantages: tensor([[ 0.3786,  0.7489, -1.2829,  1.2261, -1.0707],\n",
      "        [ 1.2160, -1.0333,  1.2151, -0.7014, -0.6964],\n",
      "        [-1.1086,  1.0098, -0.1228, -1.0536,  1.2752],\n",
      "        [ 0.5832, -0.0150, -1.9314,  0.6866,  0.6766],\n",
      "        [ 0.8673,  0.8149, -1.1663, -1.2803,  0.7644]])\n"
     ]
    }
   ],
   "source": [
    "advantages = compute_advantages(rewards)\n",
    "print(\"Advantages:\", advantages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO Loss: 19490114.0\n"
     ]
    }
   ],
   "source": [
    "# Compute PPO loss across G sampled outputs\n",
    "ppo_loss_value = ppo_loss(\n",
    "    new_log_probs=new_log_probs,\n",
    "    old_log_probs=old_log_probs.detach(),  # Old policy is detached to prevent gradient flow\n",
    "    advantages=advantages,\n",
    "    epsilon=0.2,\n",
    "    beta=0.01,\n",
    "    reference_log_probs=reference_log_probs,\n",
    ")\n",
    "\n",
    "print(\"PPO Loss:\", ppo_loss_value.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grpo_train_step(\n",
    "    new_model,\n",
    "    old_model,\n",
    "    reference_model,\n",
    "    tokenizer,\n",
    "    inputs,\n",
    "    max_length=10,\n",
    "    top_k=50,\n",
    "    n_sample_sequences=5,\n",
    "    epsilon=0.2,\n",
    "    beta=0.01,\n",
    "    optimizer=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform one GRPO update step.\n",
    "\n",
    "    Args:\n",
    "        new_model: The current (new) policy to optimize.\n",
    "        old_model: A frozen copy of the policy before the update.\n",
    "        reference_model: A reference policy (e.g. a pretrained model) for KL penalty.\n",
    "        tokenizer: The tokenizer corresponding to the models.\n",
    "        inputs: Tokenized prompts (a dict with \"input_ids\" and \"attention_mask\").\n",
    "        max_length: Number of new tokens to generate.\n",
    "        top_k: Top-k parameter for sampling.\n",
    "        n_sample_sequences: Number of responses (G) to sample per prompt.\n",
    "        epsilon: PPO clipping parameter.\n",
    "        beta: KL divergence regularization weight.\n",
    "        optimizer: The optimizer used for updating new_model.\n",
    "\n",
    "    Returns:\n",
    "        loss: The computed loss (a scalar tensor).\n",
    "    \"\"\"\n",
    "    # Sample responses using each policy.\n",
    "    new_policy_sequences = sample_sequences(\n",
    "        new_model,\n",
    "        tokenizer,\n",
    "        inputs,\n",
    "        max_length,\n",
    "        top_k,\n",
    "        n_sample_sequences,\n",
    "    )\n",
    "    old_policy_sequences = sample_sequences(\n",
    "        old_model,\n",
    "        tokenizer,\n",
    "        inputs,\n",
    "        max_length,\n",
    "        top_k,\n",
    "        n_sample_sequences,\n",
    "    )\n",
    "    if reference_model is not None:\n",
    "        reference_policy_sequences = sample_sequences(\n",
    "            reference_model,\n",
    "            tokenizer,\n",
    "            inputs,\n",
    "            max_length,\n",
    "            top_k,\n",
    "            n_sample_sequences,\n",
    "        )\n",
    "    else:\n",
    "        reference_policy_sequences = None\n",
    "\n",
    "    # The prompt length (assumed the same for all examples)\n",
    "    prompt_length = inputs[\"input_ids\"].size(1)\n",
    "    pad_token_id = (\n",
    "        tokenizer.pad_token_id\n",
    "        if tokenizer.pad_token_id is not None\n",
    "        else tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Compute log–probabilities for the generated (i.e. non–prompt) part.\n",
    "    new_log_probs = compute_sequence_log_probs(\n",
    "        new_model, new_policy_sequences, prompt_length, pad_token_id\n",
    "    )\n",
    "    old_log_probs = compute_sequence_log_probs(\n",
    "        old_model, old_policy_sequences, prompt_length, pad_token_id\n",
    "    )\n",
    "    if reference_policy_sequences is not None:\n",
    "        reference_log_probs = compute_sequence_log_probs(\n",
    "            reference_model, reference_policy_sequences, prompt_length, pad_token_id\n",
    "        )\n",
    "    else:\n",
    "        reference_log_probs = None\n",
    "\n",
    "    # Compute rewards and advantages.\n",
    "    rewards = compute_reward(new_policy_sequences)  # shape: (batch_size, G)\n",
    "    advantages = compute_advantages(rewards)  # shape: (batch_size, G)\n",
    "\n",
    "    # Compute the GRPO (PPO-style) loss.\n",
    "    loss = ppo_loss(\n",
    "        new_log_probs,\n",
    "        old_log_probs.detach(),\n",
    "        advantages,\n",
    "        epsilon,\n",
    "        beta,\n",
    "        reference_log_probs,\n",
    "    )\n",
    "\n",
    "    if optimizer is not None:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRPO Loss: 2426870.5\n",
      "The numbers can vary due to the random nature of the sampling and rewards.\n"
     ]
    }
   ],
   "source": [
    "# Run one GRPO training step.\n",
    "loss = grpo_train_step(\n",
    "    new_model,\n",
    "    old_model,\n",
    "    reference_model,\n",
    "    tokenizer,\n",
    "    inputs,\n",
    "    max_length=10,\n",
    "    top_k=50,\n",
    "    n_sample_sequences=5,\n",
    "    epsilon=0.2,\n",
    "    beta=0.01,\n",
    ")\n",
    "\n",
    "print(\"GRPO Loss:\", loss.item())\n",
    "print(\"The numbers can vary due to the random nature of the sampling and rewards.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
