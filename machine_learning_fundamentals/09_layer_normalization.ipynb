{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "\n",
    "Purpose:\n",
    "\n",
    "* BatchNorm is primarily used to address the internal covariate shift problem in deep neural networks by normalizing the activations of each layer across mini-batches during training.\n",
    "Normalization Strategy:\n",
    "\n",
    "* BatchNorm normalizes the activations across the mini-batch dimension. It calculates the mean and standard deviation of each feature across the mini-batch and normalizes the features using these statistics.\n",
    "\n",
    "Learnable Parameters:\n",
    "\n",
    "* BatchNorm introduces learnable parameters (scale and shift) to the normalized data, allowing the model to adapt and learn the optimal normalization for each layer.\n",
    "\n",
    "Applicability:\n",
    "\n",
    "* BatchNorm is commonly used in deep neural networks, especially in convolutional neural networks (CNNs) and recurrent neural networks (RNNs).\n",
    "\n",
    "Performance:\n",
    "\n",
    "* BatchNorm can suffer from issues such as batch size sensitivity during inference and training instability in small batch sizes.\n",
    "\n",
    "### References:\n",
    "\n",
    "[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm1d(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
    "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "        self.register_buffer('running_var', torch.ones(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            mean = x.mean(dim=0, keepdim=True)\n",
    "            var = x.var(dim=0, unbiased=False, keepdim=True)\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.squeeze()\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var.squeeze()\n",
    "        else:\n",
    "            mean = self.running_mean.unsqueeze(0)\n",
    "            var = self.running_var.unsqueeze(0)\n",
    "\n",
    "        x_normalized = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        output = self.gamma * x_normalized + self.beta\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Normalization:\n",
    "\n",
    "Purpose:\n",
    "\n",
    "* LayerNorm normalizes the activations of each layer across feature dimensions independently for each sample in a mini-batch. It aims to stabilize the training process and reduce the impact of the scale of input features.\n",
    "\n",
    "Normalization Strategy:\n",
    "\n",
    "* LayerNorm normalizes the activations across the feature dimension independently for each sample in the mini-batch. It calculates the mean and standard deviation of each feature across the feature dimension and normalizes the features using these statistics.\n",
    "\n",
    "Learnable Parameters:\n",
    "\n",
    "* LayerNorm does not introduce any learnable parameters. It normalizes the activations based solely on the mean and standard deviation computed across the feature dimension.\n",
    "\n",
    "Applicability:\n",
    "\n",
    "* LayerNorm is often used in natural language processing (NLP) tasks, such as sequence modeling and language understanding, where the feature dimensions can vary across samples.\n",
    "\n",
    "Performance:\n",
    "\n",
    "* LayerNorm is less sensitive to batch size and performs well even in small batch sizes. It can also be applied in scenarios where the sequence length varies across samples.\n",
    "\n",
    "### References:\n",
    "[LAYERNORM](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm)\n",
    "\n",
    "[Layer Normalization](https://arxiv.org/abs/1607.06450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(normalized_shape))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Root Mean Square Normalization:\n",
    "Purpose:\n",
    "\n",
    "- RMSNorm normalizes the inputs by dividing them by the root mean square (RMS) value along a specific axis, typically along the feature dimension. It aims to stabilize and accelerate training by normalizing the magnitude of the inputs.\n",
    "\n",
    "Normalization Strategy:\n",
    "\n",
    "- RMSNorm normalizes the inputs by dividing them by the root mean square (RMS) value along a specific axis, typically along the feature dimension. It calculates the RMS value of each feature along the specified axis and normalizes the features using these RMS values.\n",
    "\n",
    "Learnable Parameters:\n",
    "\n",
    "- RMSNorm introduces a single learnable parameter (gamma) to scale the normalized data, but it does not introduce any parameter to shift the data.\n",
    "\n",
    "Applicability:\n",
    "\n",
    "- RMSNorm is less common compared to BatchNorm and LayerNorm but can be useful in scenarios where stabilizing the magnitude of inputs is beneficial, such as in reinforcement learning or certain types of recurrent neural networks.\n",
    "\n",
    "Performance:\n",
    "\n",
    "- RMSNorm is relatively simple and computationally efficient compared to BatchNorm and LayerNorm. It can provide stable performance across different batch sizes and input dimensions.\n",
    "\n",
    "### References:\n",
    "\n",
    "[Root Mean Square Layer Normalization](https://arxiv.org/abs/1910.07467)\n",
    "\n",
    "[]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        rms = x.pow(2).mean(-1, keepdim=True).add_(self.eps).sqrt_()\n",
    "        return self.weight * (x / rms)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
