{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WTF Does nn.Embedding Do?\n",
    "\n",
    "### Concept:\n",
    "nn.Embedding is a PyTorch layer that transforms categorical data (like words or characters) in a torch.long() format into dense vector representations of type torch.float(). It's essentially a lookup table mapping integer indices (representing discrete categories) to continuous vectors of fixed size.\n",
    "\n",
    "### How It Works:\n",
    "Lookup Table: The embedding layer maintains a weight matrix of size (num_embeddings, embedding_dim).\n",
    "Indexing: When you pass an index through this layer, it retrieves the corresponding row from the matrix, which is the embedding vector for that index.\n",
    "Training: These vectors are initialized randomly and are updated during training via backpropagation, just like other parameters in the model.\n",
    "\n",
    "If you don't specify any initialization methods, the embeddings are normally distributed with mean 0 and standard deviation 1.\n",
    "\n",
    "### Differentiability:\n",
    "Although it seems like a simple lookup table, nn.Embedding is differentiable and trainable.\n",
    "The embedding layer can be thought of as multiplying a one-hot encoded vector by the weight matrix, effectively selecting a specific row.\n",
    "During training, gradients are computed for the embeddings, allowing them to be refined based on the model’s loss function.\n",
    "\n",
    "### How are these embeddings different from Sentence-BERT embeddings?:\n",
    "While nn.Embedding offers static embeddings, meaning each word is assigned a fixed vector, models like Sentence-BERT introduce dynamic, context-dependent embeddings. In these models, the representation of a token can vary based on the tokens surrounding it, enabling a more nuanced understanding of language.\n",
    "\n",
    "### Contextual Understanding:\n",
    "Dynamic embeddings consider the context in which a token appears, allowing its representation to adapt and capture variations in meaning. This contextual understanding enables the model to handle polysemy—words with multiple meanings—more effectively, as the embedding of a token can change depending on its neighboring tokens.\n",
    "\n",
    "# References:\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 100 possible characters\n",
    "chars = list(string.printable)\n",
    "n_chars = len(chars)\n",
    "tokenizer = dict(zip(chars, range(len(chars))))\n",
    "detokenizer = dict(zip(range(len(chars)), chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape: torch.Size([1, 25])\n"
     ]
    }
   ],
   "source": [
    "input_string = \"WTF does nn.Embedding do?\"\n",
    "# Shape: (batch_size, seq_len)\n",
    "input_tensor = torch.tensor([[tokenizer[c] for c in input_string]], dtype=torch.long)\n",
    "print(f\"Input Tensor Shape: {input_tensor.shape}\")\n",
    "\n",
    "# Dimension of the embedding space\n",
    "seq_len = input_tensor.shape[1]\n",
    "embed_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Tensor Shape: torch.Size([1, 25, 10])\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(n_chars, embed_dim)\n",
    "output_embedding = embedding(input_tensor)\n",
    "# Shape: (batch_size, seq_len, embed_dim)\n",
    "print(f\"Output Tensor Shape: {output_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'a': tensor([[[-0.1633,  0.2602, -1.3810, -0.9100, -1.3273,  0.0258,  1.2236,\n",
      "           0.0988,  1.5362, -0.0035]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a_tokenized = torch.tensor([[tokenizer['a']]], dtype=torch.long)\n",
    "a_embedded = embedding(a_tokenized)\n",
    "print(f\"Embedding for 'a': {a_embedded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.Embedding vs nn.Linear\n",
    "\n",
    "Title: Exploring the Differences Between nn.Linear and nn.Embedding in Neural Networks\n",
    "\n",
    "Hey, Reddit fam!\n",
    "\n",
    "I've been diving deep into neural network architectures lately, and I stumbled upon an interesting question: What sets nn.Linear and nn.Embedding apart, and can one replace the other? Let's unpack this together.\n",
    "\n",
    "Understanding the Basics\n",
    "nn.Linear: This powerhouse layer is all about linear transformations, focusing on individual features. It takes continuous data and reshapes it through matrix multiplication and addition, allowing it to capture intricate patterns within the data.\n",
    "\n",
    "nn.Embedding: On the other hand, nn.Embedding is the go-to for handling categorical data, such as words or characters in natural language processing tasks. It transforms discrete categories into dense vectors, known as embeddings, by mapping integer indices to continuous vectors of fixed size.\n",
    "\n",
    "The Divergence Point: Features vs. Sequences\n",
    "Here's where it gets interesting: Although both methods can create continuous vectors, their focus differs. While nn.Linear zeroes in on individual features, nn.Embedding hones in on the sequence as a whole. In essence, nn.Linear pays attention to the nitty-gritty details of each feature, whereas nn.Embedding considers the context and order of the sequence.\n",
    "\n",
    "Can nn.Linear Replace nn.Embedding?\n",
    "Now, for the million-dollar question: Can nn.Linear replace nn.Embedding? The short answer: not quite. While nn.Linear excels at capturing feature-level intricacies, it lacks the contextual understanding that nn.Embedding brings to the table. In tasks where sequence order and relationships matter—think natural language processing—nn.Embedding reigns supreme.\n",
    "\n",
    "Conclusion: Embracing Diversity in Neural Networks\n",
    "In the dynamic world of neural networks, diversity is key. Both nn.Linear and nn.Embedding play vital roles in shaping the architecture of our models, each bringing its unique strengths to the table. So, instead of pitting them against each other, let's celebrate their diversity and leverage them wisely to unlock the full potential of our neural networks.\n",
    "\n",
    "What are your thoughts on nn.Linear vs. nn.Embedding? Have you encountered any interesting use cases for either? Let's keep the conversation going in the comments below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one row: One sample in the training set\n",
    "x = torch.tensor([\n",
    "    [1, 0, 1, 0],\n",
    "    [0, 0, 1, 1],\n",
    "    [1, 1, 1, 0],\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "rows = torch.tensor([[0, 1, 2]], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0976, -0.2872, -0.1632,  0.0354],\n",
       "        [ 0.2979,  0.3021,  0.4652,  0.1204],\n",
       "        [-0.3786,  0.2526, -0.3794, -0.0699]], requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_linear = nn.Linear(4, 3, bias = False)\n",
    "w_linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2608,  0.7631, -0.7580], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "print(w_linear(x[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0976,  0.2979, -0.3786],\n",
       "        [-0.2872,  0.3021,  0.2526],\n",
       "        [-0.1632,  0.4652, -0.3794],\n",
       "        [ 0.0354,  0.1204, -0.0699]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_embedding = nn.Embedding(4, 3).from_pretrained(w_linear.weight.t())\n",
    "w_embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2608,  0.7631, -0.7580])\n"
     ]
    }
   ],
   "source": [
    "# Indices from the first row (non-zero entries)\n",
    "row_indices = torch.tensor([0, 2], dtype=torch.long)\n",
    "\n",
    "# Pass indices through embedding layer and sum the embeddings\n",
    "embedding_output = w_embedding(row_indices).sum(dim=0)\n",
    "print(embedding_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
