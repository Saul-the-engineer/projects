{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) Basics\n",
    "\n",
    "In this notebook, we will cover the basics of Retrieval Augmented Generation (RAG) model. RAG is a model that combines the best of both worlds - retrieval and generation. It uses a retriever to retrieve relevant passages from a large corpus and then uses a generator to generate the answer.\n",
    "\n",
    "References:\n",
    "\n",
    "https://github.com/zenml-io/zenml-projects/blob/feature/evaluation-llm-complete-guide/llm-complete-guide/most_basic_rag_pipeline.py\n",
    "\n",
    "https://docs.zenml.io/user-guide/llmops-guide/evaluation/evaluation-in-65-loc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "from openai import OpenAI\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize(text: str):\n",
    "    return preprocess_text(text).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(query, corpus, top_n=2):\n",
    "    query_tokens = set(tokenize(query))\n",
    "    similarities = []\n",
    "    for chunk in corpus:\n",
    "        chunk_tokens = set(tokenize(chunk))\n",
    "        similarity = len(query_tokens.intersection(chunk_tokens)) / len(query_tokens.union(chunk_tokens))\n",
    "        similarities.append(similarity)\n",
    "    top_chunks = sorted(list(enumerate(similarities)), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    return [corpus[i] for i, _ in top_chunks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_query(query:str, chunks: List[str]):\n",
    "    context = \"/n\".join(chunks)\n",
    "    new_query = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"Based on the provided context, answer the following question: {query}\\n\\nContext:\\n{context}\",\n",
    "                },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "                },\n",
    "        ]\n",
    "    return new_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(query: str, corpus: str, top_n=2):\n",
    "    relevant_chunks = retrieve_relevant_chunks(query, corpus, top_n)\n",
    "    if not relevant_chunks:\n",
    "        return \"I'm sorry, I don't know the answer to that question.\"\n",
    "    client = OpenAI(api_key = os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=modify_query(query, chunks=relevant_chunks),\n",
    "        max_tokens=100,\n",
    "        temperature=0,\n",
    "    )\n",
    "    answer = chat_completion.choices[0].message.content.strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant chunks: ['The president of the United States is Joe Biden.', 'Joe Biden is the current president of the United States.']\n",
      "Modified query: [{'role': 'system', 'content': 'Based on the provided context, answer the following question: Who is the president of the United States?\\n\\nContext:\\nThe president of the United States is Joe Biden./nJoe Biden is the current president of the United States.'}, {'role': 'user', 'content': 'Who is the president of the United States?'}]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"Who is the president of the United States?\"\n",
    "corpus = [\n",
    "    \"The president of the United States is Joe Biden.\",\n",
    "    \"Joe Biden is the current president of the United States.\",\n",
    "    \"The current president of the United States is Joe Biden.\",\n",
    "]\n",
    "\n",
    "relevant_chunks = retrieve_relevant_chunks(query=query, corpus=corpus, top_n=2)\n",
    "query_modification = modify_query(query=query, chunks=relevant_chunks)\n",
    "\n",
    "print(f\"Relevant chunks: {relevant_chunks}\")\n",
    "print(f\"Modified query: {query_modification}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The president of the United States is Joe Biden.\n"
     ]
    }
   ],
   "source": [
    "run_query = True\n",
    "if run_query:\n",
    "    answer = answer_question(query, corpus)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(question, expected_answer, corpus, top_n=2):\n",
    "    \"\"\"Check if the retrieved chunks contain any words from expected answer\"\"\"\n",
    "    relevant_chunks = retrieve_relevant_chunks(question, corpus, top_n)\n",
    "    score = any(\n",
    "        any(word in chunk for word in tokenize(expected_answer))\n",
    "        for chunk in relevant_chunks\n",
    "    )\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_generation(question, expected_answer, generated_answer):\n",
    "    \"\"\"Use ChatGPT to evaluate the relevance and accuracy of a generated answer.\"\"\"\n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an evaluation judge. Given a question, an expected answer, and a generated answer, your task is to determine if the generated answer is relevant and accurate. Respond with 'YES' if the generated answer is satisfactory, or 'NO' if it is not.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Question: {question}\\nExpected Answer: {expected_answer}\\nGenerated Answer: {generated_answer}\\nIs the generated answer relevant and accurate?\",\n",
    "            },\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "\n",
    "    judgment = chat_completion.choices[0].message.content.strip().lower()\n",
    "    return judgment == \"yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = [\n",
    "    {\n",
    "        \"question\": \"What is the role of the Speaker of the House in the United States Congress?\",\n",
    "        \"expected_answer\": \"The Speaker of the House in the United States Congress is responsible for presiding over the House of Representatives, setting the legislative agenda, and representing the House to the executive branch and the public.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does the electoral college determine the outcome of the presidential election in the United States?\",\n",
    "        \"expected_answer\": \"The electoral college determines the outcome of the presidential election in the United States by allocating electors to each state based on its representation in Congress. The candidate who wins the majority of electoral votes (270 out of 538) becomes the president-elect.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Why is the nickname of the President of the United States 'Dark Brandon'?\",\n",
    "        \"expected_answer\": \"The purpose of a filibuster in the United States Senate is to prolong debate on a proposed legislation, with the aim of delaying or preventing a vote on the bill. It requires a supermajority of 60 votes to invoke cloture and end a filibuster.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Accuracy: 1.00\n",
      "Generation Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "retrieval_scores = []\n",
    "generation_scores = []\n",
    "\n",
    "for item in eval_data:\n",
    "    retrieval_score = evaluate_retrieval(\n",
    "        item[\"question\"], item[\"expected_answer\"], corpus\n",
    "    )\n",
    "    retrieval_scores.append(retrieval_score)\n",
    "\n",
    "    generated_answer = answer_question(item[\"question\"], corpus)\n",
    "    generation_score = evaluate_generation(\n",
    "        item[\"question\"], item[\"expected_answer\"], generated_answer\n",
    "    )\n",
    "    generation_scores.append(generation_score)\n",
    "\n",
    "retrieval_accuracy = sum(retrieval_scores) / len(retrieval_scores)\n",
    "generation_accuracy = sum(generation_scores) / len(generation_scores)\n",
    "\n",
    "print(f\"Retrieval Accuracy: {retrieval_accuracy:.2f}\")\n",
    "print(f\"Generation Accuracy: {generation_accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
