{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kullback-Leibler Divergence and Jensen-Shannon Divergence\n",
    "\n",
    "KL Divergence calculates a value, in bits, that represents how much information required to represent the differences between two probability distributions\n",
    "\n",
    "## Statistical Distance\n",
    "\n",
    "Statistical distance is a concept used to quantify the difference between statistical objects, such as probability distributions for a random variable.\n",
    "\n",
    "This is about comparing the entire probability distribution in terms of shape, not just comparing the mean or variance of the distribution.\n",
    "### Applications of Statistical Distances and Divergence Measures\n",
    "\n",
    "There are various situations where statistical distances and divergence measures, such as Kullback-Leibler Divergence (KL Divergence) and Jensen-Shannon Divergence, are commonly used:\n",
    "\n",
    "1. **Comparing Probability Distributions:**\n",
    "   - Evaluate the difference between two probability distributions, like a true distribution and an approximation. This is crucial for understanding how well an approximation matches the true distribution.\n",
    "\n",
    "2. **Model Evaluation:**\n",
    "   - Assess the performance of machine learning models by comparing predicted and actual probability distributions. Statistical distances provide insights into the accuracy and generalization of models.\n",
    "\n",
    "3. **Information Gain and Mutual Information:**\n",
    "   - Serve as the foundation for calculating information gain and mutual information. These measures are essential for feature selection, identifying informative features for building robust models.\n",
    "\n",
    "4. **Loss Functions for Classification Models:**\n",
    "   - Cross-entropy, derived from KL Divergence, is a commonly used loss function for classification models. It measures the difference between predicted and true distributions, guiding the model towards better predictions.\n",
    "\n",
    "5. **Generative Adversarial Networks (GANs):**\n",
    "   - Play a role in training GANs to approximate target probability distributions. Divergence scores are used in optimizing the generator to produce distributions similar to the target.\n",
    "\n",
    "6. **Understanding Complex Modeling Problems:**\n",
    "   - Valuable tools for gaining insights into complex modeling problems, helping understand how well a model approximates a target distribution and guiding the optimization process.\n",
    "\n",
    "\n",
    "## Kullback-Leibler Divergence\n",
    "\n",
    "Kullback-Leibler Divergence (KL Divergence) calculates a score that measures the divergence of one probability distribution (P) from another (Q). It is denoted as KL(P || Q) and is not symmetric. Where the “||” operator indicates “divergence” or Ps divergence from Q. KL Divergence is always non-negative and becomes zero if and only if P and Q are identical.\n",
    "\n",
    "KL divergence can be calculated as the negative sum of probability of each event in P multiplied by the log of the probability of the event in Q over the probability of the event in P.\n",
    "\n",
    "KL(P || Q) = sum x in X P(x) * log(P(x) / Q(x))\n",
    "\n",
    "The log can be base-2 to give units in “bits,” or the natural logarithm base-e with units in “nats.” When the score is 0, it suggests that both distributions are identical, otherwise the score is positive. When KL Divergence is calculated with a logarithmic base of 2, the resulting unit is in \"bits.\"\n",
    "This means that the score obtained from KL Divergence, when expressed in bits, represents the amount of additional information needed to encode events from one distribution using an optimal code based on the other distribution.\n",
    "\n",
    "The intuition for the KL divergence score is that when the probability for an event from P is large, but the probability for the same event in Q is small, there is a large divergence. When the probability from P is small and the probability from Q is large, there is also a large divergence, but not as large as the first case.\n",
    "\n",
    "## Jensen-Shannon Divergence\n",
    "\n",
    "Jensen-Shannon Divergence is an extension of KL Divergence that provides a normalized and symmetrical version. It calculates a score and distance measure between two probability distributions in a symmetrical way. It is defined as the average of the KL Divergence between P and the average of P and Q.\n",
    "\n",
    "## Applications\n",
    "\n",
    "- KL Divergence and Jensen-Shannon Divergence find applications in machine learning.\n",
    "- They are used to calculate the difference between an actual and observed probability distribution, valuable in tasks like model evaluation and feature selection.\n",
    "- KL Divergence is a key concept in information theory and is used in various fields where probability distributions are compared.\n",
    "\n",
    "Understanding these measures is crucial for tasks involving the comparison of probability distributions, helping in assessing the similarity or dissimilarity between different sets of data.\n",
    "\n",
    "## Differences between KL Divergence and Cross-Entropy\n",
    "### Cross Entropy and KL Divergence\n",
    "cross entropy is focused on the distribution of the labels which kl divergence is focused on the entire distribution\n",
    "\n",
    "#### Cross Entropy (H(P, Q))\n",
    "Cross entropy is a measure of how well one probability distribution predicts a set of events from another distribution. In the context of machine learning it is how well the model's predicted probability distribution aligns with the true distribution of the labels. Cross entropy is primarily focused on measuring the dissimilarity or difference between two probability distributions over the same events, with a particular emphasis on the distribution of the labels. For two probability distributions P and Q, the cross entropy from P to Q is given by the formula:\n",
    "\n",
    "\\[ H(P, Q) = -\\sum_{i} P(i) \\log(Q(i)) \\]\n",
    "\n",
    "It quantifies the average number of bits needed to encode events from P when using the optimal code based on Q. Cross entropy is always non-negative and is minimized when P and Q are the same.\n",
    "\n",
    "#### KL Divergence (\\(D_{KL}(P \\,||\\, Q)\\))\n",
    "Kullback-Leibler (KL) divergence is a broader concept that measures the information lost when one probability distribution is used to approximate another. It quantifies the extra amount of information needed to represent events from one distribution when using the optimal code based on another distribution. In the context of probability distributions associated with machine learning, KL divergence can be used to assess how one distribution diverges from another, considering the entire distribution.\n",
    "\n",
    "<b>KL divergence measures the information lost when using Q to approximate P. It quantifies the difference between two probability distributions P and Q.</b>\n",
    "\n",
    "\\[ D_{KL}(P \\,||\\, Q) = \\sum_{i} P(i) \\log\\left(\\frac{P(i)}{Q(i)}\\right) \\]\n",
    "\n",
    "It represents the extra amount of information needed to encode events from P when using the code based on Q instead of the optimal code for P. KL divergence is not symmetric (\\(D_{KL}(P \\,||\\, Q) \\neq D_{KL}(Q \\,||\\, P)\\)).\n",
    "\n",
    "#### Relationship\n",
    "\n",
    "Cross entropy includes both the entropy of the true distribution P and the KL divergence between P and Q:\n",
    "\n",
    "\\[ H(P, Q) = H(P) + D_{KL}(P \\,||\\, Q) \\]\n",
    "\n",
    "The entropy term H(P) is a constant for a given true distribution and does not depend on the predicted distribution Q.\n",
    "\n",
    "\n",
    "### References\n",
    "https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-understanding-kl-divergence-2b382ca2b2a8\n",
    "\n",
    "https://machinelearningmastery.com/divergence-between-probability-distributions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple, Dict, Any, Optional, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is information Entropy?\n",
    "\n",
    "Quantifying information is the foundation of the field of information theory.\n",
    "\n",
    "The intuition behind quantifying information is the idea of measuring how much surprise there is in an event. Those events that are rare (low probability) are more surprising and therefore have more information than those events that are common (high probability).\n",
    "\n",
    "Low Probability Event: High Information (surprising).\n",
    "High Probability Event: Low Information (unsurprising).\n",
    "\n",
    "Rare events are more uncertain or more surprising and require more information to represent them than common events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(x)=0.500, information: 1.000 bits\n",
      "p(x)=0.900, information: 0.152 bits\n",
      "p(x)=0.100, information: 3.322 bits\n",
      "p(x)=0.167, information: 2.585 bits\n",
      "Events that are more likely to happen have less information, and events that are less likely to happen have more information.\n"
     ]
    }
   ],
   "source": [
    "def get_shannon_information(p: float) -> float:\n",
    "    h = -np.log2(p)\n",
    "    return h\n",
    "\n",
    "# probability of getting heads in a fair coin toss: p = 0.5\n",
    "p = 0.5\n",
    "print(f'p(x)={p:.3f}, information: {get_shannon_information(p):.3f} bits')\n",
    "\n",
    "# probability of getting heads in a biased coin toss: p = 0.9\n",
    "p = 0.9\n",
    "print(f'p(x)={p:.3f}, information: {get_shannon_information(p):.3f} bits')\n",
    "\n",
    "# probability of getting heads in a biased coin toss: p = 0.1\n",
    "p = 0.1\n",
    "print(f'p(x)={p:.3f}, information: {get_shannon_information(p):.3f} bits')\n",
    "\n",
    "# probability of rolling a 6 on a fair die: p = 1/6\n",
    "p = 1/6\n",
    "print(f'p(x)={p:.3f}, information: {get_shannon_information(p):.3f} bits')\n",
    "\n",
    "print('Events that are more likely to happen have less information, and events that are less likely to happen have more information.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to Kl Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P=1.000 Q=1.000\n"
     ]
    }
   ],
   "source": [
    "events = ['red', 'green', 'blue']\n",
    "target_probability_distribution = [0.10, 0.40, 0.50]\n",
    "p = torch.tensor(target_probability_distribution)\n",
    "approximation_probability_distribution = [0.80, 0.15, 0.05]\n",
    "q = torch.tensor(approximation_probability_distribution)\n",
    "print('P=%.3f Q=%.3f' % (sum(p), sum(q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot first distribution\n",
    "plt.subplot(2,1,1)\n",
    "plt.bar(events, p)\n",
    "# plot second distribution\n",
    "plt.subplot(2,1,2)\n",
    "plt.bar(events, q)\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_bits(p:torch.Tensor, q:torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the Kullback-Leibler (KL) divergence between two probability distributions\n",
    "    \"\"\"\n",
    "\n",
    "    # Avoiding division by zero by adding a small epsilon\n",
    "    epsilon = 1e-10\n",
    "    \n",
    "    # Calculate KL Divergence using NumPy functions\n",
    "    kl_divergence = torch.sum(p * torch.log2((p + epsilon) / (q + epsilon)))\n",
    "    \n",
    "    return kl_divergence\n",
    "\n",
    "def kl_divergence_nats(p:torch.Tensor, q:torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the Kullback-Leibler (KL) divergence between two probability distributions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Avoiding division by zero by adding a small epsilon\n",
    "    epsilon = 1e-10\n",
    "    \n",
    "    # Calculate KL Divergence using NumPy functions\n",
    "    kl_divergence = torch.sum(p * torch.log((p + epsilon) / (q + epsilon)))\n",
    "    \n",
    "    return kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL(P || Q): 1.927 bits\n",
      "KL(Q || P): 2.022 bits\n"
     ]
    }
   ],
   "source": [
    "# calculate (P || Q)\n",
    "kl_pq = kl_divergence_bits(p, q)\n",
    "print('KL(P || Q): %.3f bits' % kl_pq)\n",
    "# calculate (Q || P)\n",
    "kl_qp = kl_divergence_bits(q, p)\n",
    "print('KL(Q || P): %.3f bits' % kl_qp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL(P || Q): 1.336 nats\n",
      "KL(Q || P): 1.401 nats\n"
     ]
    }
   ],
   "source": [
    "# calculate (P || Q)\n",
    "kl_pq = kl_divergence_nats(p, q)\n",
    "print('KL(P || Q): %.3f nats' % kl_pq)\n",
    "# calculate (Q || P)\n",
    "kl_qp = kl_divergence_nats(q, p)\n",
    "print('KL(Q || P): %.3f nats' % kl_qp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jensen-Shannon Divergence\n",
    "\n",
    "The Jensen-Shannon divergence (JS divergence) is a measure of difference or similarity between two probability distributions. It provides a symmetrical and normalized score, making it more practical for comparisons.\n",
    "\n",
    "### Calculation Formula\n",
    "\n",
    "The JS divergence between distributions P and Q is calculated as:\n",
    "\n",
    "\\[ JS(P || Q) = \\frac{1}{2} \\cdot KL(P || M) + \\frac{1}{2} \\cdot KL(Q || M) \\]\n",
    "\n",
    "Where \\( M = \\frac{1}{2} \\cdot (P + Q) \\), and \\( KL() \\) is the Kullback-Leibler divergence.\n",
    "\n",
    "### Properties\n",
    "\n",
    "- Symmetry: \\( JS(P || Q) = JS(Q || P) \\)\n",
    "- Normalization: Scores range from 0 (identical) to 1 (maximally different) when using the base-2 logarithm.\n",
    "\n",
    "### Jensen-Shannon Distance\n",
    "\n",
    "Taking the square root of the JS divergence score gives the Jensen-Shannon distance, denoted as JS distance.\n",
    "\n",
    "### Example\n",
    "\n",
    "A function to calculate JS divergence can be defined, utilizing the previously prepared kl_divergence() function.\n",
    "\n",
    "```python\n",
    "def js_divergence(p, q):\n",
    "    M = 0.5 * (p + q)\n",
    "    return 0.5 * kl_divergence(p, M) + 0.5 * kl_divergence(q, M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def js_divergence(p: torch.Tensor, q: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the Jensen-Shannon (JS) divergence between two probability distributions\n",
    "\n",
    "    :param p: True probability distribution\n",
    "    :type p: List[float]\n",
    "    :param q: Approximation probability distribution\n",
    "    :type q: List[float]\n",
    "    :return: Statistical distance between two probability distributions\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    # Calculate JS Divergence using PyTorch functions\n",
    "    m_tensor = 0.5 * (p + q)\n",
    "    js_divergence = 0.5 * kl_divergence_bits(p, m_tensor) + 0.5 * kl_divergence_bits(q, m_tensor)\n",
    "    \n",
    "    return js_divergence  # Convert result to Python float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JS(P || Q) divergence: 0.420 bits\n"
     ]
    }
   ],
   "source": [
    "# calculate JS(P || Q)\n",
    "js_pq = js_divergence(p, q)\n",
    "print('JS(P || Q) divergence: %.3f bits' % js_pq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JS(Q || P) divergence: 0.420 bits\n"
     ]
    }
   ],
   "source": [
    "# calculate JS(Q || P)\n",
    "js_qp = js_divergence(q, p)\n",
    "print('JS(Q || P) divergence: %.3f bits' % js_qp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example as a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def kl_divergence_torch(p:torch.Tensor, q:torch.Tensor) -> float:\n",
    "#     \"\"\"\n",
    "#     Calculation of KL divergence using PyTorch functions\n",
    "#     \"\"\"\n",
    "#     eps = 1e-10\n",
    "#     p_log = torch.log(p + eps)\n",
    "#     q_log = torch.log(q + eps)\n",
    "#     loss = (p * (p_log - q_log))\n",
    "#     loss = loss.sum() / p.size(0)\n",
    "#     return loss\n",
    "\n",
    "def kl_divergence_torch(input, target, reduction='batchmean', log_target=False):\n",
    "    # Avoiding underflow issues by adding a small epsilon\n",
    "    epsilon = 1e-10\n",
    "\n",
    "    if not log_target:\n",
    "        # Compute KL divergence pointwise\n",
    "        loss_pointwise = target * (target.log() - input)\n",
    "    else:\n",
    "        # Compute KL divergence pointwise with log_target=True\n",
    "        loss_pointwise = target.exp() * (target - input)\n",
    "\n",
    "    # Apply reduction\n",
    "    if reduction == 'mean':\n",
    "        loss = loss_pointwise.mean()\n",
    "    elif reduction == 'batchmean':\n",
    "        loss = loss_pointwise.sum() / input.size(0)\n",
    "    elif reduction == 'sum':\n",
    "        loss = loss_pointwise.sum()\n",
    "    else:  # reduction == 'none'\n",
    "        loss = loss_pointwise\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL(P || Q): 0.265 nats\n"
     ]
    }
   ],
   "source": [
    "# calculate (P || Q)\n",
    "kl_pq = kl_divergence_torch(p , q, log_target=True)\n",
    "print('KL(P || Q): %.3f nats' % kl_pq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL(P || Q) divergence: 0.265 bits\n"
     ]
    }
   ],
   "source": [
    "kl_loss = F.kl_div(p, q, reduction='batchmean', log_target=True)\n",
    "print('KL(P || Q) divergence: %.3f bits' % kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # It should be implemented as follows:\n",
    "# outputs = model(x)\n",
    "# targets_log = torch.og(targets + 1e-10)\n",
    "# loss = F.kl_div(outputs, targets_log, reduction='batchmean')\n",
    "# kl_loss.backward\n",
    "# optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
