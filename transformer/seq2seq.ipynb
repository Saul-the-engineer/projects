{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saulg\\Anaconda3\\envs\\DL_PyTorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Seq2Seq using Transformers on the Multi30k\n",
    "dataset. In this video I utilize Pytorch\n",
    "inbuilt Transformer modules, and have a\n",
    "separate implementation for Transformers\n",
    "from scratch. Training this model for a\n",
    "while (not too long) gives a BLEU score\n",
    "of ~35, and I think training for longer\n",
    "would give even better results.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import spacy\n",
    "from utils import translate_sentence, bleu, save_checkpoint, load_checkpoint\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "     --------------------------------------- 12.8/12.8 MB 25.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from en-core-web-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.6)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: setuptools in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (65.5.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.7)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'de' are deprecated. Please use the\n",
      "full pipeline package name 'de_core_news_sm' instead.\u001b[0m\n",
      "Collecting de-core-news-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.5.0/de_core_news_sm-3.5.0-py3-none-any.whl (14.6 MB)\n",
      "     --------------------------------------- 14.6/14.6 MB 21.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from de-core-news-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: setuptools in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (65.5.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.10.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.0.7)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.23.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\saulg\\anaconda3\\envs\\dl_pytorch\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en\n",
    "!python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_ger = spacy.load(\"de_core_news_sm\")\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_ger(text):\n",
    "    return [tok.text for tok in spacy_ger.tokenizer(text)]\n",
    "\n",
    "\n",
    "def tokenize_eng(text):\n",
    "    return [tok.text for tok in spacy_eng.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "german = Field(tokenize=tokenize_ger, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "\n",
    "english = Field(tokenize=tokenize_eng, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(\n",
    "    exts=(\".de\", \".en\"), fields=(german, english)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "german.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "english.build_vocab(train_data, max_size=10000, min_freq=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_len,\n",
    "        device,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n",
    "        self.src_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n",
    "        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "\n",
    "        self.device = device\n",
    "        self.transformer = nn.Transformer(\n",
    "            embedding_size,\n",
    "            num_heads,\n",
    "            num_encoder_layers,\n",
    "            num_decoder_layers,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = src.transpose(0, 1) == self.src_pad_idx\n",
    "\n",
    "        # (N, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_seq_length, N = src.shape\n",
    "        trg_seq_length, N = trg.shape\n",
    "\n",
    "        src_positions = (\n",
    "            torch.arange(0, src_seq_length)\n",
    "            .unsqueeze(1)\n",
    "            .expand(src_seq_length, N)\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        trg_positions = (\n",
    "            torch.arange(0, trg_seq_length)\n",
    "            .unsqueeze(1)\n",
    "            .expand(trg_seq_length, N)\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        embed_src = self.dropout(\n",
    "            (self.src_word_embedding(src) + self.src_position_embedding(src_positions))\n",
    "        )\n",
    "        embed_trg = self.dropout(\n",
    "            (self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions))\n",
    "        )\n",
    "\n",
    "        src_padding_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        out = self.transformer(\n",
    "            embed_src,\n",
    "            embed_trg,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_mask=trg_mask,\n",
    "        )\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're ready to define everything we need for training our Seq2Seq model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "load_model = False\n",
    "save_model = False\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 3e-4\n",
    "batch_size = 32\n",
    "\n",
    "# Model hyperparameters\n",
    "src_vocab_size = len(german.vocab)\n",
    "trg_vocab_size = len(english.vocab)\n",
    "embedding_size = 512\n",
    "num_heads = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "dropout = 0.10\n",
    "max_len = 100\n",
    "forward_expansion = 4\n",
    "src_pad_idx = english.vocab.stoi[\"<pad>\"]\n",
    "\n",
    "# Tensorboard to get nice loss plot\n",
    "#writer = SummaryWriter(\"runs/loss_plot\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    embedding_size,\n",
    "    src_vocab_size,\n",
    "    trg_vocab_size,\n",
    "    src_pad_idx,\n",
    "    num_heads,\n",
    "    num_encoder_layers,\n",
    "    num_decoder_layers,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    max_len,\n",
    "    device,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.1, patience=10, verbose=True\n",
    ")\n",
    "\n",
    "pad_idx = english.vocab.stoi[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 / 10]\n",
      "Translated example sentence: \n",
      " ['reporter', 'quarterback', 'smiles', 'smiles', 'smiles', 'newborn', 'pitching', 'smiles', 'smiles', 'smiles', 'smiles', 'reporter', 'quarterback', 'smiles', 'quarterback', 'smiles', 'smiles', 'quarterback', 'smiles', 'smiles', 'quarterback', 'smiles', 'smiles', 'smiles', 'smiles', 'smiles', 'quarterback', 'pitching', 'smiles', 'quarterback', 'smiles', 'smiles', 'smiles', 'quarterback', 'smiles', 'smiles', 'quarterback', 'smiles', 'smiles', 'innertube', 'asphalt', 'innertube', 'asphalt', 'quarterback', 'smiles', 'smiles', 'smiles', 'smiles', 'innertube', 'smiles']\n",
      "[Epoch 1 / 10]\n",
      "Translated example sentence: \n",
      " ['there', 'is', '<unk>', '<unk>', '<unk>', 'with', 'a', 'tennis', 'ball', ',', 'tennis', 'tennis', 'tennis', 'tennis', 'tennis', 'tennis', '.', '<eos>']\n",
      "[Epoch 2 / 10]\n",
      "Translated example sentence: \n",
      " ['there', 'is', 'a', 'tennis', 'game', 'with', 'a', 'tennis', 'ball', 'ball', ',', 'as', 'a', 'tennis', 'ball', '.', '<eos>']\n",
      "[Epoch 3 / 10]\n",
      "Translated example sentence: \n",
      " ['there', 'is', 'a', 'soccer', 'game', 'with', 'a', 'ball', 'ball', 'and', 'tennis', 'racket', 'play', 'with', 'a', 'tennis', 'ball', '.', '<eos>']\n",
      "[Epoch 4 / 10]\n",
      "Translated example sentence: \n",
      " ['there', 'is', '<unk>', '<unk>', 'to', 'play', 'with', 'a', 'tennis', 'ball', ',', 'as', 'it', 'plays', 'with', 'a', 'tennis', 'ball', '.', '<eos>']\n",
      "[Epoch 5 / 10]\n",
      "Translated example sentence: \n",
      " ['it', 'is', '<unk>', '<unk>', 'with', 'a', 'tennis', 'ball', ',', 'as', 'a', 'tennis', 'ball', '.', '<eos>']\n",
      "[Epoch 6 / 10]\n",
      "Translated example sentence: \n",
      " ['it', 'is', '<unk>', '<unk>', ',', 'with', 'a', 'tennis', 'ball', ',', 'as', 'a', 'tennis', 'ball', ',', 'play', 'tennis', 'ball', '.', '<eos>']\n",
      "[Epoch 7 / 10]\n",
      "Translated example sentence: \n",
      " ['it', 'is', '<unk>', '<unk>', ',', 'with', 'a', 'tennis', 'ball', ',', 'as', 'it', 'play', 'with', 'a', 'tennis', 'ball', '.', '<eos>']\n",
      "[Epoch 8 / 10]\n",
      "Translated example sentence: \n",
      " ['it', 'is', 'about', '<unk>', 'tennis', 'ball', 'to', 'play', 'tennis', 'ball', 'with', 'a', 'tennis', 'ball', '.', '<eos>']\n",
      "[Epoch 9 / 10]\n",
      "Translated example sentence: \n",
      " ['there', 'is', 'a', 'lot', 'of', '<unk>', ',', 'with', 'a', 'tennis', 'ball', ',', 'as', 'if', 'competition', ',', 'a', 'tennis', 'ball', '.', '<eos>']\n",
      "Bleu score 29.59\n"
     ]
    }
   ],
   "source": [
    "if load_model:\n",
    "    load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
    "\n",
    "sentence = \"Es ist viel schwieriger, mit einem Bowlingball Tennis zu spielen, als mit einem Tennisball zu bowlen.\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "\n",
    "    if save_model:\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "        }\n",
    "        save_checkpoint(checkpoint)\n",
    "\n",
    "    model.eval()\n",
    "    translated_sentence = translate_sentence(\n",
    "        model, sentence, german, english, device, max_length=50\n",
    "    )\n",
    "\n",
    "    print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        # Get input and targets and get to cuda\n",
    "        inp_data = batch.src.to(device)\n",
    "        target = batch.trg.to(device)\n",
    "\n",
    "        # Forward prop\n",
    "        output = model(inp_data, target[:-1, :])\n",
    "\n",
    "        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n",
    "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
    "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
    "        # way that we have output_words * batch_size that we want to send in into\n",
    "        # our cost function, so we need to do some reshapin.\n",
    "        # Let's also remove the start token while we're at it\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Back prop\n",
    "        loss.backward()\n",
    "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
    "        # within a healthy range\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Gradient descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        # plot to tensorboard\n",
    "        #writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "        step += 1\n",
    "\n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    scheduler.step(mean_loss)\n",
    "\n",
    "# running on entire test data takes a while\n",
    "score = bleu(test_data[1:100], model, german, english, device)\n",
    "print(f\"Bleu score {score * 100:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
